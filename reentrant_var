asmrun/startup.c:  ctx->caml_globals = malloc(sizeof(value)*(i+1));
asmrun/startup.c:  ctx->caml_globals_len = i;
asmrun/startup.c:// printf("ctx->caml_globals = %p\n", ctx->caml_globals);
asmrun/startup.c:    *(ctx->caml_globals+i) = w;
asmrun/startup.c:  //  printf("ctx->caml_globals[%d] = %p wosize = %d\n", i, w, Wosize_val(w));
asmrun/startup.c:  *(ctx->caml_globals+i) = 0;
asmrun/fail.c:  if (ctx->caml_exception_pointer == NULL) caml_fatal_uncaught_exception_r(ctx, v);
asmrun/fail.c:  while (ctx->caml_local_roots != NULL &&
asmrun/fail.c:         (char *) ctx->caml_local_roots PUSHED_AFTER ctx->caml_exception_pointer) {
asmrun/fail.c:    ctx->caml_local_roots = ctx->caml_local_roots->next;
asmrun/fail.c:  if (! ctx->array_bound_error_bucket_inited) {
asmrun/fail.c:    ctx->array_bound_error_msg.hdr = Make_header(wosize, String_tag, Caml_white);
asmrun/fail.c:    ctx->array_bound_error_msg.data[offset_index] = offset_index - BOUND_MSG_LEN;
asmrun/fail.c:    ctx->array_bound_error_bucket.hdr = Make_header(2, 0, Caml_white);
asmrun/fail.c:    ctx->array_bound_error_bucket.exn = (value) caml_exn_Invalid_argument;
asmrun/fail.c:    ctx->array_bound_error_bucket.arg = (value) ctx->array_bound_error_msg.data;
asmrun/fail.c:    ctx->array_bound_error_bucket_inited = 1;
asmrun/fail.c:                        &ctx->array_bound_error_msg,
asmrun/fail.c:                        &ctx->array_bound_error_msg + 1);
asmrun/fail.c:    ctx->array_bound_error_bucket_inited = 1;
asmrun/fail.c:  caml_raise_r(ctx, (value) &ctx->array_bound_error_bucket.exn);
asmrun/signals_asm.c:  ctx->caml_young_limit = ctx->caml_young_start;
asmrun/signals_asm.c:  if (ctx->caml_young_ptr < ctx->caml_young_start || ctx->caml_force_major_slice) {
asmrun/signals_asm.c:        CONTEXT_YOUNG_LIMIT = (context_reg) main_ctx->caml_young_limit;
asmrun/signals_asm.c:      CONTEXT_YOUNG_LIMIT = (context_reg) handler_ctx->caml_young_limit;
asmrun/gc_ctrl.c:  char *chunk = ctx->caml_heap_start, *chunk_end;
asmrun/gc_ctrl.c:                  || cur_hp == ctx->caml_gc_sweep_hp);
asmrun/gc_ctrl.c:          if (ctx->caml_gc_phase == Phase_sweep && cur_hp >= ctx->caml_gc_sweep_hp){
asmrun/gc_ctrl.c:  Assert (heap_chunks == ctx->caml_stat_heap_chunks);
asmrun/gc_ctrl.c:          == Wsize_bsize (ctx->caml_stat_heap_size));
asmrun/gc_ctrl.c:    double minwords = ctx->caml_stat_minor_words
asmrun/gc_ctrl.c:                      + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
asmrun/gc_ctrl.c:    double prowords = ctx->caml_stat_promoted_words;
asmrun/gc_ctrl.c:    double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
asmrun/gc_ctrl.c:    intnat mincoll = ctx->caml_stat_minor_collections;
asmrun/gc_ctrl.c:    intnat majcoll = ctx->caml_stat_major_collections;
asmrun/gc_ctrl.c:    intnat heap_words = Wsize_bsize (ctx->caml_stat_heap_size);
asmrun/gc_ctrl.c:    intnat cpct = ctx->caml_stat_compactions;
asmrun/gc_ctrl.c:    intnat top_heap_words = Wsize_bsize (ctx->caml_stat_top_heap_size);
asmrun/gc_ctrl.c:  double prowords = ctx->caml_stat_promoted_words;
asmrun/gc_ctrl.c:  double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
asmrun/gc_ctrl.c:  intnat mincoll = ctx->caml_stat_minor_collections;
asmrun/gc_ctrl.c:  intnat majcoll = ctx->caml_stat_major_collections;
asmrun/gc_ctrl.c:  intnat heap_words = ctx->caml_stat_heap_size / sizeof (value);
asmrun/gc_ctrl.c:  intnat top_heap_words = ctx->caml_stat_top_heap_size / sizeof (value);
asmrun/gc_ctrl.c:  intnat cpct = ctx->caml_stat_compactions;
asmrun/gc_ctrl.c:  intnat heap_chunks = ctx->caml_stat_heap_chunks;
asmrun/gc_ctrl.c:  minwords = ctx->caml_stat_minor_words
asmrun/gc_ctrl.c:             + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
asmrun/gc_ctrl.c:  double minwords = ctx->caml_stat_minor_words
asmrun/gc_ctrl.c:                    + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
asmrun/gc_ctrl.c:  double prowords = ctx->caml_stat_promoted_words;
asmrun/gc_ctrl.c:  double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
asmrun/gc_ctrl.c:  Store_field (res, 0, Val_long (Wsize_bsize (ctx->caml_minor_heap_size)));  /* s */
asmrun/gc_ctrl.c:  Store_field (res, 1,Val_long(Wsize_bsize(ctx->caml_major_heap_increment)));/* i */
asmrun/gc_ctrl.c:  Store_field (res, 2, Val_long (ctx->caml_percent_free));                   /* o */
asmrun/gc_ctrl.c:  Store_field (res, 4, Val_long (ctx->caml_percent_max));                    /* O */
asmrun/gc_ctrl.c:  if (newpf != ctx->caml_percent_free){
asmrun/gc_ctrl.c:    ctx->caml_percent_free = newpf;
asmrun/gc_ctrl.c:    caml_gc_message (0x20, "New space overhead: %d%%\n", ctx->caml_percent_free);
asmrun/gc_ctrl.c:  if (newpm != ctx->caml_percent_max){
asmrun/gc_ctrl.c:    ctx->caml_percent_max = newpm;
asmrun/gc_ctrl.c:    caml_gc_message (0x20, "New max overhead: %d%%\n", ctx->caml_percent_max);
asmrun/gc_ctrl.c:  if (newheapincr != ctx->caml_major_heap_increment){
asmrun/gc_ctrl.c:    ctx->caml_major_heap_increment = newheapincr;
asmrun/gc_ctrl.c:                     ctx->caml_major_heap_increment/1024);
asmrun/gc_ctrl.c:  if (newminsize != ctx->caml_minor_heap_size){
asmrun/gc_ctrl.c:  fp = 100.0 * ctx->caml_fl_cur_size
asmrun/gc_ctrl.c:       / (Wsize_bsize (caml_stat_heap_size) - ctx->caml_fl_cur_size);
asmrun/gc_ctrl.c:  if (fp >= ctx->caml_percent_max && ctx->caml_stat_heap_chunks > 1){
asmrun/gc_ctrl.c:  ctx->caml_major_heap_increment = Bsize_wsize (norm_heapincr (major_incr));
asmrun/gc_ctrl.c:  ctx->caml_percent_free = norm_pfree (percent_fr);
asmrun/gc_ctrl.c:                   ctx->caml_minor_heap_size / 1024);
asmrun/gc_ctrl.c:  caml_gc_message (0x20, "Initial space overhead: %lu%%\n", ctx->caml_percent_free);
asmrun/gc_ctrl.c:                   ctx->caml_major_heap_increment / 1024);
asmrun/printexc.c:  saved_backtrace_active = ctx->caml_backtrace_active;
asmrun/printexc.c:  saved_backtrace_pos = ctx->caml_backtrace_pos;
asmrun/printexc.c:  ctx->caml_backtrace_active = 0;
asmrun/printexc.c:  ctx->caml_backtrace_active = saved_backtrace_active;
asmrun/printexc.c:  ctx->caml_backtrace_pos = saved_backtrace_pos;
asmrun/printexc.c:  if (ctx->caml_backtrace_active
asmrun/memory.c:                   (ctx->caml_stat_heap_size + Chunk_size (m)) / 1024);
asmrun/memory.c:    char **last = &ctx->caml_heap_start;
asmrun/memory.c:    ++ ctx->caml_stat_heap_chunks;
asmrun/memory.c:  ctx->caml_stat_heap_size += Chunk_size (m);
asmrun/memory.c:  if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
asmrun/memory.c:    ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
asmrun/memory.c:  over_request = request + request / 100 * ctx->caml_percent_free;
asmrun/memory.c:  if (chunk == ctx->caml_heap_start) return;
asmrun/memory.c:  ctx->caml_stat_heap_size -= Chunk_size (chunk);
asmrun/memory.c:                   (unsigned long) ctx->caml_stat_heap_size / 1024);
asmrun/memory.c:  -- ctx->caml_stat_heap_chunks;
asmrun/memory.c:  cp = &ctx->caml_heap_start;
asmrun/memory.c:  if (ctx->caml_gc_phase == Phase_mark
asmrun/memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
asmrun/memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
asmrun/memory.c:            || (ctx->caml_gc_phase == Phase_sweep
asmrun/memory.c:                && (addr)hp < (addr)ctx->caml_gc_sweep_hp));
asmrun/memory.c:      if (ctx->caml_in_minor_collection)
asmrun/memory.c:  if (ctx->caml_gc_phase == Phase_mark
asmrun/memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
asmrun/memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
asmrun/memory.c:            || (ctx->caml_gc_phase == Phase_sweep
asmrun/memory.c:                && (addr)hp < (addr)ctx->caml_gc_sweep_hp));
asmrun/memory.c:  ctx->caml_allocated_words += Whsize_wosize (wosize);
asmrun/memory.c:  if (ctx->caml_allocated_words > Wsize_bsize (ctx->caml_minor_heap_size)){
asmrun/memory.c:  ctx->caml_extra_heap_resources += (double) res / (double) max;
asmrun/memory.c:  if (ctx->caml_extra_heap_resources > 1.0){
asmrun/memory.c:    ctx->caml_extra_heap_resources = 1.0;
asmrun/memory.c:  if (ctx->caml_extra_heap_resources
asmrun/memory.c:           > (double) Wsize_bsize (ctx->caml_minor_heap_size) / 2.0
asmrun/memory.c:             / (double) Wsize_bsize (ctx->caml_stat_heap_size)) {
asmrun/memory.c:    if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){
asmrun/memory.c:      caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);
asmrun/memory.c:    *(ctx->caml_ref_table.ptr++) = fp;
asmrun/globroots.c:  r = ctx->random_seed = ctx->random_seed * 69069 + 25173;
asmrun/globroots.c:  caml_insert_global_root_r(ctx, &ctx->caml_global_roots, r);
asmrun/globroots.c:  caml_delete_global_root_r(ctx, &ctx->caml_global_roots, r);
asmrun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
asmrun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
asmrun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
asmrun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
asmrun/globroots.c:    caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
asmrun/globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
asmrun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
asmrun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
asmrun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
asmrun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
asmrun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
asmrun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
asmrun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_old);
asmrun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
asmrun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
asmrun/globroots.c:  for (gr = ctx->caml_global_roots_young.forward[0];
asmrun/globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, gr->root);
asmrun/globroots.c:  caml_empty_global_roots(&ctx->caml_global_roots_young);
asmrun/md5.c:    ctx->buf[0] = 0x67452301;
asmrun/md5.c:    ctx->buf[1] = 0xefcdab89;
asmrun/md5.c:    ctx->buf[2] = 0x98badcfe;
asmrun/md5.c:    ctx->buf[3] = 0x10325476;
asmrun/md5.c:    ctx->bits[0] = 0;
asmrun/md5.c:    ctx->bits[1] = 0;
asmrun/md5.c:    t = ctx->bits[0];
asmrun/md5.c:    if ((ctx->bits[0] = t + ((uint32) len << 3)) < t)
asmrun/md5.c:        ctx->bits[1]++;         /* Carry from low to high */
asmrun/md5.c:    ctx->bits[1] += len >> 29;
asmrun/md5.c:        unsigned char *p = (unsigned char *) ctx->in + t;
asmrun/md5.c:        byteReverse(ctx->in, 16);
asmrun/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
asmrun/md5.c:        memcpy(ctx->in, buf, 64);
asmrun/md5.c:        byteReverse(ctx->in, 16);
asmrun/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
asmrun/md5.c:    memcpy(ctx->in, buf, len);
asmrun/md5.c:    count = (ctx->bits[0] >> 3) & 0x3F;
asmrun/md5.c:    p = ctx->in + count;
asmrun/md5.c:        byteReverse(ctx->in, 16);
asmrun/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
asmrun/md5.c:        memset(ctx->in, 0, 56);
asmrun/md5.c:    byteReverse(ctx->in, 14);
asmrun/md5.c:    ((uint32 *) ctx->in)[14] = ctx->bits[0];
asmrun/md5.c:    ((uint32 *) ctx->in)[15] = ctx->bits[1];
asmrun/md5.c:    caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
asmrun/md5.c:    byteReverse((unsigned char *) ctx->buf, 4);
asmrun/md5.c:    memcpy(digest, ctx->buf, 16);
asmrun/signals.c:  ctx->caml_young_limit = ctx->caml_young_end;
asmrun/signals.c:  if (main_ctx) main_ctx->caml_young_limit = main_ctx->caml_young_end;
asmrun/signals.c:  ctx->caml_force_major_slice = 1;
asmrun/signals.c:  ctx->caml_young_limit = ctx->caml_young_end;
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) {
asmrun/compare.c:    free(ctx->compare_stack);
asmrun/compare.c:    ctx->compare_stack = ctx->compare_stack_init;
asmrun/compare.c:    ctx->compare_stack_limit = ctx->compare_stack + COMPARE_STACK_INIT_SIZE;
asmrun/compare.c:  asize_t newsize = 2 * (ctx->compare_stack_limit - ctx->compare_stack);
asmrun/compare.c:  asize_t sp_offset = sp - ctx->compare_stack;
asmrun/compare.c:  if (ctx->compare_stack == ctx->compare_stack_init) {
asmrun/compare.c:    memcpy(newstack, ctx->compare_stack_init,
asmrun/compare.c:      realloc(ctx->compare_stack, sizeof(struct compare_item) * newsize);
asmrun/compare.c:  ctx->compare_stack = newstack;
asmrun/compare.c:  ctx->compare_stack_limit = newstack + newsize;
asmrun/compare.c:  sp = ctx->compare_stack;
asmrun/compare.c:          ctx->caml_compare_unordered = 0;
asmrun/compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
asmrun/compare.c:          ctx->caml_compare_unordered = 0;
asmrun/compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
asmrun/compare.c:      ctx->caml_compare_unordered = 0;
asmrun/compare.c:      if (ctx->caml_compare_unordered && !total) return UNORDERED;
asmrun/compare.c:        if (sp >= ctx->compare_stack_limit) sp = compare_resize_stack_r(ctx, sp);
asmrun/compare.c:    if (sp == ctx->compare_stack) return EQUAL; /* we're done */
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
asmrun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
asmrun/backtrace.c:  if (flag != ctx->caml_backtrace_active) {
asmrun/backtrace.c:    ctx->caml_backtrace_active = flag;
asmrun/backtrace.c:    ctx->caml_backtrace_pos = 0;
asmrun/backtrace.c:      caml_register_global_root_r(ctx, &ctx->caml_backtrace_last_exn);
asmrun/backtrace.c:      caml_remove_global_root_r(ctx, &ctx->caml_backtrace_last_exn);
asmrun/backtrace.c:  return Val_bool(ctx->caml_backtrace_active);
asmrun/backtrace.c:  if (exn != ctx->caml_backtrace_last_exn) {
asmrun/backtrace.c:    ctx->caml_backtrace_pos = 0;
asmrun/backtrace.c:    ctx->caml_backtrace_last_exn = exn;
asmrun/backtrace.c:  if (ctx->caml_backtrace_buffer == NULL) {
asmrun/backtrace.c:    ctx->caml_backtrace_buffer = malloc(BACKTRACE_BUFFER_SIZE * sizeof(code_t));
asmrun/backtrace.c:    if (ctx->caml_backtrace_buffer == NULL) return ctx;
asmrun/backtrace.c:  if (ctx->caml_frame_descriptors == NULL) caml_init_frame_descriptors_r(ctx);
asmrun/backtrace.c:      d = ctx->caml_frame_descriptors[h];
asmrun/backtrace.c:      h = (h+1) & ctx->caml_frame_descriptors_mask;
asmrun/backtrace.c:      if (ctx->caml_backtrace_pos >= BACKTRACE_BUFFER_SIZE) return ctx;
asmrun/backtrace.c:      ctx->caml_backtrace_buffer[ctx->caml_backtrace_pos++] = (code_t) d;
asmrun/backtrace.c:  for (i = 0; i < ctx->caml_backtrace_pos; i++) {
asmrun/backtrace.c:    extract_location_info_r(ctx, (frame_descr *) (ctx->caml_backtrace_buffer[i]), &li);
asmrun/backtrace.c:  arr = caml_alloc_r(ctx, ctx->caml_backtrace_pos, 0);
asmrun/backtrace.c:  for (i = 0; i < ctx->caml_backtrace_pos; i++) {
asmrun/backtrace.c:    extract_location_info_r(ctx, (frame_descr *) (ctx->caml_backtrace_buffer[i]), &li);
asmrun/major_gc.c:  Assert (ctx->gray_vals_cur == ctx->gray_vals_end);
asmrun/major_gc.c:  if (ctx->gray_vals_size < ctx->caml_stat_heap_size / 128){
asmrun/major_gc.c:                     (intnat) ctx->gray_vals_size * sizeof (value) / 512);
asmrun/major_gc.c:    new = (value *) realloc ((char *) ctx->gray_vals,
asmrun/major_gc.c:                             2 * ctx->gray_vals_size * sizeof (value));
asmrun/major_gc.c:      ctx->gray_vals_cur = ctx->gray_vals;
asmrun/major_gc.c:      ctx->heap_is_pure = 0;
asmrun/major_gc.c:      ctx->gray_vals = new;
asmrun/major_gc.c:      ctx->gray_vals_cur = ctx->gray_vals + ctx->gray_vals_size;
asmrun/major_gc.c:      ctx->gray_vals_size *= 2;
asmrun/major_gc.c:      ctx->gray_vals_end = ctx->gray_vals + ctx->gray_vals_size;
asmrun/major_gc.c:    ctx->gray_vals_cur = ctx->gray_vals + ctx->gray_vals_size / 2;
asmrun/major_gc.c:    ctx->heap_is_pure = 0;
asmrun/major_gc.c:        *ctx->gray_vals_cur++ = v;
asmrun/major_gc.c:        if (ctx->gray_vals_cur >= ctx->gray_vals_end) realloc_gray_vals_r (ctx);
asmrun/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
asmrun/major_gc.c:  Assert (ctx->gray_vals_cur == gray_vals);
asmrun/major_gc.c:  ctx->caml_gc_phase = Phase_mark;
asmrun/major_gc.c:  ctx->caml_gc_subphase = Subphase_main;
asmrun/major_gc.c:  ctx->markhp = NULL;
asmrun/major_gc.c:  ++ ctx->major_gc_counter;
asmrun/major_gc.c:  caml_gc_message (0x40, "Subphase = %ld\n", ctx->caml_gc_subphase);
asmrun/major_gc.c:  gray_vals_ptr = ctx->gray_vals_cur;
asmrun/major_gc.c:    if (gray_vals_ptr > ctx->gray_vals){
asmrun/major_gc.c:              if (gray_vals_ptr >= ctx->gray_vals_end) {
asmrun/major_gc.c:                ctx->gray_vals_cur = gray_vals_ptr;
asmrun/major_gc.c:                gray_vals_ptr = ctx->gray_vals_cur;
asmrun/major_gc.c:    }else if (ctx->markhp != NULL){       // phc - keep processing current chunk
asmrun/major_gc.c:      if (ctx->markhp == ctx->limit){     // this chunk is done with marking
asmrun/major_gc.c:        ctx->chunk = Chunk_next (ctx->chunk);
asmrun/major_gc.c:        if (ctx->chunk == NULL){          // no more chunk to mark
asmrun/major_gc.c:          ctx->markhp = NULL;
asmrun/major_gc.c:          ctx->markhp = ctx->chunk;       // next chunk
asmrun/major_gc.c:          ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
asmrun/major_gc.c:        if (Is_gray_val (Val_hp (ctx->markhp))){
asmrun/major_gc.c:          Assert (gray_vals_ptr == ctx->gray_vals);
asmrun/major_gc.c:          *gray_vals_ptr++ = Val_hp (ctx->markhp);
asmrun/major_gc.c:        ctx->markhp += Bhsize_hp (ctx->markhp); // mark next block as todo
asmrun/major_gc.c:      ctx->chunk = ctx->caml_heap_start;
asmrun/major_gc.c:      ctx->markhp = ctx->chunk;
asmrun/major_gc.c:      ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
asmrun/major_gc.c:      switch (ctx->caml_gc_subphase){
asmrun/major_gc.c:        ctx->caml_gc_subphase = Subphase_weak1;
asmrun/major_gc.c:        ctx->weak_prev = &(ctx->caml_weak_list_head);
asmrun/major_gc.c:        cur = *(ctx->weak_prev);
asmrun/major_gc.c:          ctx->weak_prev = &Field (cur, 0);
asmrun/major_gc.c:          ctx->gray_vals_cur = gray_vals_ptr;
asmrun/major_gc.c:          gray_vals_ptr = ctx->gray_vals_cur;
asmrun/major_gc.c:          ctx->caml_gc_subphase = Subphase_weak2;
asmrun/major_gc.c:          ctx->weak_prev = &(ctx->caml_weak_list_head);
asmrun/major_gc.c:        cur = *ctx->weak_prev;
asmrun/major_gc.c:            *ctx->weak_prev = Field (cur, 0);
asmrun/major_gc.c:            // ctx->weak_prev==&ptr
asmrun/major_gc.c:            ctx->weak_prev = &Field (cur, 0);
asmrun/major_gc.c:          ctx->caml_gc_subphase = Subphase_final;
asmrun/major_gc.c:        ctx->gray_vals_cur = gray_vals_ptr;
asmrun/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->caml_heap_start;
asmrun/major_gc.c:        ctx->caml_gc_phase = Phase_sweep;
asmrun/major_gc.c:        ctx->chunk = ctx->caml_heap_start;
asmrun/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->chunk;
asmrun/major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
asmrun/major_gc.c:        ctx->caml_fl_size_at_phase_change = ctx->caml_fl_cur_size;
asmrun/major_gc.c:  ctx->gray_vals_cur = gray_vals_ptr;
asmrun/major_gc.c:    if (ctx->caml_gc_sweep_hp < ctx->limit){
asmrun/major_gc.c:      hp = ctx->caml_gc_sweep_hp;
asmrun/major_gc.c:      ctx->caml_gc_sweep_hp += Bhsize_hd (hd);
asmrun/major_gc.c:        ctx->caml_gc_sweep_hp = caml_fl_merge_block_r (ctx, Bp_hp (hp));
asmrun/major_gc.c:        ctx->caml_fl_merge = Bp_hp (hp);
asmrun/major_gc.c:      Assert (ctx->caml_gc_sweep_hp <= ctx->limit);
asmrun/major_gc.c:      ctx->chunk = Chunk_next (ctx->chunk);
asmrun/major_gc.c:      if (ctx->chunk == NULL){
asmrun/major_gc.c:        ++ (ctx->caml_stat_major_collections);
asmrun/major_gc.c:        ctx->caml_gc_phase = Phase_idle;
asmrun/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->chunk;
asmrun/major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
asmrun/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
asmrun/major_gc.c:  p = (double) ctx->caml_allocated_words * 3.0 * (100 + ctx->caml_percent_free)
asmrun/major_gc.c:      / Wsize_bsize (ctx->caml_stat_heap_size) / ctx->caml_percent_free / 2.0;
asmrun/major_gc.c:  if (ctx->caml_dependent_size > 0){
asmrun/major_gc.c:    dp = (double) ctx->caml_dependent_allocated * (100 + ctx->caml_percent_free)
asmrun/major_gc.c:         / ctx->caml_dependent_size / ctx->caml_percent_free;
asmrun/major_gc.c:  if (p < ctx->caml_extra_heap_resources) p = ctx->caml_extra_heap_resources;
asmrun/major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
asmrun/major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 250
asmrun/major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 5 / 3);
asmrun/major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
asmrun/major_gc.c:    Assert (ctx->caml_gc_phase == Phase_sweep);
asmrun/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) caml_compact_heap_maybe_r (ctx);
asmrun/major_gc.c:  ctx->caml_stat_major_words += ctx->caml_allocated_words;
asmrun/major_gc.c:  ctx->caml_allocated_words = 0;
asmrun/major_gc.c:  ctx->caml_dependent_allocated = 0;
asmrun/major_gc.c:  ctx->caml_extra_heap_resources = 0.0;
asmrun/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
asmrun/major_gc.c:  while (ctx->caml_gc_phase == Phase_mark) mark_slice_r (ctx, LONG_MAX);
asmrun/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_sweep);
asmrun/major_gc.c:  while (ctx->caml_gc_phase == Phase_sweep) sweep_slice_r (ctx, LONG_MAX);
asmrun/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
asmrun/major_gc.c:  ctx->caml_stat_major_words += ctx->caml_allocated_words;
asmrun/major_gc.c:  ctx->caml_allocated_words = 0;
asmrun/major_gc.c:  if (result < ctx->caml_major_heap_increment){
asmrun/major_gc.c:    result = ctx->caml_major_heap_increment;
asmrun/major_gc.c:  ctx->caml_stat_heap_size = clip_heap_chunk_size (heap_size);
asmrun/major_gc.c:  ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
asmrun/major_gc.c:  Assert (ctx->caml_stat_heap_size % Page_size == 0);
asmrun/major_gc.c:  ctx->caml_heap_start = (char *) caml_alloc_for_heap (ctx->caml_stat_heap_size);
asmrun/major_gc.c:  if (ctx->caml_heap_start == NULL)
asmrun/major_gc.c:  Chunk_next (ctx->caml_heap_start) = NULL;
asmrun/major_gc.c:  if (caml_page_table_add(In_heap, ctx->caml_heap_start,
asmrun/major_gc.c:                          ctx->caml_heap_start + ctx->caml_stat_heap_size) != 0) {
asmrun/major_gc.c:  caml_make_free_blocks_r (ctx, (value *) ctx->caml_heap_start,
asmrun/major_gc.c:                         Wsize_bsize (ctx->caml_stat_heap_size), 1, Caml_white);
asmrun/major_gc.c:  ctx->caml_gc_phase = Phase_idle;
asmrun/major_gc.c:  ctx->gray_vals_size = 2048;
asmrun/major_gc.c:  ctx->gray_vals = (value *) malloc (ctx->gray_vals_size * sizeof (value));
asmrun/major_gc.c:  if (ctx->gray_vals == NULL)
asmrun/major_gc.c:  ctx->gray_vals_cur = ctx->gray_vals;
asmrun/major_gc.c:  ctx->gray_vals_end = ctx->gray_vals + ctx->gray_vals_size;
asmrun/major_gc.c:  ctx->heap_is_pure = 1;
asmrun/major_gc.c:  ctx->caml_allocated_words = 0;
asmrun/major_gc.c:  ctx->caml_extra_heap_resources = 0.0;
asmrun/callback.c:  for (nv = ctx->named_value_table[h]; nv != NULL; nv = nv->next) {
asmrun/callback.c:  nv->next = ctx->named_value_table[h];
asmrun/callback.c:  ctx->named_value_table[h] = nv;
asmrun/callback.c:  for (nv = ctx->named_value_table[hash_value_name(name)];
asmrun/roots.c:  ctx->frametables = cons(table,ctx->frametables);
asmrun/roots.c:  if (NULL != ctx->caml_frame_descriptors) {
asmrun/roots.c:    caml_stat_free(ctx->caml_frame_descriptors);
asmrun/roots.c:    ctx->caml_frame_descriptors = NULL;
asmrun/roots.c:  if (!ctx->caml_frame_descr_inited) {
asmrun/roots.c:    ctx->caml_frame_descr_inited = 1;
asmrun/roots.c:  iter_list(ctx->frametables,lnk) {
asmrun/roots.c:  ctx->caml_frame_descriptors =
asmrun/roots.c:  for (i = 0; i < tblsize; i++) ctx->caml_frame_descriptors[i] = NULL;
asmrun/roots.c:  ctx->caml_frame_descriptors_mask = tblsize - 1;
asmrun/roots.c:  iter_list(ctx->frametables,lnk) {
asmrun/roots.c:      while (ctx->caml_frame_descriptors[h] != NULL) {
asmrun/roots.c:        int _mask = ctx->caml_frame_descriptors_mask;
asmrun/roots.c:      ctx->caml_frame_descriptors[h] = d;
asmrun/roots.c:  ctx->caml_dyn_globals = cons((void*) v,ctx->caml_dyn_globals);
asmrun/roots.c: // printf("ctx = %p ctx->caml_globals = %p ctx->caml_globals_scanned = %d ctx->caml_globals_inited = %d\n", ctx, ctx->caml_globals, ctx->caml_globals_scanned, ctx->caml_globals_inited);
asmrun/roots.c:  for (i = ctx->caml_globals_scanned;
asmrun/roots.c:       i <= ctx->caml_globals_inited && ctx->caml_globals[i] != 0;
asmrun/roots.c:    glob = ctx->caml_globals[i];
asmrun/roots.c:   // printf("ctx->caml_globals[%d] = %p wosize = %d\n", i, glob, Wosize_val(glob));
asmrun/roots.c:  ctx->caml_globals_scanned = ctx->caml_globals_inited;
asmrun/roots.c: // printf("ctx->caml_dyn_globals = %p\n", ctx->caml_dyn_globals);
asmrun/roots.c:  iter_list(ctx->caml_dyn_globals, lnk) {
asmrun/roots.c:   // printf("iter(ctx->caml_dyn_globals) = %p wosize = %d\n", glob, Wosize_val(glob));
asmrun/roots.c: // printf("ctx->caml_bottom_of_stack          = %p\n", ctx->caml_bottom_of_stack);
asmrun/roots.c: // printf("ctx->caml_last_return_address = %p\n", ctx->caml_last_return_address);
asmrun/roots.c: // printf("ctx->caml_gc_regs             = %p\n", ctx->caml_gc_regs);
asmrun/roots.c:  if (ctx->caml_frame_descriptors == NULL) caml_init_frame_descriptors_r(ctx);
asmrun/roots.c: // printf("ctx->caml_frame_descriptors   = %p\n", ctx->caml_frame_descriptors);
asmrun/roots.c: // printf("ctx->caml_frame_descriptors_mask = %p\n", ctx->caml_frame_descriptors_mask);
asmrun/roots.c:  sp = ctx->caml_bottom_of_stack;
asmrun/roots.c:  retaddr = ctx->caml_last_return_address;
asmrun/roots.c:  regs = ctx->caml_gc_regs;
asmrun/roots.c:        d = ctx->caml_frame_descriptors[h];
asmrun/roots.c:        h = (h+1) & (ctx->caml_frame_descriptors_mask);
asmrun/roots.c: // printf("ctx->caml_local_roots  = %p\n", ctx->caml_local_roots);
asmrun/roots.c:  for (lr = ctx->caml_local_roots; lr != NULL; lr = lr->next) {
asmrun/roots.c:       // printf("iter(ctx->caml_local_roots)[i][j] = %p[%d][%d] = %p, v = %p \n", lr, i, j, root, *(value*)root);
asmrun/roots.c:  if (ctx->caml_scan_roots_hook != NULL) (*ctx->caml_scan_roots_hook)(ctx, &caml_oldify_one_r);
asmrun/roots.c:  for (i = 0; ctx->caml_globals[i] != 0; i++) {
asmrun/roots.c:    glob = ctx->caml_globals[i];
asmrun/roots.c:  iter_list(ctx->caml_dyn_globals, lnk) {
asmrun/roots.c:  if (ctx->caml_frame_descriptors == NULL) caml_init_frame_descriptors_r(ctx);
asmrun/roots.c:  caml_do_local_roots_r (ctx, f, ctx->caml_bottom_of_stack, ctx->caml_last_return_address,
asmrun/roots.c:                      ctx->caml_gc_regs, ctx->caml_local_roots);
asmrun/roots.c:  if (ctx->caml_scan_roots_hook != NULL) (*ctx->caml_scan_roots_hook)(ctx, f);
asmrun/roots.c:        d = ctx->caml_frame_descriptors[h];
asmrun/roots.c:        h = (h+1) & (ctx->caml_frame_descriptors_mask);
asmrun/roots.c:  sz = (value *) ctx->caml_top_of_stack - (value *) ctx->caml_bottom_of_stack;
asmrun/roots.c:  if (ctx->caml_stack_usage_hook != NULL)
asmrun/roots.c:    sz += (*ctx->caml_stack_usage_hook)(ctx);
asmrun/compact.c:  char *ch = ctx->caml_heap_start;
asmrun/compact.c:  ctx->compact_fl = ctx->caml_heap_start;
asmrun/compact.c:  while (Chunk_size (ctx->compact_fl) - Chunk_alloc (ctx->compact_fl) <= Bhsize_wosize (3)
asmrun/compact.c:         && Chunk_size (Chunk_next (ctx->compact_fl))
asmrun/compact.c:            - Chunk_alloc (Chunk_next (ctx->compact_fl))
asmrun/compact.c:    ctx->compact_fl = Chunk_next (ctx->compact_fl);
asmrun/compact.c:  chunk = ctx->compact_fl;
asmrun/compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
asmrun/compact.c:    ch = ctx->caml_heap_start;
asmrun/compact.c:    ch = ctx->caml_heap_start;
asmrun/compact.c:      value *pp = &ctx->caml_weak_list_head;
asmrun/compact.c:          if (Field (p,i) != ctx->caml_weak_none){
asmrun/compact.c:    ch = ctx->caml_heap_start;
asmrun/compact.c:    ch = ctx->caml_heap_start;
asmrun/compact.c:    ch = ctx->caml_heap_start;
asmrun/compact.c:    wanted = ctx->caml_percent_free * (live / 100 + 1);
asmrun/compact.c:    ch = ctx->caml_heap_start;
asmrun/compact.c:    ch = ctx->caml_heap_start;
asmrun/compact.c:  ++ ctx->caml_stat_compactions;
asmrun/compact.c:  live = Wsize_bsize (ctx->caml_stat_heap_size) - ctx->caml_fl_cur_size;
asmrun/compact.c:  target_words = live + ctx->caml_percent_free * (live / 100 + 1)
asmrun/compact.c:  if (target_size < ctx->caml_stat_heap_size / 2){
asmrun/compact.c:    Chunk_next (chunk) = ctx->caml_heap_start;
asmrun/compact.c:    ctx->caml_heap_start = chunk;
asmrun/compact.c:    ++ ctx->caml_stat_heap_chunks;
asmrun/compact.c:    ctx->caml_stat_heap_size += Chunk_size (chunk);
asmrun/compact.c:    if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
asmrun/compact.c:      ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
asmrun/compact.c:    Assert (ctx->caml_stat_heap_chunks == 1);
asmrun/compact.c:    Assert (Chunk_next (ctx->caml_heap_start) == NULL);
asmrun/compact.c:    Assert (ctx->caml_stat_heap_size == Chunk_size (chunk));
asmrun/compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
asmrun/compact.c:  if (ctx->caml_percent_max >= 1000000) return;
asmrun/compact.c:  if (ctx->caml_stat_major_collections < 3) return;
asmrun/compact.c:  fw = 3.0 * ctx->caml_fl_cur_size - 2.0 * ctx->caml_fl_size_at_phase_change;
asmrun/compact.c:  if (fw < 0) fw = ctx->caml_fl_cur_size;
asmrun/compact.c:  if (fw >= Wsize_bsize (ctx->caml_stat_heap_size)){
asmrun/compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
asmrun/compact.c:                   (uintnat) ctx->caml_fl_size_at_phase_change);
asmrun/compact.c:  if (fp >= ctx->caml_percent_max){
asmrun/compact.c:    fw = ctx->caml_fl_cur_size;
asmrun/compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
asmrun/startup.p.c:  ctx->caml_globals = malloc(sizeof(value)*(i+1));
asmrun/startup.p.c:  ctx->caml_globals_len = i;
asmrun/startup.p.c:// printf("ctx->caml_globals = %p\n", ctx->caml_globals);
asmrun/startup.p.c:    *(ctx->caml_globals+i) = w;
asmrun/startup.p.c:  //  printf("ctx->caml_globals[%d] = %p wosize = %d\n", i, w, Wosize_val(w));
asmrun/startup.p.c:  *(ctx->caml_globals+i) = 0;
asmrun/io.c:  channel->next = ctx->caml_all_opened_channels;
asmrun/io.c:  if (ctx->caml_all_opened_channels != NULL)
asmrun/io.c:    ctx->caml_all_opened_channels->prev = channel;
asmrun/io.c:  ctx->caml_all_opened_channels = channel;
asmrun/io.c:    Assert (channel == ctx->caml_all_opened_channels);
asmrun/io.c:    ctx->caml_all_opened_channels = ctx->caml_all_opened_channels->next;
asmrun/io.c:    if (ctx->caml_all_opened_channels != NULL)
asmrun/io.c:      ctx->caml_all_opened_channels->prev = NULL;
asmrun/io.c:  for (channel = ctx->caml_all_opened_channels;
asmrun/weak.c:  for (i = 1; i < size; i++) Field (res, i) = ctx->caml_weak_none;
asmrun/weak.c:  Field (res, 0) = ctx->caml_weak_list_head;
asmrun/weak.c:  ctx->caml_weak_list_head = res;
asmrun/weak.c:      if (ctx->caml_weak_ref_table.ptr >= ctx->caml_weak_ref_table.limit){
asmrun/weak.c:        CAMLassert (ctx->caml_weak_ref_table.ptr == ctx->caml_weak_ref_table.limit);
asmrun/weak.c:        caml_realloc_ref_table_r (ctx, &(ctx->caml_weak_ref_table));
asmrun/weak.c:      *(ctx->caml_weak_ref_table.ptr++) = &Field (ar, offset);
asmrun/weak.c:    Field (ar, offset) = ctx->caml_weak_none;
asmrun/weak.c:  if (Field (ar, offset) == ctx->caml_weak_none){
asmrun/weak.c:    if (ctx->caml_gc_phase == Phase_mark && Is_block (elt) && Is_in_heap (elt)){
asmrun/weak.c:  if (v == ctx->caml_weak_none) CAMLreturn (None_val);
asmrun/weak.c:    if (v == ctx->caml_weak_none) CAMLreturn (None_val);
asmrun/weak.c:        if (ctx->caml_gc_phase == Phase_mark && Is_block (f) && Is_in_heap (f)){
asmrun/weak.c:  return Val_bool (Field (ar, offset) != ctx->caml_weak_none);
asmrun/weak.c:  if (ctx->caml_gc_phase == Phase_mark && ctx->caml_gc_subphase == Subphase_weak1){
asmrun/weak.c:      if (v != ctx->caml_weak_none && Is_block (v) && Is_in_heap (v)
asmrun/weak.c:        Field (ars, offset_s + i) = ctx->caml_weak_none;
asmrun/custom.c:  l->next = ctx->custom_ops_table;
asmrun/custom.c:  ctx->custom_ops_table = l;
asmrun/custom.c:  for (l = ctx->custom_ops_table; l != NULL; l = l->next)
asmrun/custom.c:  for (l = ctx->custom_ops_final_table; l != NULL; l = l->next)
asmrun/custom.c:  l->next = ctx->custom_ops_final_table;
asmrun/custom.c:  ctx->custom_ops_final_table = l;
asmrun/context.c:  ctx->major_gc_counter = 0;
asmrun/context.c:  return Val_int(ctx->count_id);
asmrun/context.c:  printf("caml_young_ptr     : %p\n", (void*)ctx->caml_young_ptr);
asmrun/context.c:  printf("caml_young_limit   : %p\n", (void*)ctx->caml_young_limit);
asmrun/context.c:  printf("caml_young_base    : %p\n", (void*)ctx->caml_young_base);
asmrun/context.c:  printf("caml_young_start   : %p\n", (void*)ctx->caml_young_start);
asmrun/context.c:  printf("caml_young_end     : %p\n", (void*)ctx->caml_young_end);
asmrun/context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
asmrun/context.c:           ctx->caml_young_ptr, caml_young_ptr);
asmrun/context.c:  if (ctx->caml_young_limit!=caml_young_limit)
asmrun/context.c:           ctx->caml_young_limit, caml_young_limit);
asmrun/context.c:  if (ctx->caml_young_base!=caml_young_base)
asmrun/context.c:           ctx->caml_young_base, caml_young_base);
asmrun/context.c:  if (ctx->caml_young_start!=caml_young_start)
asmrun/context.c:           ctx->caml_young_start, caml_young_start);
asmrun/context.c:  if (ctx->caml_young_end!=caml_young_end)
asmrun/context.c:           ctx->caml_young_end, caml_young_end);
asmrun/context.c:  ctx->caml_young_ptr     = caml_young_ptr;
asmrun/context.c:  ctx->caml_young_limit   = caml_young_limit;
asmrun/context.c:  ctx->caml_young_base    = caml_young_base;
asmrun/context.c:  ctx->caml_young_start   = caml_young_start;
asmrun/context.c:  ctx->caml_young_end     = caml_young_end;
asmrun/context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
asmrun/context.c:           ctx->caml_young_ptr, caml_young_ptr);
asmrun/context.c:  if (ctx->caml_young_limit!=caml_young_limit)
asmrun/context.c:           ctx->caml_young_limit, caml_young_limit);
asmrun/context.c:  if (ctx->caml_young_base!=caml_young_base)
asmrun/context.c:           ctx->caml_young_base, caml_young_base);
asmrun/context.c:  if (ctx->caml_young_start!=caml_young_start)
asmrun/context.c:           ctx->caml_young_start, caml_young_start);
asmrun/context.c:  if (ctx->caml_young_end!=caml_young_end)
asmrun/context.c:           ctx->caml_young_end, caml_young_end);
asmrun/context.c:  caml_young_ptr     = ctx->caml_young_ptr; 
asmrun/context.c:  caml_young_limit   = ctx->caml_young_limit;
asmrun/context.c:  caml_young_base    = ctx->caml_young_base;
asmrun/context.c:  caml_young_start   = ctx->caml_young_start;  
asmrun/context.c:  caml_young_end     = ctx->caml_young_end;
asmrun/freelist.c:    ctx->caml_fl_cur_size -= Whsize_hd (h);
asmrun/freelist.c:    if (ctx->caml_fl_merge == cur) ctx->caml_fl_merge = prev;
asmrun/freelist.c:      if (flpi + 1 < ctx->flp_size && ctx->flp[flpi + 1] == cur){
asmrun/freelist.c:        ctx->flp[flpi + 1] = prev;
asmrun/freelist.c:      }else if (flpi == ctx->flp_size - 1){
asmrun/freelist.c:        ctx->beyond = (prev == ctx->fl_head) ? NULL : prev;
asmrun/freelist.c:        -- ctx->flp_size;
asmrun/freelist.c:    ctx->caml_fl_cur_size -= wh_sz;
asmrun/freelist.c:  if (policy == Policy_next_fit) ctx->fl_prev = prev;
asmrun/freelist.c:                                  Assert (ctx->fl_prev != NULL);
asmrun/freelist.c:    prev = ctx->fl_prev;
asmrun/freelist.c:    ctx->fl_last = prev;
asmrun/freelist.c:    prev = ctx->fl_head;
asmrun/freelist.c:    while (prev != ctx->fl_prev){
asmrun/freelist.c:    for (i = 0; i < ctx->flp_size; i++){
asmrun/freelist.c:      sz = Wosize_bp (Next (ctx->flp[i]));
asmrun/freelist.c:                                   i, ctx->flp[i], Next(ctx->flp[i]));
asmrun/freelist.c:    if (ctx->flp_size == 0){ 
asmrun/freelist.c:      prev = ctx->fl_head;
asmrun/freelist.c:      prev = Next (ctx->flp[ctx->flp_size - 1]);
asmrun/freelist.c:      if (ctx->beyond != NULL) prev = ctx->beyond;
asmrun/freelist.c:    while (ctx->flp_size < FLP_MAX){
asmrun/freelist.c:        ctx->fl_last = prev;
asmrun/freelist.c:        ctx->beyond = (prev==ctx->fl_head) ? NULL : prev;
asmrun/freelist.c:          ctx->flp[ctx->flp_size] = prev;
asmrun/freelist.c:          ++ ctx->flp_size;
asmrun/freelist.c:            ctx->beyond = cur;
asmrun/freelist.c:            i = ctx->flp_size - 1;
asmrun/freelist.c:                                       ctx->flp_size - 1, prev, cur);
asmrun/freelist.c:    ctx->beyond = cur;
asmrun/freelist.c:    if (ctx->beyond != NULL){
asmrun/freelist.c:      prev = ctx->beyond;
asmrun/freelist.c:      prev = ctx->flp[ctx->flp_size - 1];
asmrun/freelist.c:    prevsz = Wosize_bp (Next (ctx->flp[FLP_MAX-1]));
asmrun/freelist.c:        ctx->beyond = cur;
asmrun/freelist.c:                                 ctx->flp_size, prev, cur);
asmrun/freelist.c:    ctx->fl_last = prev;
asmrun/freelist.c:    Assert (0 <= i && i < ctx->flp_size + 1);
asmrun/freelist.c:    if (i < ctx->flp_size){
asmrun/freelist.c:        prevsz = Wosize_bp (Next (ctx->flp[i-1]));
asmrun/freelist.c:      if (i == ctx->flp_size - 1){
asmrun/freelist.c:        if (Wosize_bp (Next (ctx->flp[i])) <= prevsz){
asmrun/freelist.c:          ctx->beyond = Next (ctx->flp[i]);
asmrun/freelist.c:          -- ctx->flp_size;
asmrun/freelist.c:          ctx->beyond = NULL;
asmrun/freelist.c:        prev = ctx->flp[i];
asmrun/freelist.c:        while (prev != ctx->flp[i+1]){
asmrun/freelist.c:        if (FLP_MAX >= ctx->flp_size + j - 1){
asmrun/freelist.c:            memmove (&(ctx->flp[i+j]), 
asmrun/freelist.c:                     &(ctx->flp[i+1]), 
asmrun/freelist.c:                     sizeof (block *) * (ctx->flp_size-i-1));
asmrun/freelist.c:             memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
asmrun/freelist.c:          ctx->flp_size += j - 1;
asmrun/freelist.c:              memmove (&(ctx->flp[i+j]), 
asmrun/freelist.c:                       &(ctx->flp[i+1]), 
asmrun/freelist.c:              memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
asmrun/freelist.c:              memmove (&(ctx->flp[i]), &buf[0], 
asmrun/freelist.c:          ctx->flp_size = FLP_MAX - 1;
asmrun/freelist.c:          ctx->beyond = Next (ctx->flp[FLP_MAX - 1]);
asmrun/freelist.c:  ctx->last_fragment = NULL;
asmrun/freelist.c:  ctx->caml_fl_merge = ctx->fl_head;
asmrun/freelist.c:  if (changed == ctx->fl_head){
asmrun/freelist.c:    ctx->flp_size = 0;
asmrun/freelist.c:    ctx->beyond = NULL;
asmrun/freelist.c:    while (ctx->flp_size > 0 && Next (ctx->flp[ctx->flp_size - 1]) >= changed) 
asmrun/freelist.c:      -- ctx->flp_size;
asmrun/freelist.c:    if (ctx->beyond >= changed) 
asmrun/freelist.c:      ctx->beyond = NULL;
asmrun/freelist.c:  Next (ctx->fl_head) = NULL;
asmrun/freelist.c:    ctx->fl_prev = ctx->fl_head;
asmrun/freelist.c:    truncate_flp_r (ctx, ctx->fl_head);
asmrun/freelist.c:  ctx->caml_fl_cur_size = 0;
asmrun/freelist.c://    last_fragment or Next(ctx->caml_fl_merge).
asmrun/freelist.c:  ctx->caml_fl_cur_size += Whsize_hd (hd);
asmrun/freelist.c:  prev = ctx->caml_fl_merge;
asmrun/freelist.c:  Assert (prev < bp || prev == ctx->fl_head);
asmrun/freelist.c:  if (ctx->last_fragment == Hp_bp (bp)){
asmrun/freelist.c:      bp = ctx->last_fragment;
asmrun/freelist.c:      ctx->caml_fl_cur_size += Whsize_wosize (0);
asmrun/freelist.c:      if (policy == Policy_next_fit && ctx->fl_prev == cur) ctx->fl_prev = prev;
asmrun/freelist.c:    Assert (ctx->caml_fl_merge == prev);
asmrun/freelist.c:    ctx->caml_fl_merge = bp;
asmrun/freelist.c:    ctx->last_fragment = bp;
asmrun/freelist.c:    ctx->caml_fl_cur_size -= Whsize_wosize (0);
asmrun/freelist.c:                                                  Assert (ctx->fl_last != NULL);
asmrun/freelist.c:                                           Assert (Next (ctx->fl_last) == NULL);
asmrun/freelist.c:  ctx->caml_fl_cur_size += Whsize_bp (bp);
asmrun/freelist.c:  if (bp > ctx->fl_last){
asmrun/freelist.c:    Next (ctx->fl_last) = bp;
asmrun/freelist.c:    if (ctx->fl_last==ctx->caml_fl_merge && bp<ctx->caml_gc_sweep_hp){
asmrun/freelist.c:      ctx->caml_fl_merge = (char *) Field (bp, 1);
asmrun/freelist.c:    if (policy==Policy_first_fit && ctx->flp_size<FLP_MAX){
asmrun/freelist.c:      ctx->flp [ctx->flp_size++] = ctx->fl_last;
asmrun/freelist.c:    prev = ctx->fl_head;
asmrun/freelist.c:    while (cur != NULL && cur < bp){   Assert (prev < bp || prev == ctx->fl_head);
asmrun/freelist.c:    }                                  Assert (prev < bp || prev == ctx->fl_head);
asmrun/freelist.c:    if (prev == ctx->caml_fl_merge && bp < ctx->caml_gc_sweep_hp){
asmrun/freelist.c:      ctx->caml_fl_merge = (char *) Field (bp, 1);
asmrun/freelist.c:    ctx->fl_prev = ctx->fl_head;
asmrun/freelist.c:    ctx->flp_size = 0;
asmrun/freelist.c:    ctx->beyond = NULL;
asmrun/minor_gc.c://  if (ctx->caml_young_ptr != ctx->caml_young_end) caml_minor_collection_r (ctx);
asmrun/minor_gc.c:                                    Assert (ctx->caml_young_ptr == ctx->caml_young_end);
asmrun/minor_gc.c:  if (ctx->caml_young_start != NULL){
asmrun/minor_gc.c:    caml_page_table_remove(In_young, ctx->caml_young_start, ctx->caml_young_end);
asmrun/minor_gc.c:    free (ctx->caml_young_base);
asmrun/minor_gc.c:  ctx->caml_young_base = new_heap_base;
asmrun/minor_gc.c:  ctx->caml_young_start = new_heap;
asmrun/minor_gc.c:  ctx->caml_young_end = new_heap + size;
asmrun/minor_gc.c:  ctx->caml_young_limit = ctx->caml_young_start;
asmrun/minor_gc.c:  ctx->caml_young_ptr = ctx->caml_young_end;
asmrun/minor_gc.c:  ctx->caml_minor_heap_size = size;
asmrun/minor_gc.c:  reset_table (&(ctx->caml_ref_table));
asmrun/minor_gc.c:  reset_table (&(ctx->caml_weak_ref_table));
asmrun/minor_gc.c:    Assert (Hp_val (v) >= ctx->caml_young_ptr);
asmrun/minor_gc.c:          Field (result, 1) = ctx->oldify_todo_list;    /* Add this block */
asmrun/minor_gc.c:          ctx->oldify_todo_list = v;                    /*  to the "to do" list. */
asmrun/minor_gc.c:  while (ctx->oldify_todo_list != 0){
asmrun/minor_gc.c:    v = ctx->oldify_todo_list;                /* Get the head. */
asmrun/minor_gc.c:    ctx->oldify_todo_list = Field (new_v, 1); /* Remove from list. */
asmrun/minor_gc.c:  if (ctx->caml_young_ptr != ctx->caml_young_end){
asmrun/minor_gc.c:    ctx->caml_in_minor_collection = 1;
asmrun/minor_gc.c:    for (r = ctx->caml_ref_table.base; r < ctx->caml_ref_table.ptr; r++){
asmrun/minor_gc.c:    for (r = ctx->caml_weak_ref_table.base; r < ctx->caml_weak_ref_table.ptr; r++){
asmrun/minor_gc.c:    if (ctx->caml_young_ptr < ctx->caml_young_start)
asmrun/minor_gc.c:      ctx->caml_young_ptr = ctx->caml_young_start;
asmrun/minor_gc.c:    ctx->caml_stat_minor_words += Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
asmrun/minor_gc.c:    ctx->caml_young_ptr = ctx->caml_young_end;
asmrun/minor_gc.c:    ctx->caml_young_limit = ctx->caml_young_start;
asmrun/minor_gc.c:    clear_table (&(ctx->caml_ref_table));
asmrun/minor_gc.c:    clear_table (&(ctx->caml_weak_ref_table));
asmrun/minor_gc.c:    ctx->caml_in_minor_collection = 0;
asmrun/minor_gc.c:    for (p = (value *) ctx->caml_young_start; p < (value *) ctx->caml_young_end; ++p){
asmrun/minor_gc.c:  intnat prev_alloc_words = ctx->caml_allocated_words;
asmrun/minor_gc.c:  ctx->caml_stat_promoted_words += ctx->caml_allocated_words - prev_alloc_words;
asmrun/minor_gc.c:  ++ ctx->caml_stat_minor_collections;
asmrun/minor_gc.c:  ctx->caml_force_major_slice = 0;
asmrun/minor_gc.c:  if (ctx->caml_force_major_slice) caml_minor_collection_r(ctx);
asmrun/minor_gc.c:    caml_alloc_table (tbl, ctx->caml_minor_heap_size / sizeof (value) / 8, 256);
asmrun/minor_gc.c:                                             Assert (ctx->caml_force_major_slice);
asmrun/finalise.c:  if (ctx->to_do_tl == NULL){
asmrun/finalise.c:    ctx->to_do_hd = result;
asmrun/finalise.c:    ctx->to_do_tl = result;
asmrun/finalise.c:    Assert (ctx->to_do_tl->next == NULL);
asmrun/finalise.c:    ctx->to_do_tl->next = result;
asmrun/finalise.c:    ctx->to_do_tl = result;
asmrun/finalise.c: * 2. put white blocks in the ctx->final_table into the todo table
asmrun/finalise.c:  Assert (ctx->final_young == ctx->final_old);
asmrun/finalise.c:  for (i = 0; i < ctx->final_old; i++){
asmrun/finalise.c:    Assert (Is_block (ctx->final_table[i].val));
asmrun/finalise.c:    Assert (Is_in_heap (ctx->final_table[i].val));
asmrun/finalise.c:    if (Is_white_val (ctx->final_table[i].val)) ++ todo_count;
asmrun/finalise.c:    for (i = 0; i < ctx->final_old; i++){
asmrun/finalise.c:      Assert (Is_block (ctx->final_table[i].val));
asmrun/finalise.c:      Assert (Is_in_heap (ctx->final_table[i].val));
asmrun/finalise.c:      if (Is_white_val (ctx->final_table[i].val)){
asmrun/finalise.c:        if (Tag_val (ctx->final_table[i].val) == Forward_tag){
asmrun/finalise.c:          Assert (ctx->final_table[i].offset == 0);
asmrun/finalise.c:          fv = Forward_val (ctx->final_table[i].val);
asmrun/finalise.c:            ctx->final_table[i].val = fv;
asmrun/finalise.c:            if (Is_block (ctx->final_table[i].val)
asmrun/finalise.c:                && Is_in_heap (ctx->final_table[i].val)){
asmrun/finalise.c:        ctx->to_do_tl->item[k++] = ctx->final_table[i];
asmrun/finalise.c:        ctx->final_table[j++] = ctx->final_table[i];
asmrun/finalise.c:    ctx->final_young = ctx->final_old = j;
asmrun/finalise.c:    ctx->to_do_tl->size = k;
asmrun/finalise.c:      CAMLassert (Is_white_val (ctx->to_do_tl->item[i].val));
asmrun/finalise.c:      caml_darken_r (ctx, ctx->to_do_tl->item[i].val, NULL);
asmrun/finalise.c:  if (ctx->running_finalisation_function) return;
asmrun/finalise.c:  if (ctx->to_do_hd != NULL){
asmrun/finalise.c:      while (ctx->to_do_hd != NULL && ctx->to_do_hd->size == 0){
asmrun/finalise.c:        struct to_do *next_hd = ctx->to_do_hd->next;
asmrun/finalise.c:        free (ctx->to_do_hd);
asmrun/finalise.c:        ctx->to_do_hd = next_hd;
asmrun/finalise.c:        if (ctx->to_do_hd == NULL) ctx->to_do_tl = NULL;
asmrun/finalise.c:      if (ctx->to_do_hd == NULL) break;
asmrun/finalise.c:      Assert (ctx->to_do_hd->size > 0);
asmrun/finalise.c:      // phc - for f in reverse(ctx->to_do_head)
asmrun/finalise.c:      -- ctx->to_do_hd->size;
asmrun/finalise.c:      f = ctx->to_do_hd->item[ctx->to_do_hd->size];
asmrun/finalise.c:      ctx->running_finalisation_function = 1;
asmrun/finalise.c:      ctx->running_finalisation_function = 0;
asmrun/finalise.c:  Assert (ctx->final_old == ctx->final_young);
asmrun/finalise.c:  for (i = 0; i < ctx->final_old; i++) 
asmrun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
asmrun/finalise.c:  for (todo = ctx->to_do_hd; todo != NULL; todo = todo->next){
asmrun/finalise.c:  Assert (ctx->final_old == ctx->final_young);
asmrun/finalise.c:  for (i = 0; i < ctx->final_old; i++) 
asmrun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
asmrun/finalise.c://                 ctx->final_old, ctx->final_young);
asmrun/finalise.c:  Assert (ctx->final_old <= ctx->final_young);
asmrun/finalise.c:  for (i = ctx->final_old; i < ctx->final_young; i++){
asmrun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
asmrun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
asmrun/finalise.c:  ctx->final_old = ctx->final_young;
asmrun/finalise.c:  Assert (ctx->final_old <= ctx->final_young);
asmrun/finalise.c:  if (ctx->final_young >= ctx->final_size){
asmrun/finalise.c:    if (ctx->final_table == NULL){
asmrun/finalise.c:      ctx->final_table = caml_stat_alloc (new_size * sizeof (struct final));
asmrun/finalise.c:      Assert (ctx->final_old == 0);
asmrun/finalise.c:      Assert (ctx->final_young == 0);
asmrun/finalise.c:      ctx->final_size = new_size;
asmrun/finalise.c:      uintnat new_size = ctx->final_size * 2;
asmrun/finalise.c:      ctx->final_table = caml_stat_resize (ctx->final_table,
asmrun/finalise.c:      ctx->final_size = new_size;
asmrun/finalise.c:  Assert (ctx->final_young < ctx->final_size);
asmrun/finalise.c:  ctx->final_table[ctx->final_young].fun = f;
asmrun/finalise.c:    ctx->final_table[ctx->final_young].offset = Infix_offset_val (v);
asmrun/finalise.c:    ctx->final_table[ctx->final_young].val = v - Infix_offset_val (v);
asmrun/finalise.c:    ctx->final_table[ctx->final_young].offset = 0;
asmrun/finalise.c:    ctx->final_table[ctx->final_young].val = v;
asmrun/finalise.c:  ++ ctx->final_young;
asmrun/finalise.c:  ctx->running_finalisation_function = 0;
byterun/gc_ctrl.c:  char *chunk = ctx->caml_heap_start, *chunk_end;
byterun/gc_ctrl.c:                  || cur_hp == ctx->caml_gc_sweep_hp);
byterun/gc_ctrl.c:          if (ctx->caml_gc_phase == Phase_sweep && cur_hp >= ctx->caml_gc_sweep_hp){
byterun/gc_ctrl.c:  Assert (heap_chunks == ctx->caml_stat_heap_chunks);
byterun/gc_ctrl.c:          == Wsize_bsize (ctx->caml_stat_heap_size));
byterun/gc_ctrl.c:    double minwords = ctx->caml_stat_minor_words
byterun/gc_ctrl.c:                      + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/gc_ctrl.c:    double prowords = ctx->caml_stat_promoted_words;
byterun/gc_ctrl.c:    double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
byterun/gc_ctrl.c:    intnat mincoll = ctx->caml_stat_minor_collections;
byterun/gc_ctrl.c:    intnat majcoll = ctx->caml_stat_major_collections;
byterun/gc_ctrl.c:    intnat heap_words = Wsize_bsize (ctx->caml_stat_heap_size);
byterun/gc_ctrl.c:    intnat cpct = ctx->caml_stat_compactions;
byterun/gc_ctrl.c:    intnat top_heap_words = Wsize_bsize (ctx->caml_stat_top_heap_size);
byterun/gc_ctrl.c:  double prowords = ctx->caml_stat_promoted_words;
byterun/gc_ctrl.c:  double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
byterun/gc_ctrl.c:  intnat mincoll = ctx->caml_stat_minor_collections;
byterun/gc_ctrl.c:  intnat majcoll = ctx->caml_stat_major_collections;
byterun/gc_ctrl.c:  intnat heap_words = ctx->caml_stat_heap_size / sizeof (value);
byterun/gc_ctrl.c:  intnat top_heap_words = ctx->caml_stat_top_heap_size / sizeof (value);
byterun/gc_ctrl.c:  intnat cpct = ctx->caml_stat_compactions;
byterun/gc_ctrl.c:  intnat heap_chunks = ctx->caml_stat_heap_chunks;
byterun/gc_ctrl.c:  minwords = ctx->caml_stat_minor_words
byterun/gc_ctrl.c:             + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/gc_ctrl.c:  double minwords = ctx->caml_stat_minor_words
byterun/gc_ctrl.c:                    + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/gc_ctrl.c:  double prowords = ctx->caml_stat_promoted_words;
byterun/gc_ctrl.c:  double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
byterun/gc_ctrl.c:  Store_field (res, 0, Val_long (Wsize_bsize (ctx->caml_minor_heap_size)));  /* s */
byterun/gc_ctrl.c:  Store_field (res, 1,Val_long(Wsize_bsize(ctx->caml_major_heap_increment)));/* i */
byterun/gc_ctrl.c:  Store_field (res, 2, Val_long (ctx->caml_percent_free));                   /* o */
byterun/gc_ctrl.c:  Store_field (res, 4, Val_long (ctx->caml_percent_max));                    /* O */
byterun/gc_ctrl.c:  if (newpf != ctx->caml_percent_free){
byterun/gc_ctrl.c:    ctx->caml_percent_free = newpf;
byterun/gc_ctrl.c:    caml_gc_message (0x20, "New space overhead: %d%%\n", ctx->caml_percent_free);
byterun/gc_ctrl.c:  if (newpm != ctx->caml_percent_max){
byterun/gc_ctrl.c:    ctx->caml_percent_max = newpm;
byterun/gc_ctrl.c:    caml_gc_message (0x20, "New max overhead: %d%%\n", ctx->caml_percent_max);
byterun/gc_ctrl.c:  if (newheapincr != ctx->caml_major_heap_increment){
byterun/gc_ctrl.c:    ctx->caml_major_heap_increment = newheapincr;
byterun/gc_ctrl.c:                     ctx->caml_major_heap_increment/1024);
byterun/gc_ctrl.c:  if (newminsize != ctx->caml_minor_heap_size){
byterun/gc_ctrl.c:  fp = 100.0 * ctx->caml_fl_cur_size
byterun/gc_ctrl.c:       / (Wsize_bsize (caml_stat_heap_size) - ctx->caml_fl_cur_size);
byterun/gc_ctrl.c:  if (fp >= ctx->caml_percent_max && ctx->caml_stat_heap_chunks > 1){
byterun/gc_ctrl.c:  ctx->caml_major_heap_increment = Bsize_wsize (norm_heapincr (major_incr));
byterun/gc_ctrl.c:  ctx->caml_percent_free = norm_pfree (percent_fr);
byterun/gc_ctrl.c:                   ctx->caml_minor_heap_size / 1024);
byterun/gc_ctrl.c:  caml_gc_message (0x20, "Initial space overhead: %lu%%\n", ctx->caml_percent_free);
byterun/gc_ctrl.c:                   ctx->caml_major_heap_increment / 1024);
byterun/printexc.c:  saved_backtrace_active = ctx->caml_backtrace_active;
byterun/printexc.c:  saved_backtrace_pos = ctx->caml_backtrace_pos;
byterun/printexc.c:  ctx->caml_backtrace_active = 0;
byterun/printexc.c:  ctx->caml_backtrace_active = saved_backtrace_active;
byterun/printexc.c:  ctx->caml_backtrace_pos = saved_backtrace_pos;
byterun/printexc.c:  if (ctx->caml_backtrace_active
byterun/memory.c:                   (ctx->caml_stat_heap_size + Chunk_size (m)) / 1024);
byterun/memory.c:    char **last = &ctx->caml_heap_start;
byterun/memory.c:    ++ ctx->caml_stat_heap_chunks;
byterun/memory.c:  ctx->caml_stat_heap_size += Chunk_size (m);
byterun/memory.c:  if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
byterun/memory.c:    ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
byterun/memory.c:  over_request = request + request / 100 * ctx->caml_percent_free;
byterun/memory.c:  if (chunk == ctx->caml_heap_start) return;
byterun/memory.c:  ctx->caml_stat_heap_size -= Chunk_size (chunk);
byterun/memory.c:                   (unsigned long) ctx->caml_stat_heap_size / 1024);
byterun/memory.c:  -- ctx->caml_stat_heap_chunks;
byterun/memory.c:  cp = &ctx->caml_heap_start;
byterun/memory.c:  if (ctx->caml_gc_phase == Phase_mark
byterun/memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
byterun/memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
byterun/memory.c:            || (ctx->caml_gc_phase == Phase_sweep
byterun/memory.c:                && (addr)hp < (addr)ctx->caml_gc_sweep_hp));
byterun/memory.c:      if (ctx->caml_in_minor_collection)
byterun/memory.c:  if (ctx->caml_gc_phase == Phase_mark
byterun/memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
byterun/memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
byterun/memory.c:            || (ctx->caml_gc_phase == Phase_sweep
byterun/memory.c:                && (addr)hp < (addr)ctx->caml_gc_sweep_hp));
byterun/memory.c:  ctx->caml_allocated_words += Whsize_wosize (wosize);
byterun/memory.c:  if (ctx->caml_allocated_words > Wsize_bsize (ctx->caml_minor_heap_size)){
byterun/memory.c:  ctx->caml_extra_heap_resources += (double) res / (double) max;
byterun/memory.c:  if (ctx->caml_extra_heap_resources > 1.0){
byterun/memory.c:    ctx->caml_extra_heap_resources = 1.0;
byterun/memory.c:  if (ctx->caml_extra_heap_resources
byterun/memory.c:           > (double) Wsize_bsize (ctx->caml_minor_heap_size) / 2.0
byterun/memory.c:             / (double) Wsize_bsize (ctx->caml_stat_heap_size)) {
byterun/memory.c:    if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){
byterun/memory.c:      caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);
byterun/memory.c:    *(ctx->caml_ref_table.ptr++) = fp;
byterun/globroots.c:  r = ctx->random_seed = ctx->random_seed * 69069 + 25173;
byterun/globroots.c:  caml_insert_global_root_r(ctx, &ctx->caml_global_roots, r);
byterun/globroots.c:  caml_delete_global_root_r(ctx, &ctx->caml_global_roots, r);
byterun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/globroots.c:    caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
byterun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
byterun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_old);
byterun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
byterun/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
byterun/globroots.c:  for (gr = ctx->caml_global_roots_young.forward[0];
byterun/globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, gr->root);
byterun/globroots.c:  caml_empty_global_roots(&ctx->caml_global_roots_young);
byterun/md5.c:    ctx->buf[0] = 0x67452301;
byterun/md5.c:    ctx->buf[1] = 0xefcdab89;
byterun/md5.c:    ctx->buf[2] = 0x98badcfe;
byterun/md5.c:    ctx->buf[3] = 0x10325476;
byterun/md5.c:    ctx->bits[0] = 0;
byterun/md5.c:    ctx->bits[1] = 0;
byterun/md5.c:    t = ctx->bits[0];
byterun/md5.c:    if ((ctx->bits[0] = t + ((uint32) len << 3)) < t)
byterun/md5.c:        ctx->bits[1]++;         /* Carry from low to high */
byterun/md5.c:    ctx->bits[1] += len >> 29;
byterun/md5.c:        unsigned char *p = (unsigned char *) ctx->in + t;
byterun/md5.c:        byteReverse(ctx->in, 16);
byterun/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/md5.c:        memcpy(ctx->in, buf, 64);
byterun/md5.c:        byteReverse(ctx->in, 16);
byterun/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/md5.c:    memcpy(ctx->in, buf, len);
byterun/md5.c:    count = (ctx->bits[0] >> 3) & 0x3F;
byterun/md5.c:    p = ctx->in + count;
byterun/md5.c:        byteReverse(ctx->in, 16);
byterun/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/md5.c:        memset(ctx->in, 0, 56);
byterun/md5.c:    byteReverse(ctx->in, 14);
byterun/md5.c:    ((uint32 *) ctx->in)[14] = ctx->bits[0];
byterun/md5.c:    ((uint32 *) ctx->in)[15] = ctx->bits[1];
byterun/md5.c:    caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/md5.c:    byteReverse((unsigned char *) ctx->buf, 4);
byterun/md5.c:    memcpy(digest, ctx->buf, 16);
byterun/minor_gc.h:   (addr)(val) < (addr)ctx->caml_young_end && (addr)(val) > (addr)ctx->caml_young_start)
byterun/signals.c:  ctx->caml_young_limit = ctx->caml_young_end;
byterun/signals.c:  if (main_ctx) main_ctx->caml_young_limit = main_ctx->caml_young_end;
byterun/signals.c:  ctx->caml_force_major_slice = 1;
byterun/signals.c:  ctx->caml_young_limit = ctx->caml_young_end;
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) {
byterun/compare.c:    free(ctx->compare_stack);
byterun/compare.c:    ctx->compare_stack = ctx->compare_stack_init;
byterun/compare.c:    ctx->compare_stack_limit = ctx->compare_stack + COMPARE_STACK_INIT_SIZE;
byterun/compare.c:  asize_t newsize = 2 * (ctx->compare_stack_limit - ctx->compare_stack);
byterun/compare.c:  asize_t sp_offset = sp - ctx->compare_stack;
byterun/compare.c:  if (ctx->compare_stack == ctx->compare_stack_init) {
byterun/compare.c:    memcpy(newstack, ctx->compare_stack_init,
byterun/compare.c:      realloc(ctx->compare_stack, sizeof(struct compare_item) * newsize);
byterun/compare.c:  ctx->compare_stack = newstack;
byterun/compare.c:  ctx->compare_stack_limit = newstack + newsize;
byterun/compare.c:  sp = ctx->compare_stack;
byterun/compare.c:          ctx->caml_compare_unordered = 0;
byterun/compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
byterun/compare.c:          ctx->caml_compare_unordered = 0;
byterun/compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
byterun/compare.c:      ctx->caml_compare_unordered = 0;
byterun/compare.c:      if (ctx->caml_compare_unordered && !total) return UNORDERED;
byterun/compare.c:        if (sp >= ctx->compare_stack_limit) sp = compare_resize_stack_r(ctx, sp);
byterun/compare.c:    if (sp == ctx->compare_stack) return EQUAL; /* we're done */
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/major_gc.c:  Assert (ctx->gray_vals_cur == ctx->gray_vals_end);
byterun/major_gc.c:  if (ctx->gray_vals_size < ctx->caml_stat_heap_size / 128){
byterun/major_gc.c:                     (intnat) ctx->gray_vals_size * sizeof (value) / 512);
byterun/major_gc.c:    new = (value *) realloc ((char *) ctx->gray_vals,
byterun/major_gc.c:                             2 * ctx->gray_vals_size * sizeof (value));
byterun/major_gc.c:      ctx->gray_vals_cur = ctx->gray_vals;
byterun/major_gc.c:      ctx->heap_is_pure = 0;
byterun/major_gc.c:      ctx->gray_vals = new;
byterun/major_gc.c:      ctx->gray_vals_cur = ctx->gray_vals + ctx->gray_vals_size;
byterun/major_gc.c:      ctx->gray_vals_size *= 2;
byterun/major_gc.c:      ctx->gray_vals_end = ctx->gray_vals + ctx->gray_vals_size;
byterun/major_gc.c:    ctx->gray_vals_cur = ctx->gray_vals + ctx->gray_vals_size / 2;
byterun/major_gc.c:    ctx->heap_is_pure = 0;
byterun/major_gc.c:        *ctx->gray_vals_cur++ = v;
byterun/major_gc.c:        if (ctx->gray_vals_cur >= ctx->gray_vals_end) realloc_gray_vals_r (ctx);
byterun/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
byterun/major_gc.c:  Assert (ctx->gray_vals_cur == gray_vals);
byterun/major_gc.c:  ctx->caml_gc_phase = Phase_mark;
byterun/major_gc.c:  ctx->caml_gc_subphase = Subphase_main;
byterun/major_gc.c:  ctx->markhp = NULL;
byterun/major_gc.c:  ++ ctx->major_gc_counter;
byterun/major_gc.c:  caml_gc_message (0x40, "Subphase = %ld\n", ctx->caml_gc_subphase);
byterun/major_gc.c:  gray_vals_ptr = ctx->gray_vals_cur;
byterun/major_gc.c:    if (gray_vals_ptr > ctx->gray_vals){
byterun/major_gc.c:              if (gray_vals_ptr >= ctx->gray_vals_end) {
byterun/major_gc.c:                ctx->gray_vals_cur = gray_vals_ptr;
byterun/major_gc.c:                gray_vals_ptr = ctx->gray_vals_cur;
byterun/major_gc.c:    }else if (ctx->markhp != NULL){       // phc - keep processing current chunk
byterun/major_gc.c:      if (ctx->markhp == ctx->limit){     // this chunk is done with marking
byterun/major_gc.c:        ctx->chunk = Chunk_next (ctx->chunk);
byterun/major_gc.c:        if (ctx->chunk == NULL){          // no more chunk to mark
byterun/major_gc.c:          ctx->markhp = NULL;
byterun/major_gc.c:          ctx->markhp = ctx->chunk;       // next chunk
byterun/major_gc.c:          ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/major_gc.c:        if (Is_gray_val (Val_hp (ctx->markhp))){
byterun/major_gc.c:          Assert (gray_vals_ptr == ctx->gray_vals);
byterun/major_gc.c:          *gray_vals_ptr++ = Val_hp (ctx->markhp);
byterun/major_gc.c:        ctx->markhp += Bhsize_hp (ctx->markhp); // mark next block as todo
byterun/major_gc.c:      ctx->chunk = ctx->caml_heap_start;
byterun/major_gc.c:      ctx->markhp = ctx->chunk;
byterun/major_gc.c:      ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/major_gc.c:      switch (ctx->caml_gc_subphase){
byterun/major_gc.c:        ctx->caml_gc_subphase = Subphase_weak1;
byterun/major_gc.c:        ctx->weak_prev = &(ctx->caml_weak_list_head);
byterun/major_gc.c:        cur = *(ctx->weak_prev);
byterun/major_gc.c:          ctx->weak_prev = &Field (cur, 0);
byterun/major_gc.c:          ctx->gray_vals_cur = gray_vals_ptr;
byterun/major_gc.c:          gray_vals_ptr = ctx->gray_vals_cur;
byterun/major_gc.c:          ctx->caml_gc_subphase = Subphase_weak2;
byterun/major_gc.c:          ctx->weak_prev = &(ctx->caml_weak_list_head);
byterun/major_gc.c:        cur = *ctx->weak_prev;
byterun/major_gc.c:            *ctx->weak_prev = Field (cur, 0);
byterun/major_gc.c:            // ctx->weak_prev==&ptr
byterun/major_gc.c:            ctx->weak_prev = &Field (cur, 0);
byterun/major_gc.c:          ctx->caml_gc_subphase = Subphase_final;
byterun/major_gc.c:        ctx->gray_vals_cur = gray_vals_ptr;
byterun/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->caml_heap_start;
byterun/major_gc.c:        ctx->caml_gc_phase = Phase_sweep;
byterun/major_gc.c:        ctx->chunk = ctx->caml_heap_start;
byterun/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->chunk;
byterun/major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/major_gc.c:        ctx->caml_fl_size_at_phase_change = ctx->caml_fl_cur_size;
byterun/major_gc.c:  ctx->gray_vals_cur = gray_vals_ptr;
byterun/major_gc.c:    if (ctx->caml_gc_sweep_hp < ctx->limit){
byterun/major_gc.c:      hp = ctx->caml_gc_sweep_hp;
byterun/major_gc.c:      ctx->caml_gc_sweep_hp += Bhsize_hd (hd);
byterun/major_gc.c:        ctx->caml_gc_sweep_hp = caml_fl_merge_block_r (ctx, Bp_hp (hp));
byterun/major_gc.c:        ctx->caml_fl_merge = Bp_hp (hp);
byterun/major_gc.c:      Assert (ctx->caml_gc_sweep_hp <= ctx->limit);
byterun/major_gc.c:      ctx->chunk = Chunk_next (ctx->chunk);
byterun/major_gc.c:      if (ctx->chunk == NULL){
byterun/major_gc.c:        ++ (ctx->caml_stat_major_collections);
byterun/major_gc.c:        ctx->caml_gc_phase = Phase_idle;
byterun/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->chunk;
byterun/major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
byterun/major_gc.c:  p = (double) ctx->caml_allocated_words * 3.0 * (100 + ctx->caml_percent_free)
byterun/major_gc.c:      / Wsize_bsize (ctx->caml_stat_heap_size) / ctx->caml_percent_free / 2.0;
byterun/major_gc.c:  if (ctx->caml_dependent_size > 0){
byterun/major_gc.c:    dp = (double) ctx->caml_dependent_allocated * (100 + ctx->caml_percent_free)
byterun/major_gc.c:         / ctx->caml_dependent_size / ctx->caml_percent_free;
byterun/major_gc.c:  if (p < ctx->caml_extra_heap_resources) p = ctx->caml_extra_heap_resources;
byterun/major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
byterun/major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 250
byterun/major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 5 / 3);
byterun/major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
byterun/major_gc.c:    Assert (ctx->caml_gc_phase == Phase_sweep);
byterun/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) caml_compact_heap_maybe_r (ctx);
byterun/major_gc.c:  ctx->caml_stat_major_words += ctx->caml_allocated_words;
byterun/major_gc.c:  ctx->caml_allocated_words = 0;
byterun/major_gc.c:  ctx->caml_dependent_allocated = 0;
byterun/major_gc.c:  ctx->caml_extra_heap_resources = 0.0;
byterun/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
byterun/major_gc.c:  while (ctx->caml_gc_phase == Phase_mark) mark_slice_r (ctx, LONG_MAX);
byterun/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_sweep);
byterun/major_gc.c:  while (ctx->caml_gc_phase == Phase_sweep) sweep_slice_r (ctx, LONG_MAX);
byterun/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
byterun/major_gc.c:  ctx->caml_stat_major_words += ctx->caml_allocated_words;
byterun/major_gc.c:  ctx->caml_allocated_words = 0;
byterun/major_gc.c:  if (result < ctx->caml_major_heap_increment){
byterun/major_gc.c:    result = ctx->caml_major_heap_increment;
byterun/major_gc.c:  ctx->caml_stat_heap_size = clip_heap_chunk_size (heap_size);
byterun/major_gc.c:  ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
byterun/major_gc.c:  Assert (ctx->caml_stat_heap_size % Page_size == 0);
byterun/major_gc.c:  ctx->caml_heap_start = (char *) caml_alloc_for_heap (ctx->caml_stat_heap_size);
byterun/major_gc.c:  if (ctx->caml_heap_start == NULL)
byterun/major_gc.c:  Chunk_next (ctx->caml_heap_start) = NULL;
byterun/major_gc.c:  if (caml_page_table_add(In_heap, ctx->caml_heap_start,
byterun/major_gc.c:                          ctx->caml_heap_start + ctx->caml_stat_heap_size) != 0) {
byterun/major_gc.c:  caml_make_free_blocks_r (ctx, (value *) ctx->caml_heap_start,
byterun/major_gc.c:                         Wsize_bsize (ctx->caml_stat_heap_size), 1, Caml_white);
byterun/major_gc.c:  ctx->caml_gc_phase = Phase_idle;
byterun/major_gc.c:  ctx->gray_vals_size = 2048;
byterun/major_gc.c:  ctx->gray_vals = (value *) malloc (ctx->gray_vals_size * sizeof (value));
byterun/major_gc.c:  if (ctx->gray_vals == NULL)
byterun/major_gc.c:  ctx->gray_vals_cur = ctx->gray_vals;
byterun/major_gc.c:  ctx->gray_vals_end = ctx->gray_vals + ctx->gray_vals_size;
byterun/major_gc.c:  ctx->heap_is_pure = 1;
byterun/major_gc.c:  ctx->caml_allocated_words = 0;
byterun/major_gc.c:  ctx->caml_extra_heap_resources = 0.0;
byterun/callback.c:  for (nv = ctx->named_value_table[h]; nv != NULL; nv = nv->next) {
byterun/callback.c:  nv->next = ctx->named_value_table[h];
byterun/callback.c:  ctx->named_value_table[h] = nv;
byterun/callback.c:  for (nv = ctx->named_value_table[hash_value_name(name)];
byterun/compact.c:  char *ch = ctx->caml_heap_start;
byterun/compact.c:  ctx->compact_fl = ctx->caml_heap_start;
byterun/compact.c:  while (Chunk_size (ctx->compact_fl) - Chunk_alloc (ctx->compact_fl) <= Bhsize_wosize (3)
byterun/compact.c:         && Chunk_size (Chunk_next (ctx->compact_fl))
byterun/compact.c:            - Chunk_alloc (Chunk_next (ctx->compact_fl))
byterun/compact.c:    ctx->compact_fl = Chunk_next (ctx->compact_fl);
byterun/compact.c:  chunk = ctx->compact_fl;
byterun/compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
byterun/compact.c:    ch = ctx->caml_heap_start;
byterun/compact.c:    ch = ctx->caml_heap_start;
byterun/compact.c:      value *pp = &ctx->caml_weak_list_head;
byterun/compact.c:          if (Field (p,i) != ctx->caml_weak_none){
byterun/compact.c:    ch = ctx->caml_heap_start;
byterun/compact.c:    ch = ctx->caml_heap_start;
byterun/compact.c:    ch = ctx->caml_heap_start;
byterun/compact.c:    wanted = ctx->caml_percent_free * (live / 100 + 1);
byterun/compact.c:    ch = ctx->caml_heap_start;
byterun/compact.c:    ch = ctx->caml_heap_start;
byterun/compact.c:  ++ ctx->caml_stat_compactions;
byterun/compact.c:  live = Wsize_bsize (ctx->caml_stat_heap_size) - ctx->caml_fl_cur_size;
byterun/compact.c:  target_words = live + ctx->caml_percent_free * (live / 100 + 1)
byterun/compact.c:  if (target_size < ctx->caml_stat_heap_size / 2){
byterun/compact.c:    Chunk_next (chunk) = ctx->caml_heap_start;
byterun/compact.c:    ctx->caml_heap_start = chunk;
byterun/compact.c:    ++ ctx->caml_stat_heap_chunks;
byterun/compact.c:    ctx->caml_stat_heap_size += Chunk_size (chunk);
byterun/compact.c:    if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
byterun/compact.c:      ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
byterun/compact.c:    Assert (ctx->caml_stat_heap_chunks == 1);
byterun/compact.c:    Assert (Chunk_next (ctx->caml_heap_start) == NULL);
byterun/compact.c:    Assert (ctx->caml_stat_heap_size == Chunk_size (chunk));
byterun/compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
byterun/compact.c:  if (ctx->caml_percent_max >= 1000000) return;
byterun/compact.c:  if (ctx->caml_stat_major_collections < 3) return;
byterun/compact.c:  fw = 3.0 * ctx->caml_fl_cur_size - 2.0 * ctx->caml_fl_size_at_phase_change;
byterun/compact.c:  if (fw < 0) fw = ctx->caml_fl_cur_size;
byterun/compact.c:  if (fw >= Wsize_bsize (ctx->caml_stat_heap_size)){
byterun/compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
byterun/compact.c:                   (uintnat) ctx->caml_fl_size_at_phase_change);
byterun/compact.c:  if (fp >= ctx->caml_percent_max){
byterun/compact.c:    fw = ctx->caml_fl_cur_size;
byterun/compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
byterun/win32.c:  DWORD *ctx_ip = &(ctx->Eip);
byterun/win32.c:  DWORD *ctx_sp = &(ctx->Esp);
byterun/io.c:  channel->next = ctx->caml_all_opened_channels;
byterun/io.c:  if (ctx->caml_all_opened_channels != NULL)
byterun/io.c:    ctx->caml_all_opened_channels->prev = channel;
byterun/io.c:  ctx->caml_all_opened_channels = channel;
byterun/io.c:    Assert (channel == ctx->caml_all_opened_channels);
byterun/io.c:    ctx->caml_all_opened_channels = ctx->caml_all_opened_channels->next;
byterun/io.c:    if (ctx->caml_all_opened_channels != NULL)
byterun/io.c:      ctx->caml_all_opened_channels->prev = NULL;
byterun/io.c:  for (channel = ctx->caml_all_opened_channels;
byterun/weak.c:  for (i = 1; i < size; i++) Field (res, i) = ctx->caml_weak_none;
byterun/weak.c:  Field (res, 0) = ctx->caml_weak_list_head;
byterun/weak.c:  ctx->caml_weak_list_head = res;
byterun/weak.c:      if (ctx->caml_weak_ref_table.ptr >= ctx->caml_weak_ref_table.limit){
byterun/weak.c:        CAMLassert (ctx->caml_weak_ref_table.ptr == ctx->caml_weak_ref_table.limit);
byterun/weak.c:        caml_realloc_ref_table_r (ctx, &(ctx->caml_weak_ref_table));
byterun/weak.c:      *(ctx->caml_weak_ref_table.ptr++) = &Field (ar, offset);
byterun/weak.c:    Field (ar, offset) = ctx->caml_weak_none;
byterun/weak.c:  if (Field (ar, offset) == ctx->caml_weak_none){
byterun/weak.c:    if (ctx->caml_gc_phase == Phase_mark && Is_block (elt) && Is_in_heap (elt)){
byterun/weak.c:  if (v == ctx->caml_weak_none) CAMLreturn (None_val);
byterun/weak.c:    if (v == ctx->caml_weak_none) CAMLreturn (None_val);
byterun/weak.c:        if (ctx->caml_gc_phase == Phase_mark && Is_block (f) && Is_in_heap (f)){
byterun/weak.c:  return Val_bool (Field (ar, offset) != ctx->caml_weak_none);
byterun/weak.c:  if (ctx->caml_gc_phase == Phase_mark && ctx->caml_gc_subphase == Subphase_weak1){
byterun/weak.c:      if (v != ctx->caml_weak_none && Is_block (v) && Is_in_heap (v)
byterun/weak.c:        Field (ars, offset_s + i) = ctx->caml_weak_none;
byterun/custom.c:  l->next = ctx->custom_ops_table;
byterun/custom.c:  ctx->custom_ops_table = l;
byterun/custom.c:  for (l = ctx->custom_ops_table; l != NULL; l = l->next)
byterun/custom.c:  for (l = ctx->custom_ops_final_table; l != NULL; l = l->next)
byterun/custom.c:  l->next = ctx->custom_ops_final_table;
byterun/custom.c:  ctx->custom_ops_final_table = l;
byterun/memory.h:  ctx->caml_young_ptr -= Bhsize_wosize (wosize);                            \
byterun/memory.h:  if (ctx->caml_young_ptr < ctx->caml_young_start){                              \
byterun/memory.h:    ctx->caml_young_ptr += Bhsize_wosize (wosize);                          \
byterun/memory.h:    ctx->caml_young_ptr -= Bhsize_wosize (wosize);                          \
byterun/memory.h:  Hd_hp (ctx->caml_young_ptr) = Make_header ((wosize), (tag), Caml_black);  \
byterun/memory.h:  (result) = Val_hp (ctx->caml_young_ptr);                                  \
byterun/memory.h:    if (ctx->caml_gc_phase == Phase_mark) caml_darken_r (ctx, _old_, NULL); \
byterun/memory.h:      if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){            \
byterun/memory.h:        CAMLassert (ctx->caml_ref_table.ptr == ctx->caml_ref_table.limit);  \
byterun/memory.h:        caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);               \
byterun/memory.h:      *(ctx->caml_ref_table).ptr++ = (fp);                                  \
byterun/memory.h:  struct caml__roots_block *caml__frame = ctx->caml_local_roots
byterun/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
byterun/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
byterun/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
byterun/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
byterun/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
byterun/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
byterun/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/memory.h:  ctx->caml_local_roots = caml__frame; \
byterun/memory.h:  ctx->caml_local_roots = caml__frame; \
byterun/context.c:  ctx->major_gc_counter = 0;
byterun/context.c:  return Val_int(ctx->count_id);
byterun/context.c:  printf("caml_young_ptr     : %p\n", (void*)ctx->caml_young_ptr);
byterun/context.c:  printf("caml_young_limit   : %p\n", (void*)ctx->caml_young_limit);
byterun/context.c:  printf("caml_young_base    : %p\n", (void*)ctx->caml_young_base);
byterun/context.c:  printf("caml_young_start   : %p\n", (void*)ctx->caml_young_start);
byterun/context.c:  printf("caml_young_end     : %p\n", (void*)ctx->caml_young_end);
byterun/context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
byterun/context.c:           ctx->caml_young_ptr, caml_young_ptr);
byterun/context.c:  if (ctx->caml_young_limit!=caml_young_limit)
byterun/context.c:           ctx->caml_young_limit, caml_young_limit);
byterun/context.c:  if (ctx->caml_young_base!=caml_young_base)
byterun/context.c:           ctx->caml_young_base, caml_young_base);
byterun/context.c:  if (ctx->caml_young_start!=caml_young_start)
byterun/context.c:           ctx->caml_young_start, caml_young_start);
byterun/context.c:  if (ctx->caml_young_end!=caml_young_end)
byterun/context.c:           ctx->caml_young_end, caml_young_end);
byterun/context.c:  ctx->caml_young_ptr     = caml_young_ptr;
byterun/context.c:  ctx->caml_young_limit   = caml_young_limit;
byterun/context.c:  ctx->caml_young_base    = caml_young_base;
byterun/context.c:  ctx->caml_young_start   = caml_young_start;
byterun/context.c:  ctx->caml_young_end     = caml_young_end;
byterun/context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
byterun/context.c:           ctx->caml_young_ptr, caml_young_ptr);
byterun/context.c:  if (ctx->caml_young_limit!=caml_young_limit)
byterun/context.c:           ctx->caml_young_limit, caml_young_limit);
byterun/context.c:  if (ctx->caml_young_base!=caml_young_base)
byterun/context.c:           ctx->caml_young_base, caml_young_base);
byterun/context.c:  if (ctx->caml_young_start!=caml_young_start)
byterun/context.c:           ctx->caml_young_start, caml_young_start);
byterun/context.c:  if (ctx->caml_young_end!=caml_young_end)
byterun/context.c:           ctx->caml_young_end, caml_young_end);
byterun/context.c:  caml_young_ptr     = ctx->caml_young_ptr; 
byterun/context.c:  caml_young_limit   = ctx->caml_young_limit;
byterun/context.c:  caml_young_base    = ctx->caml_young_base;
byterun/context.c:  caml_young_start   = ctx->caml_young_start;  
byterun/context.c:  caml_young_end     = ctx->caml_young_end;
byterun/reentrant_call:compact.c:  while (Chunk_size (ctx->compact_fl) - Chunk_alloc (ctx->compact_fl) <= Bhsize_wosize (3)
byterun/reentrant_call:compact.c:         && Chunk_size (Chunk_next (ctx->compact_fl))
byterun/reentrant_call:compact.c:            - Chunk_alloc (Chunk_next (ctx->compact_fl))
byterun/reentrant_call:compact.c:    ctx->compact_fl = Chunk_next (ctx->compact_fl);
byterun/reentrant_call:compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
byterun/reentrant_call:compact.c:  live = Wsize_bsize (ctx->caml_stat_heap_size) - ctx->caml_fl_cur_size;
byterun/reentrant_call:compact.c:    if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
byterun/reentrant_call:compact.c:    Assert (ctx->caml_stat_heap_chunks == 1);
byterun/reentrant_call:compact.c:    Assert (Chunk_next (ctx->caml_heap_start) == NULL);
byterun/reentrant_call:compact.c:    Assert (ctx->caml_stat_heap_size == Chunk_size (chunk));
byterun/reentrant_call:compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
byterun/reentrant_call:compact.c:  if (ctx->caml_percent_max >= 1000000) return;
byterun/reentrant_call:compact.c:  if (ctx->caml_stat_major_collections < 3) return;
byterun/reentrant_call:compact.c:  if (fw >= Wsize_bsize (ctx->caml_stat_heap_size)){
byterun/reentrant_call:compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
byterun/reentrant_call:compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) {
byterun/reentrant_call:compare.c:    free(ctx->compare_stack);
byterun/reentrant_call:compare.c:  asize_t newsize = 2 * (ctx->compare_stack_limit - ctx->compare_stack);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack == ctx->compare_stack_init) {
byterun/reentrant_call:compare.c:      realloc(ctx->compare_stack, sizeof(struct compare_item) * newsize);
byterun/reentrant_call:compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
byterun/reentrant_call:compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
byterun/reentrant_call:compare.c:      if (ctx->caml_compare_unordered && !total) return UNORDERED;
byterun/reentrant_call:compare.c:        if (sp >= ctx->compare_stack_limit) sp = compare_resize_stack_r(ctx, sp);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
byterun/reentrant_call:context.c:  return Val_int(ctx->count_id);
byterun/reentrant_call:context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
byterun/reentrant_call:context.c:  if (ctx->caml_young_limit!=caml_young_limit)
byterun/reentrant_call:context.c:  if (ctx->caml_young_base!=caml_young_base)
byterun/reentrant_call:context.c:  if (ctx->caml_young_start!=caml_young_start)
byterun/reentrant_call:context.c:  if (ctx->caml_young_end!=caml_young_end)
byterun/reentrant_call:context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
byterun/reentrant_call:context.c:  if (ctx->caml_young_limit!=caml_young_limit)
byterun/reentrant_call:context.c:  if (ctx->caml_young_base!=caml_young_base)
byterun/reentrant_call:context.c:  if (ctx->caml_young_start!=caml_young_start)
byterun/reentrant_call:context.c:  if (ctx->caml_young_end!=caml_young_end)
byterun/reentrant_call:finalise.c:  if (ctx->to_do_tl == NULL){
byterun/reentrant_call:finalise.c:    Assert (ctx->to_do_tl->next == NULL);
byterun/reentrant_call:finalise.c:  Assert (ctx->final_young == ctx->final_old);
byterun/reentrant_call:finalise.c:    Assert (Is_block (ctx->final_table[i].val));
byterun/reentrant_call:finalise.c:    Assert (Is_in_heap (ctx->final_table[i].val));
byterun/reentrant_call:finalise.c:    if (Is_white_val (ctx->final_table[i].val)) ++ todo_count;
byterun/reentrant_call:finalise.c:      Assert (Is_block (ctx->final_table[i].val));
byterun/reentrant_call:finalise.c:      Assert (Is_in_heap (ctx->final_table[i].val));
byterun/reentrant_call:finalise.c:      if (Is_white_val (ctx->final_table[i].val)){
byterun/reentrant_call:finalise.c:        if (Tag_val (ctx->final_table[i].val) == Forward_tag){
byterun/reentrant_call:finalise.c:          Assert (ctx->final_table[i].offset == 0);
byterun/reentrant_call:finalise.c:          fv = Forward_val (ctx->final_table[i].val);
byterun/reentrant_call:finalise.c:            if (Is_block (ctx->final_table[i].val)
byterun/reentrant_call:finalise.c:                && Is_in_heap (ctx->final_table[i].val)){
byterun/reentrant_call:finalise.c:      CAMLassert (Is_white_val (ctx->to_do_tl->item[i].val));
byterun/reentrant_call:finalise.c:      caml_darken_r (ctx, ctx->to_do_tl->item[i].val, NULL);
byterun/reentrant_call:finalise.c:  if (ctx->running_finalisation_function) return;
byterun/reentrant_call:finalise.c:  if (ctx->to_do_hd != NULL){
byterun/reentrant_call:finalise.c:      while (ctx->to_do_hd != NULL && ctx->to_do_hd->size == 0){
byterun/reentrant_call:finalise.c:        free (ctx->to_do_hd);
byterun/reentrant_call:finalise.c:        if (ctx->to_do_hd == NULL) ctx->to_do_tl = NULL;
byterun/reentrant_call:finalise.c:      if (ctx->to_do_hd == NULL) break;
byterun/reentrant_call:finalise.c:      Assert (ctx->to_do_hd->size > 0);
byterun/reentrant_call:finalise.c:      // phc - for f in reverse(ctx->to_do_head)
byterun/reentrant_call:finalise.c:  Assert (ctx->final_old == ctx->final_young);
byterun/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
byterun/reentrant_call:finalise.c:  Assert (ctx->final_old == ctx->final_young);
byterun/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
byterun/reentrant_call:finalise.c:  Assert (ctx->final_old <= ctx->final_young);
byterun/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
byterun/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
byterun/reentrant_call:finalise.c:  Assert (ctx->final_old <= ctx->final_young);
byterun/reentrant_call:finalise.c:  if (ctx->final_young >= ctx->final_size){
byterun/reentrant_call:finalise.c:    if (ctx->final_table == NULL){
byterun/reentrant_call:finalise.c:      Assert (ctx->final_old == 0);
byterun/reentrant_call:finalise.c:      Assert (ctx->final_young == 0);
byterun/reentrant_call:finalise.c:      ctx->final_table = caml_stat_resize (ctx->final_table,
byterun/reentrant_call:finalise.c:  Assert (ctx->final_young < ctx->final_size);
byterun/reentrant_call:freelist.c:    if (ctx->caml_fl_merge == cur) ctx->caml_fl_merge = prev;
byterun/reentrant_call:freelist.c:                                  Assert (ctx->fl_prev != NULL);
byterun/reentrant_call:freelist.c:      sz = Wosize_bp (Next (ctx->flp[i]));
byterun/reentrant_call:freelist.c:                                   i, ctx->flp[i], Next(ctx->flp[i]));
byterun/reentrant_call:freelist.c:    if (ctx->flp_size == 0){ 
byterun/reentrant_call:freelist.c:      prev = Next (ctx->flp[ctx->flp_size - 1]);
byterun/reentrant_call:freelist.c:      if (ctx->beyond != NULL) prev = ctx->beyond;
byterun/reentrant_call:freelist.c:    while (ctx->flp_size < FLP_MAX){
byterun/reentrant_call:freelist.c:    if (ctx->beyond != NULL){
byterun/reentrant_call:freelist.c:    prevsz = Wosize_bp (Next (ctx->flp[FLP_MAX-1]));
byterun/reentrant_call:freelist.c:        prevsz = Wosize_bp (Next (ctx->flp[i-1]));
byterun/reentrant_call:freelist.c:        if (Wosize_bp (Next (ctx->flp[i])) <= prevsz){
byterun/reentrant_call:freelist.c:          ctx->beyond = Next (ctx->flp[i]);
byterun/reentrant_call:freelist.c:            memmove (&(ctx->flp[i+j]), 
byterun/reentrant_call:freelist.c:                     &(ctx->flp[i+1]), 
byterun/reentrant_call:freelist.c:                     sizeof (block *) * (ctx->flp_size-i-1));
byterun/reentrant_call:freelist.c:             memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
byterun/reentrant_call:freelist.c:              memmove (&(ctx->flp[i+j]), 
byterun/reentrant_call:freelist.c:                       &(ctx->flp[i+1]), 
byterun/reentrant_call:freelist.c:              memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
byterun/reentrant_call:freelist.c:              memmove (&(ctx->flp[i]), &buf[0], 
byterun/reentrant_call:freelist.c:          ctx->beyond = Next (ctx->flp[FLP_MAX - 1]);
byterun/reentrant_call:freelist.c:    while (ctx->flp_size > 0 && Next (ctx->flp[ctx->flp_size - 1]) >= changed) 
byterun/reentrant_call:freelist.c:    if (ctx->beyond >= changed) 
byterun/reentrant_call:freelist.c:  Next (ctx->fl_head) = NULL;
byterun/reentrant_call:freelist.c:    truncate_flp_r (ctx, ctx->fl_head);
byterun/reentrant_call:freelist.c://    last_fragment or Next(ctx->caml_fl_merge).
byterun/reentrant_call:freelist.c:  if (ctx->last_fragment == Hp_bp (bp)){
byterun/reentrant_call:freelist.c:    Assert (ctx->caml_fl_merge == prev);
byterun/reentrant_call:freelist.c:                                                  Assert (ctx->fl_last != NULL);
byterun/reentrant_call:freelist.c:                                           Assert (Next (ctx->fl_last) == NULL);
byterun/reentrant_call:freelist.c:    Next (ctx->fl_last) = bp;
byterun/reentrant_call:freelist.c:    if (ctx->fl_last==ctx->caml_fl_merge && bp<ctx->caml_gc_sweep_hp){
byterun/reentrant_call:gc_ctrl.c:          if (ctx->caml_gc_phase == Phase_sweep && cur_hp >= ctx->caml_gc_sweep_hp){
byterun/reentrant_call:gc_ctrl.c:          == Wsize_bsize (ctx->caml_stat_heap_size));
byterun/reentrant_call:gc_ctrl.c:                      + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/reentrant_call:gc_ctrl.c:    intnat heap_words = Wsize_bsize (ctx->caml_stat_heap_size);
byterun/reentrant_call:gc_ctrl.c:    intnat top_heap_words = Wsize_bsize (ctx->caml_stat_top_heap_size);
byterun/reentrant_call:gc_ctrl.c:             + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/reentrant_call:gc_ctrl.c:                    + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/reentrant_call:gc_ctrl.c:  Store_field (res, 0, Val_long (Wsize_bsize (ctx->caml_minor_heap_size)));  /* s */
byterun/reentrant_call:gc_ctrl.c:  Store_field (res, 1,Val_long(Wsize_bsize(ctx->caml_major_heap_increment)));/* i */
byterun/reentrant_call:gc_ctrl.c:  Store_field (res, 2, Val_long (ctx->caml_percent_free));                   /* o */
byterun/reentrant_call:gc_ctrl.c:  Store_field (res, 4, Val_long (ctx->caml_percent_max));                    /* O */
byterun/reentrant_call:globroots.c:  caml_insert_global_root_r(ctx, &ctx->caml_global_roots, r);
byterun/reentrant_call:globroots.c:  caml_delete_global_root_r(ctx, &ctx->caml_global_roots, r);
byterun/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/reentrant_call:globroots.c:    caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/reentrant_call:globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
byterun/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
byterun/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
byterun/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
byterun/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_old);
byterun/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
byterun/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
byterun/reentrant_call:globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, gr->root);
byterun/reentrant_call:io.c:  if (ctx->caml_all_opened_channels != NULL)
byterun/reentrant_call:io.c:    if (ctx->caml_all_opened_channels != NULL)
byterun/reentrant_call:major_gc.c:  Assert (ctx->gray_vals_cur == ctx->gray_vals_end);
byterun/reentrant_call:major_gc.c:  if (ctx->gray_vals_size < ctx->caml_stat_heap_size / 128){
byterun/reentrant_call:major_gc.c:        if (ctx->gray_vals_cur >= ctx->gray_vals_end) realloc_gray_vals_r (ctx);
byterun/reentrant_call:major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
byterun/reentrant_call:major_gc.c:  Assert (ctx->gray_vals_cur == gray_vals);
byterun/reentrant_call:major_gc.c:    }else if (ctx->markhp != NULL){       // phc - keep processing current chunk
byterun/reentrant_call:major_gc.c:      if (ctx->markhp == ctx->limit){     // this chunk is done with marking
byterun/reentrant_call:major_gc.c:        ctx->chunk = Chunk_next (ctx->chunk);
byterun/reentrant_call:major_gc.c:        if (ctx->chunk == NULL){          // no more chunk to mark
byterun/reentrant_call:major_gc.c:          ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/reentrant_call:major_gc.c:        if (Is_gray_val (Val_hp (ctx->markhp))){
byterun/reentrant_call:major_gc.c:          *gray_vals_ptr++ = Val_hp (ctx->markhp);
byterun/reentrant_call:major_gc.c:        ctx->markhp += Bhsize_hp (ctx->markhp); // mark next block as todo
byterun/reentrant_call:major_gc.c:      ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/reentrant_call:major_gc.c:      switch (ctx->caml_gc_subphase){
byterun/reentrant_call:major_gc.c:        ctx->weak_prev = &(ctx->caml_weak_list_head);
byterun/reentrant_call:major_gc.c:        cur = *(ctx->weak_prev);
byterun/reentrant_call:major_gc.c:          ctx->weak_prev = &(ctx->caml_weak_list_head);
byterun/reentrant_call:major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/reentrant_call:major_gc.c:    if (ctx->caml_gc_sweep_hp < ctx->limit){
byterun/reentrant_call:major_gc.c:        ctx->caml_gc_sweep_hp = caml_fl_merge_block_r (ctx, Bp_hp (hp));
byterun/reentrant_call:major_gc.c:      Assert (ctx->caml_gc_sweep_hp <= ctx->limit);
byterun/reentrant_call:major_gc.c:      ctx->chunk = Chunk_next (ctx->chunk);
byterun/reentrant_call:major_gc.c:      if (ctx->chunk == NULL){
byterun/reentrant_call:major_gc.c:        ++ (ctx->caml_stat_major_collections);
byterun/reentrant_call:major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
byterun/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
byterun/reentrant_call:major_gc.c:      / Wsize_bsize (ctx->caml_stat_heap_size) / ctx->caml_percent_free / 2.0;
byterun/reentrant_call:major_gc.c:  if (ctx->caml_dependent_size > 0){
byterun/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
byterun/reentrant_call:major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 250
byterun/reentrant_call:major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 5 / 3);
byterun/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
byterun/reentrant_call:major_gc.c:    Assert (ctx->caml_gc_phase == Phase_sweep);
byterun/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) caml_compact_heap_maybe_r (ctx);
byterun/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
byterun/reentrant_call:major_gc.c:  while (ctx->caml_gc_phase == Phase_mark) mark_slice_r (ctx, LONG_MAX);
byterun/reentrant_call:major_gc.c:  Assert (ctx->caml_gc_phase == Phase_sweep);
byterun/reentrant_call:major_gc.c:  while (ctx->caml_gc_phase == Phase_sweep) sweep_slice_r (ctx, LONG_MAX);
byterun/reentrant_call:major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
byterun/reentrant_call:major_gc.c:  Assert (ctx->caml_stat_heap_size % Page_size == 0);
byterun/reentrant_call:major_gc.c:  ctx->caml_heap_start = (char *) caml_alloc_for_heap (ctx->caml_stat_heap_size);
byterun/reentrant_call:major_gc.c:  if (ctx->caml_heap_start == NULL)
byterun/reentrant_call:major_gc.c:  Chunk_next (ctx->caml_heap_start) = NULL;
byterun/reentrant_call:major_gc.c:  caml_make_free_blocks_r (ctx, (value *) ctx->caml_heap_start,
byterun/reentrant_call:major_gc.c:                         Wsize_bsize (ctx->caml_stat_heap_size), 1, Caml_white);
byterun/reentrant_call:major_gc.c:  ctx->gray_vals = (value *) malloc (ctx->gray_vals_size * sizeof (value));
byterun/reentrant_call:major_gc.c:  if (ctx->gray_vals == NULL)
byterun/reentrant_call:md5.c:    if ((ctx->bits[0] = t + ((uint32) len << 3)) < t)
byterun/reentrant_call:md5.c:        byteReverse(ctx->in, 16);
byterun/reentrant_call:md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/reentrant_call:md5.c:        memcpy(ctx->in, buf, 64);
byterun/reentrant_call:md5.c:        byteReverse(ctx->in, 16);
byterun/reentrant_call:md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/reentrant_call:md5.c:    memcpy(ctx->in, buf, len);
byterun/reentrant_call:md5.c:    count = (ctx->bits[0] >> 3) & 0x3F;
byterun/reentrant_call:md5.c:        byteReverse(ctx->in, 16);
byterun/reentrant_call:md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/reentrant_call:md5.c:        memset(ctx->in, 0, 56);
byterun/reentrant_call:md5.c:    byteReverse(ctx->in, 14);
byterun/reentrant_call:md5.c:    caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
byterun/reentrant_call:memory.c:                   (ctx->caml_stat_heap_size + Chunk_size (m)) / 1024);
byterun/reentrant_call:memory.c:  if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
byterun/reentrant_call:memory.c:  if (ctx->caml_gc_phase == Phase_mark
byterun/reentrant_call:memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
byterun/reentrant_call:memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
byterun/reentrant_call:memory.c:            || (ctx->caml_gc_phase == Phase_sweep
byterun/reentrant_call:memory.c:      if (ctx->caml_in_minor_collection)
byterun/reentrant_call:memory.c:  if (ctx->caml_gc_phase == Phase_mark
byterun/reentrant_call:memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
byterun/reentrant_call:memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
byterun/reentrant_call:memory.c:            || (ctx->caml_gc_phase == Phase_sweep
byterun/reentrant_call:memory.c:  if (ctx->caml_allocated_words > Wsize_bsize (ctx->caml_minor_heap_size)){
byterun/reentrant_call:memory.c:  if (ctx->caml_extra_heap_resources > 1.0){
byterun/reentrant_call:memory.c:  if (ctx->caml_extra_heap_resources
byterun/reentrant_call:memory.c:           > (double) Wsize_bsize (ctx->caml_minor_heap_size) / 2.0
byterun/reentrant_call:memory.c:             / (double) Wsize_bsize (ctx->caml_stat_heap_size)) {
byterun/reentrant_call:memory.c:    if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){
byterun/reentrant_call:memory.c:      caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);
byterun/reentrant_call:memory.c:    *(ctx->caml_ref_table.ptr++) = fp;
byterun/reentrant_call:memory.h:  if (ctx->caml_young_ptr < ctx->caml_young_start){                              \
byterun/reentrant_call:memory.h:  Hd_hp (ctx->caml_young_ptr) = Make_header ((wosize), (tag), Caml_black);  \
byterun/reentrant_call:memory.h:  (result) = Val_hp (ctx->caml_young_ptr);                                  \
byterun/reentrant_call:memory.h:    if (ctx->caml_gc_phase == Phase_mark) caml_darken_r (ctx, _old_, NULL); \
byterun/reentrant_call:memory.h:      if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){            \
byterun/reentrant_call:memory.h:        CAMLassert (ctx->caml_ref_table.ptr == ctx->caml_ref_table.limit);  \
byterun/reentrant_call:memory.h:        caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);               \
byterun/reentrant_call:memory.h:      *(ctx->caml_ref_table).ptr++ = (fp);                                  \
byterun/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
byterun/reentrant_call:minor_gc.c://  if (ctx->caml_young_ptr != ctx->caml_young_end) caml_minor_collection_r (ctx);
byterun/reentrant_call:minor_gc.c:                                    Assert (ctx->caml_young_ptr == ctx->caml_young_end);
byterun/reentrant_call:minor_gc.c:  if (ctx->caml_young_start != NULL){
byterun/reentrant_call:minor_gc.c:    free (ctx->caml_young_base);
byterun/reentrant_call:minor_gc.c:  reset_table (&(ctx->caml_ref_table));
byterun/reentrant_call:minor_gc.c:  reset_table (&(ctx->caml_weak_ref_table));
byterun/reentrant_call:minor_gc.c:  while (ctx->oldify_todo_list != 0){
byterun/reentrant_call:minor_gc.c:  if (ctx->caml_young_ptr != ctx->caml_young_end){
byterun/reentrant_call:minor_gc.c:    if (ctx->caml_young_ptr < ctx->caml_young_start)
byterun/reentrant_call:minor_gc.c:    ctx->caml_stat_minor_words += Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/reentrant_call:minor_gc.c:    clear_table (&(ctx->caml_ref_table));
byterun/reentrant_call:minor_gc.c:    clear_table (&(ctx->caml_weak_ref_table));
byterun/reentrant_call:minor_gc.c:  if (ctx->caml_force_major_slice) caml_minor_collection_r(ctx);
byterun/reentrant_call:minor_gc.c:                                             Assert (ctx->caml_force_major_slice);
byterun/reentrant_call:printexc.c:  if (ctx->caml_backtrace_active
byterun/reentrant_call:weak.c:      if (ctx->caml_weak_ref_table.ptr >= ctx->caml_weak_ref_table.limit){
byterun/reentrant_call:weak.c:        CAMLassert (ctx->caml_weak_ref_table.ptr == ctx->caml_weak_ref_table.limit);
byterun/reentrant_call:weak.c:        caml_realloc_ref_table_r (ctx, &(ctx->caml_weak_ref_table));
byterun/reentrant_call:weak.c:      *(ctx->caml_weak_ref_table.ptr++) = &Field (ar, offset);
byterun/reentrant_call:weak.c:    if (ctx->caml_gc_phase == Phase_mark && Is_block (elt) && Is_in_heap (elt)){
byterun/reentrant_call:weak.c:        if (ctx->caml_gc_phase == Phase_mark && Is_block (f) && Is_in_heap (f)){
byterun/reentrant_call:weak.c:  if (ctx->caml_gc_phase == Phase_mark && ctx->caml_gc_subphase == Subphase_weak1){
byterun/reentrant_call:win32.c:  DWORD *ctx_ip = &(ctx->Eip);
byterun/reentrant_call:win32.c:  DWORD *ctx_sp = &(ctx->Esp);
byterun/freelist.c:    ctx->caml_fl_cur_size -= Whsize_hd (h);
byterun/freelist.c:    if (ctx->caml_fl_merge == cur) ctx->caml_fl_merge = prev;
byterun/freelist.c:      if (flpi + 1 < ctx->flp_size && ctx->flp[flpi + 1] == cur){
byterun/freelist.c:        ctx->flp[flpi + 1] = prev;
byterun/freelist.c:      }else if (flpi == ctx->flp_size - 1){
byterun/freelist.c:        ctx->beyond = (prev == ctx->fl_head) ? NULL : prev;
byterun/freelist.c:        -- ctx->flp_size;
byterun/freelist.c:    ctx->caml_fl_cur_size -= wh_sz;
byterun/freelist.c:  if (policy == Policy_next_fit) ctx->fl_prev = prev;
byterun/freelist.c:                                  Assert (ctx->fl_prev != NULL);
byterun/freelist.c:    prev = ctx->fl_prev;
byterun/freelist.c:    ctx->fl_last = prev;
byterun/freelist.c:    prev = ctx->fl_head;
byterun/freelist.c:    while (prev != ctx->fl_prev){
byterun/freelist.c:    for (i = 0; i < ctx->flp_size; i++){
byterun/freelist.c:      sz = Wosize_bp (Next (ctx->flp[i]));
byterun/freelist.c:                                   i, ctx->flp[i], Next(ctx->flp[i]));
byterun/freelist.c:    if (ctx->flp_size == 0){ 
byterun/freelist.c:      prev = ctx->fl_head;
byterun/freelist.c:      prev = Next (ctx->flp[ctx->flp_size - 1]);
byterun/freelist.c:      if (ctx->beyond != NULL) prev = ctx->beyond;
byterun/freelist.c:    while (ctx->flp_size < FLP_MAX){
byterun/freelist.c:        ctx->fl_last = prev;
byterun/freelist.c:        ctx->beyond = (prev==ctx->fl_head) ? NULL : prev;
byterun/freelist.c:          ctx->flp[ctx->flp_size] = prev;
byterun/freelist.c:          ++ ctx->flp_size;
byterun/freelist.c:            ctx->beyond = cur;
byterun/freelist.c:            i = ctx->flp_size - 1;
byterun/freelist.c:                                       ctx->flp_size - 1, prev, cur);
byterun/freelist.c:    ctx->beyond = cur;
byterun/freelist.c:    if (ctx->beyond != NULL){
byterun/freelist.c:      prev = ctx->beyond;
byterun/freelist.c:      prev = ctx->flp[ctx->flp_size - 1];
byterun/freelist.c:    prevsz = Wosize_bp (Next (ctx->flp[FLP_MAX-1]));
byterun/freelist.c:        ctx->beyond = cur;
byterun/freelist.c:                                 ctx->flp_size, prev, cur);
byterun/freelist.c:    ctx->fl_last = prev;
byterun/freelist.c:    Assert (0 <= i && i < ctx->flp_size + 1);
byterun/freelist.c:    if (i < ctx->flp_size){
byterun/freelist.c:        prevsz = Wosize_bp (Next (ctx->flp[i-1]));
byterun/freelist.c:      if (i == ctx->flp_size - 1){
byterun/freelist.c:        if (Wosize_bp (Next (ctx->flp[i])) <= prevsz){
byterun/freelist.c:          ctx->beyond = Next (ctx->flp[i]);
byterun/freelist.c:          -- ctx->flp_size;
byterun/freelist.c:          ctx->beyond = NULL;
byterun/freelist.c:        prev = ctx->flp[i];
byterun/freelist.c:        while (prev != ctx->flp[i+1]){
byterun/freelist.c:        if (FLP_MAX >= ctx->flp_size + j - 1){
byterun/freelist.c:            memmove (&(ctx->flp[i+j]), 
byterun/freelist.c:                     &(ctx->flp[i+1]), 
byterun/freelist.c:                     sizeof (block *) * (ctx->flp_size-i-1));
byterun/freelist.c:             memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
byterun/freelist.c:          ctx->flp_size += j - 1;
byterun/freelist.c:              memmove (&(ctx->flp[i+j]), 
byterun/freelist.c:                       &(ctx->flp[i+1]), 
byterun/freelist.c:              memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
byterun/freelist.c:              memmove (&(ctx->flp[i]), &buf[0], 
byterun/freelist.c:          ctx->flp_size = FLP_MAX - 1;
byterun/freelist.c:          ctx->beyond = Next (ctx->flp[FLP_MAX - 1]);
byterun/freelist.c:  ctx->last_fragment = NULL;
byterun/freelist.c:  ctx->caml_fl_merge = ctx->fl_head;
byterun/freelist.c:  if (changed == ctx->fl_head){
byterun/freelist.c:    ctx->flp_size = 0;
byterun/freelist.c:    ctx->beyond = NULL;
byterun/freelist.c:    while (ctx->flp_size > 0 && Next (ctx->flp[ctx->flp_size - 1]) >= changed) 
byterun/freelist.c:      -- ctx->flp_size;
byterun/freelist.c:    if (ctx->beyond >= changed) 
byterun/freelist.c:      ctx->beyond = NULL;
byterun/freelist.c:  Next (ctx->fl_head) = NULL;
byterun/freelist.c:    ctx->fl_prev = ctx->fl_head;
byterun/freelist.c:    truncate_flp_r (ctx, ctx->fl_head);
byterun/freelist.c:  ctx->caml_fl_cur_size = 0;
byterun/freelist.c://    last_fragment or Next(ctx->caml_fl_merge).
byterun/freelist.c:  ctx->caml_fl_cur_size += Whsize_hd (hd);
byterun/freelist.c:  prev = ctx->caml_fl_merge;
byterun/freelist.c:  Assert (prev < bp || prev == ctx->fl_head);
byterun/freelist.c:  if (ctx->last_fragment == Hp_bp (bp)){
byterun/freelist.c:      bp = ctx->last_fragment;
byterun/freelist.c:      ctx->caml_fl_cur_size += Whsize_wosize (0);
byterun/freelist.c:      if (policy == Policy_next_fit && ctx->fl_prev == cur) ctx->fl_prev = prev;
byterun/freelist.c:    Assert (ctx->caml_fl_merge == prev);
byterun/freelist.c:    ctx->caml_fl_merge = bp;
byterun/freelist.c:    ctx->last_fragment = bp;
byterun/freelist.c:    ctx->caml_fl_cur_size -= Whsize_wosize (0);
byterun/freelist.c:                                                  Assert (ctx->fl_last != NULL);
byterun/freelist.c:                                           Assert (Next (ctx->fl_last) == NULL);
byterun/freelist.c:  ctx->caml_fl_cur_size += Whsize_bp (bp);
byterun/freelist.c:  if (bp > ctx->fl_last){
byterun/freelist.c:    Next (ctx->fl_last) = bp;
byterun/freelist.c:    if (ctx->fl_last==ctx->caml_fl_merge && bp<ctx->caml_gc_sweep_hp){
byterun/freelist.c:      ctx->caml_fl_merge = (char *) Field (bp, 1);
byterun/freelist.c:    if (policy==Policy_first_fit && ctx->flp_size<FLP_MAX){
byterun/freelist.c:      ctx->flp [ctx->flp_size++] = ctx->fl_last;
byterun/freelist.c:    prev = ctx->fl_head;
byterun/freelist.c:    while (cur != NULL && cur < bp){   Assert (prev < bp || prev == ctx->fl_head);
byterun/freelist.c:    }                                  Assert (prev < bp || prev == ctx->fl_head);
byterun/freelist.c:    if (prev == ctx->caml_fl_merge && bp < ctx->caml_gc_sweep_hp){
byterun/freelist.c:      ctx->caml_fl_merge = (char *) Field (bp, 1);
byterun/freelist.c:    ctx->fl_prev = ctx->fl_head;
byterun/freelist.c:    ctx->flp_size = 0;
byterun/freelist.c:    ctx->beyond = NULL;
byterun/minor_gc.c://  if (ctx->caml_young_ptr != ctx->caml_young_end) caml_minor_collection_r (ctx);
byterun/minor_gc.c:                                    Assert (ctx->caml_young_ptr == ctx->caml_young_end);
byterun/minor_gc.c:  if (ctx->caml_young_start != NULL){
byterun/minor_gc.c:    caml_page_table_remove(In_young, ctx->caml_young_start, ctx->caml_young_end);
byterun/minor_gc.c:    free (ctx->caml_young_base);
byterun/minor_gc.c:  ctx->caml_young_base = new_heap_base;
byterun/minor_gc.c:  ctx->caml_young_start = new_heap;
byterun/minor_gc.c:  ctx->caml_young_end = new_heap + size;
byterun/minor_gc.c:  ctx->caml_young_limit = ctx->caml_young_start;
byterun/minor_gc.c:  ctx->caml_young_ptr = ctx->caml_young_end;
byterun/minor_gc.c:  ctx->caml_minor_heap_size = size;
byterun/minor_gc.c:  reset_table (&(ctx->caml_ref_table));
byterun/minor_gc.c:  reset_table (&(ctx->caml_weak_ref_table));
byterun/minor_gc.c:    Assert (Hp_val (v) >= ctx->caml_young_ptr);
byterun/minor_gc.c:          Field (result, 1) = ctx->oldify_todo_list;    /* Add this block */
byterun/minor_gc.c:          ctx->oldify_todo_list = v;                    /*  to the "to do" list. */
byterun/minor_gc.c:  while (ctx->oldify_todo_list != 0){
byterun/minor_gc.c:    v = ctx->oldify_todo_list;                /* Get the head. */
byterun/minor_gc.c:    ctx->oldify_todo_list = Field (new_v, 1); /* Remove from list. */
byterun/minor_gc.c:  if (ctx->caml_young_ptr != ctx->caml_young_end){
byterun/minor_gc.c:    ctx->caml_in_minor_collection = 1;
byterun/minor_gc.c:    for (r = ctx->caml_ref_table.base; r < ctx->caml_ref_table.ptr; r++){
byterun/minor_gc.c:    for (r = ctx->caml_weak_ref_table.base; r < ctx->caml_weak_ref_table.ptr; r++){
byterun/minor_gc.c:    if (ctx->caml_young_ptr < ctx->caml_young_start)
byterun/minor_gc.c:      ctx->caml_young_ptr = ctx->caml_young_start;
byterun/minor_gc.c:    ctx->caml_stat_minor_words += Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
byterun/minor_gc.c:    ctx->caml_young_ptr = ctx->caml_young_end;
byterun/minor_gc.c:    ctx->caml_young_limit = ctx->caml_young_start;
byterun/minor_gc.c:    clear_table (&(ctx->caml_ref_table));
byterun/minor_gc.c:    clear_table (&(ctx->caml_weak_ref_table));
byterun/minor_gc.c:    ctx->caml_in_minor_collection = 0;
byterun/minor_gc.c:    for (p = (value *) ctx->caml_young_start; p < (value *) ctx->caml_young_end; ++p){
byterun/minor_gc.c:  intnat prev_alloc_words = ctx->caml_allocated_words;
byterun/minor_gc.c:  ctx->caml_stat_promoted_words += ctx->caml_allocated_words - prev_alloc_words;
byterun/minor_gc.c:  ++ ctx->caml_stat_minor_collections;
byterun/minor_gc.c:  ctx->caml_force_major_slice = 0;
byterun/minor_gc.c:  if (ctx->caml_force_major_slice) caml_minor_collection_r(ctx);
byterun/minor_gc.c:    caml_alloc_table (tbl, ctx->caml_minor_heap_size / sizeof (value) / 8, 256);
byterun/minor_gc.c:                                             Assert (ctx->caml_force_major_slice);
byterun/finalise.c:  if (ctx->to_do_tl == NULL){
byterun/finalise.c:    ctx->to_do_hd = result;
byterun/finalise.c:    ctx->to_do_tl = result;
byterun/finalise.c:    Assert (ctx->to_do_tl->next == NULL);
byterun/finalise.c:    ctx->to_do_tl->next = result;
byterun/finalise.c:    ctx->to_do_tl = result;
byterun/finalise.c: * 2. put white blocks in the ctx->final_table into the todo table
byterun/finalise.c:  Assert (ctx->final_young == ctx->final_old);
byterun/finalise.c:  for (i = 0; i < ctx->final_old; i++){
byterun/finalise.c:    Assert (Is_block (ctx->final_table[i].val));
byterun/finalise.c:    Assert (Is_in_heap (ctx->final_table[i].val));
byterun/finalise.c:    if (Is_white_val (ctx->final_table[i].val)) ++ todo_count;
byterun/finalise.c:    for (i = 0; i < ctx->final_old; i++){
byterun/finalise.c:      Assert (Is_block (ctx->final_table[i].val));
byterun/finalise.c:      Assert (Is_in_heap (ctx->final_table[i].val));
byterun/finalise.c:      if (Is_white_val (ctx->final_table[i].val)){
byterun/finalise.c:        if (Tag_val (ctx->final_table[i].val) == Forward_tag){
byterun/finalise.c:          Assert (ctx->final_table[i].offset == 0);
byterun/finalise.c:          fv = Forward_val (ctx->final_table[i].val);
byterun/finalise.c:            ctx->final_table[i].val = fv;
byterun/finalise.c:            if (Is_block (ctx->final_table[i].val)
byterun/finalise.c:                && Is_in_heap (ctx->final_table[i].val)){
byterun/finalise.c:        ctx->to_do_tl->item[k++] = ctx->final_table[i];
byterun/finalise.c:        ctx->final_table[j++] = ctx->final_table[i];
byterun/finalise.c:    ctx->final_young = ctx->final_old = j;
byterun/finalise.c:    ctx->to_do_tl->size = k;
byterun/finalise.c:      CAMLassert (Is_white_val (ctx->to_do_tl->item[i].val));
byterun/finalise.c:      caml_darken_r (ctx, ctx->to_do_tl->item[i].val, NULL);
byterun/finalise.c:  if (ctx->running_finalisation_function) return;
byterun/finalise.c:  if (ctx->to_do_hd != NULL){
byterun/finalise.c:      while (ctx->to_do_hd != NULL && ctx->to_do_hd->size == 0){
byterun/finalise.c:        struct to_do *next_hd = ctx->to_do_hd->next;
byterun/finalise.c:        free (ctx->to_do_hd);
byterun/finalise.c:        ctx->to_do_hd = next_hd;
byterun/finalise.c:        if (ctx->to_do_hd == NULL) ctx->to_do_tl = NULL;
byterun/finalise.c:      if (ctx->to_do_hd == NULL) break;
byterun/finalise.c:      Assert (ctx->to_do_hd->size > 0);
byterun/finalise.c:      // phc - for f in reverse(ctx->to_do_head)
byterun/finalise.c:      -- ctx->to_do_hd->size;
byterun/finalise.c:      f = ctx->to_do_hd->item[ctx->to_do_hd->size];
byterun/finalise.c:      ctx->running_finalisation_function = 1;
byterun/finalise.c:      ctx->running_finalisation_function = 0;
byterun/finalise.c:  Assert (ctx->final_old == ctx->final_young);
byterun/finalise.c:  for (i = 0; i < ctx->final_old; i++) 
byterun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
byterun/finalise.c:  for (todo = ctx->to_do_hd; todo != NULL; todo = todo->next){
byterun/finalise.c:  Assert (ctx->final_old == ctx->final_young);
byterun/finalise.c:  for (i = 0; i < ctx->final_old; i++) 
byterun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
byterun/finalise.c://                 ctx->final_old, ctx->final_young);
byterun/finalise.c:  Assert (ctx->final_old <= ctx->final_young);
byterun/finalise.c:  for (i = ctx->final_old; i < ctx->final_young; i++){
byterun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
byterun/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
byterun/finalise.c:  ctx->final_old = ctx->final_young;
byterun/finalise.c:  Assert (ctx->final_old <= ctx->final_young);
byterun/finalise.c:  if (ctx->final_young >= ctx->final_size){
byterun/finalise.c:    if (ctx->final_table == NULL){
byterun/finalise.c:      ctx->final_table = caml_stat_alloc (new_size * sizeof (struct final));
byterun/finalise.c:      Assert (ctx->final_old == 0);
byterun/finalise.c:      Assert (ctx->final_young == 0);
byterun/finalise.c:      ctx->final_size = new_size;
byterun/finalise.c:      uintnat new_size = ctx->final_size * 2;
byterun/finalise.c:      ctx->final_table = caml_stat_resize (ctx->final_table,
byterun/finalise.c:      ctx->final_size = new_size;
byterun/finalise.c:  Assert (ctx->final_young < ctx->final_size);
byterun/finalise.c:  ctx->final_table[ctx->final_young].fun = f;
byterun/finalise.c:    ctx->final_table[ctx->final_young].offset = Infix_offset_val (v);
byterun/finalise.c:    ctx->final_table[ctx->final_young].val = v - Infix_offset_val (v);
byterun/finalise.c:    ctx->final_table[ctx->final_young].offset = 0;
byterun/finalise.c:    ctx->final_table[ctx->final_young].val = v;
byterun/finalise.c:  ++ ctx->final_young;
byterun/finalise.c:  ctx->running_finalisation_function = 0;
lib/ocaml/caml/memory.h:  struct caml__roots_block *caml__frame = ctx->caml_local_roots
lib/ocaml/caml/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
lib/ocaml/caml/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
lib/ocaml/caml/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
lib/ocaml/caml/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
lib/ocaml/caml/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
lib/ocaml/caml/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
lib/ocaml/caml/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
lib/ocaml/caml/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
lib/ocaml/caml/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
lib/ocaml/caml/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
lib/ocaml/caml/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
lib/ocaml/caml/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
lib/ocaml/caml/memory.h:  ctx->caml_local_roots = caml__frame; \
lib/ocaml/caml/memory.h:  ctx->caml_local_roots = caml__frame; \
stdlib_r/caml_tmp/gc_ctrl.c:  char *chunk = ctx->caml_heap_start, *chunk_end;
stdlib_r/caml_tmp/gc_ctrl.c:                  || cur_hp == ctx->caml_gc_sweep_hp);
stdlib_r/caml_tmp/gc_ctrl.c:          if (ctx->caml_gc_phase == Phase_sweep && cur_hp >= ctx->caml_gc_sweep_hp){
stdlib_r/caml_tmp/gc_ctrl.c:  Assert (heap_chunks == ctx->caml_stat_heap_chunks);
stdlib_r/caml_tmp/gc_ctrl.c:          == Wsize_bsize (ctx->caml_stat_heap_size));
stdlib_r/caml_tmp/gc_ctrl.c:    double minwords = ctx->caml_stat_minor_words
stdlib_r/caml_tmp/gc_ctrl.c:                      + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/gc_ctrl.c:    double prowords = ctx->caml_stat_promoted_words;
stdlib_r/caml_tmp/gc_ctrl.c:    double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
stdlib_r/caml_tmp/gc_ctrl.c:    intnat mincoll = ctx->caml_stat_minor_collections;
stdlib_r/caml_tmp/gc_ctrl.c:    intnat majcoll = ctx->caml_stat_major_collections;
stdlib_r/caml_tmp/gc_ctrl.c:    intnat heap_words = Wsize_bsize (ctx->caml_stat_heap_size);
stdlib_r/caml_tmp/gc_ctrl.c:    intnat cpct = ctx->caml_stat_compactions;
stdlib_r/caml_tmp/gc_ctrl.c:    intnat top_heap_words = Wsize_bsize (ctx->caml_stat_top_heap_size);
stdlib_r/caml_tmp/gc_ctrl.c:  double prowords = ctx->caml_stat_promoted_words;
stdlib_r/caml_tmp/gc_ctrl.c:  double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
stdlib_r/caml_tmp/gc_ctrl.c:  intnat mincoll = ctx->caml_stat_minor_collections;
stdlib_r/caml_tmp/gc_ctrl.c:  intnat majcoll = ctx->caml_stat_major_collections;
stdlib_r/caml_tmp/gc_ctrl.c:  intnat heap_words = ctx->caml_stat_heap_size / sizeof (value);
stdlib_r/caml_tmp/gc_ctrl.c:  intnat top_heap_words = ctx->caml_stat_top_heap_size / sizeof (value);
stdlib_r/caml_tmp/gc_ctrl.c:  intnat cpct = ctx->caml_stat_compactions;
stdlib_r/caml_tmp/gc_ctrl.c:  intnat heap_chunks = ctx->caml_stat_heap_chunks;
stdlib_r/caml_tmp/gc_ctrl.c:  minwords = ctx->caml_stat_minor_words
stdlib_r/caml_tmp/gc_ctrl.c:             + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/gc_ctrl.c:  double minwords = ctx->caml_stat_minor_words
stdlib_r/caml_tmp/gc_ctrl.c:                    + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/gc_ctrl.c:  double prowords = ctx->caml_stat_promoted_words;
stdlib_r/caml_tmp/gc_ctrl.c:  double majwords = ctx->caml_stat_major_words + (double) ctx->caml_allocated_words;
stdlib_r/caml_tmp/gc_ctrl.c:  Store_field (res, 0, Val_long (Wsize_bsize (ctx->caml_minor_heap_size)));  /* s */
stdlib_r/caml_tmp/gc_ctrl.c:  Store_field (res, 1,Val_long(Wsize_bsize(ctx->caml_major_heap_increment)));/* i */
stdlib_r/caml_tmp/gc_ctrl.c:  Store_field (res, 2, Val_long (ctx->caml_percent_free));                   /* o */
stdlib_r/caml_tmp/gc_ctrl.c:  Store_field (res, 4, Val_long (ctx->caml_percent_max));                    /* O */
stdlib_r/caml_tmp/gc_ctrl.c:  if (newpf != ctx->caml_percent_free){
stdlib_r/caml_tmp/gc_ctrl.c:    ctx->caml_percent_free = newpf;
stdlib_r/caml_tmp/gc_ctrl.c:    caml_gc_message (0x20, "New space overhead: %d%%\n", ctx->caml_percent_free);
stdlib_r/caml_tmp/gc_ctrl.c:  if (newpm != ctx->caml_percent_max){
stdlib_r/caml_tmp/gc_ctrl.c:    ctx->caml_percent_max = newpm;
stdlib_r/caml_tmp/gc_ctrl.c:    caml_gc_message (0x20, "New max overhead: %d%%\n", ctx->caml_percent_max);
stdlib_r/caml_tmp/gc_ctrl.c:  if (newheapincr != ctx->caml_major_heap_increment){
stdlib_r/caml_tmp/gc_ctrl.c:    ctx->caml_major_heap_increment = newheapincr;
stdlib_r/caml_tmp/gc_ctrl.c:                     ctx->caml_major_heap_increment/1024);
stdlib_r/caml_tmp/gc_ctrl.c:  if (newminsize != ctx->caml_minor_heap_size){
stdlib_r/caml_tmp/gc_ctrl.c:  fp = 100.0 * ctx->caml_fl_cur_size
stdlib_r/caml_tmp/gc_ctrl.c:       / (Wsize_bsize (caml_stat_heap_size) - ctx->caml_fl_cur_size);
stdlib_r/caml_tmp/gc_ctrl.c:  if (fp >= ctx->caml_percent_max && ctx->caml_stat_heap_chunks > 1){
stdlib_r/caml_tmp/gc_ctrl.c:  ctx->caml_major_heap_increment = Bsize_wsize (norm_heapincr (major_incr));
stdlib_r/caml_tmp/gc_ctrl.c:  ctx->caml_percent_free = norm_pfree (percent_fr);
stdlib_r/caml_tmp/gc_ctrl.c:                   ctx->caml_minor_heap_size / 1024);
stdlib_r/caml_tmp/gc_ctrl.c:  caml_gc_message (0x20, "Initial space overhead: %lu%%\n", ctx->caml_percent_free);
stdlib_r/caml_tmp/gc_ctrl.c:                   ctx->caml_major_heap_increment / 1024);
stdlib_r/caml_tmp/printexc.c:  saved_backtrace_active = ctx->caml_backtrace_active;
stdlib_r/caml_tmp/printexc.c:  saved_backtrace_pos = ctx->caml_backtrace_pos;
stdlib_r/caml_tmp/printexc.c:  ctx->caml_backtrace_active = 0;
stdlib_r/caml_tmp/printexc.c:  ctx->caml_backtrace_active = saved_backtrace_active;
stdlib_r/caml_tmp/printexc.c:  ctx->caml_backtrace_pos = saved_backtrace_pos;
stdlib_r/caml_tmp/printexc.c:  if (ctx->caml_backtrace_active
stdlib_r/caml_tmp/memory.c:                   (ctx->caml_stat_heap_size + Chunk_size (m)) / 1024);
stdlib_r/caml_tmp/memory.c:    char **last = &ctx->caml_heap_start;
stdlib_r/caml_tmp/memory.c:    ++ ctx->caml_stat_heap_chunks;
stdlib_r/caml_tmp/memory.c:  ctx->caml_stat_heap_size += Chunk_size (m);
stdlib_r/caml_tmp/memory.c:  if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
stdlib_r/caml_tmp/memory.c:    ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
stdlib_r/caml_tmp/memory.c:  over_request = request + request / 100 * ctx->caml_percent_free;
stdlib_r/caml_tmp/memory.c:  if (chunk == ctx->caml_heap_start) return;
stdlib_r/caml_tmp/memory.c:  ctx->caml_stat_heap_size -= Chunk_size (chunk);
stdlib_r/caml_tmp/memory.c:                   (unsigned long) ctx->caml_stat_heap_size / 1024);
stdlib_r/caml_tmp/memory.c:  -- ctx->caml_stat_heap_chunks;
stdlib_r/caml_tmp/memory.c:  cp = &ctx->caml_heap_start;
stdlib_r/caml_tmp/memory.c:  if (ctx->caml_gc_phase == Phase_mark
stdlib_r/caml_tmp/memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
stdlib_r/caml_tmp/memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
stdlib_r/caml_tmp/memory.c:            || (ctx->caml_gc_phase == Phase_sweep
stdlib_r/caml_tmp/memory.c:                && (addr)hp < (addr)ctx->caml_gc_sweep_hp));
stdlib_r/caml_tmp/memory.c:      if (ctx->caml_in_minor_collection)
stdlib_r/caml_tmp/memory.c:  if (ctx->caml_gc_phase == Phase_mark
stdlib_r/caml_tmp/memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
stdlib_r/caml_tmp/memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
stdlib_r/caml_tmp/memory.c:            || (ctx->caml_gc_phase == Phase_sweep
stdlib_r/caml_tmp/memory.c:                && (addr)hp < (addr)ctx->caml_gc_sweep_hp));
stdlib_r/caml_tmp/memory.c:  ctx->caml_allocated_words += Whsize_wosize (wosize);
stdlib_r/caml_tmp/memory.c:  if (ctx->caml_allocated_words > Wsize_bsize (ctx->caml_minor_heap_size)){
stdlib_r/caml_tmp/memory.c:  ctx->caml_extra_heap_resources += (double) res / (double) max;
stdlib_r/caml_tmp/memory.c:  if (ctx->caml_extra_heap_resources > 1.0){
stdlib_r/caml_tmp/memory.c:    ctx->caml_extra_heap_resources = 1.0;
stdlib_r/caml_tmp/memory.c:  if (ctx->caml_extra_heap_resources
stdlib_r/caml_tmp/memory.c:           > (double) Wsize_bsize (ctx->caml_minor_heap_size) / 2.0
stdlib_r/caml_tmp/memory.c:             / (double) Wsize_bsize (ctx->caml_stat_heap_size)) {
stdlib_r/caml_tmp/memory.c:    if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){
stdlib_r/caml_tmp/memory.c:      caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);
stdlib_r/caml_tmp/memory.c:    *(ctx->caml_ref_table.ptr++) = fp;
stdlib_r/caml_tmp/globroots.c:  r = ctx->random_seed = ctx->random_seed * 69069 + 25173;
stdlib_r/caml_tmp/globroots.c:  caml_insert_global_root_r(ctx, &ctx->caml_global_roots, r);
stdlib_r/caml_tmp/globroots.c:  caml_delete_global_root_r(ctx, &ctx->caml_global_roots, r);
stdlib_r/caml_tmp/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/globroots.c:    caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
stdlib_r/caml_tmp/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
stdlib_r/caml_tmp/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_old);
stdlib_r/caml_tmp/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
stdlib_r/caml_tmp/globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
stdlib_r/caml_tmp/globroots.c:  for (gr = ctx->caml_global_roots_young.forward[0];
stdlib_r/caml_tmp/globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, gr->root);
stdlib_r/caml_tmp/globroots.c:  caml_empty_global_roots(&ctx->caml_global_roots_young);
stdlib_r/caml_tmp/md5.c:    ctx->buf[0] = 0x67452301;
stdlib_r/caml_tmp/md5.c:    ctx->buf[1] = 0xefcdab89;
stdlib_r/caml_tmp/md5.c:    ctx->buf[2] = 0x98badcfe;
stdlib_r/caml_tmp/md5.c:    ctx->buf[3] = 0x10325476;
stdlib_r/caml_tmp/md5.c:    ctx->bits[0] = 0;
stdlib_r/caml_tmp/md5.c:    ctx->bits[1] = 0;
stdlib_r/caml_tmp/md5.c:    t = ctx->bits[0];
stdlib_r/caml_tmp/md5.c:    if ((ctx->bits[0] = t + ((uint32) len << 3)) < t)
stdlib_r/caml_tmp/md5.c:        ctx->bits[1]++;         /* Carry from low to high */
stdlib_r/caml_tmp/md5.c:    ctx->bits[1] += len >> 29;
stdlib_r/caml_tmp/md5.c:        unsigned char *p = (unsigned char *) ctx->in + t;
stdlib_r/caml_tmp/md5.c:        byteReverse(ctx->in, 16);
stdlib_r/caml_tmp/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/md5.c:        memcpy(ctx->in, buf, 64);
stdlib_r/caml_tmp/md5.c:        byteReverse(ctx->in, 16);
stdlib_r/caml_tmp/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/md5.c:    memcpy(ctx->in, buf, len);
stdlib_r/caml_tmp/md5.c:    count = (ctx->bits[0] >> 3) & 0x3F;
stdlib_r/caml_tmp/md5.c:    p = ctx->in + count;
stdlib_r/caml_tmp/md5.c:        byteReverse(ctx->in, 16);
stdlib_r/caml_tmp/md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/md5.c:        memset(ctx->in, 0, 56);
stdlib_r/caml_tmp/md5.c:    byteReverse(ctx->in, 14);
stdlib_r/caml_tmp/md5.c:    ((uint32 *) ctx->in)[14] = ctx->bits[0];
stdlib_r/caml_tmp/md5.c:    ((uint32 *) ctx->in)[15] = ctx->bits[1];
stdlib_r/caml_tmp/md5.c:    caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/md5.c:    byteReverse((unsigned char *) ctx->buf, 4);
stdlib_r/caml_tmp/md5.c:    memcpy(digest, ctx->buf, 16);
stdlib_r/caml_tmp/minor_gc.h:   (addr)(val) < (addr)ctx->caml_young_end && (addr)(val) > (addr)ctx->caml_young_start)
stdlib_r/caml_tmp/signals.c:  ctx->caml_young_limit = ctx->caml_young_end;
stdlib_r/caml_tmp/signals.c:  if (main_ctx) main_ctx->caml_young_limit = main_ctx->caml_young_end;
stdlib_r/caml_tmp/signals.c:  ctx->caml_force_major_slice = 1;
stdlib_r/caml_tmp/signals.c:  ctx->caml_young_limit = ctx->caml_young_end;
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) {
stdlib_r/caml_tmp/compare.c:    free(ctx->compare_stack);
stdlib_r/caml_tmp/compare.c:    ctx->compare_stack = ctx->compare_stack_init;
stdlib_r/caml_tmp/compare.c:    ctx->compare_stack_limit = ctx->compare_stack + COMPARE_STACK_INIT_SIZE;
stdlib_r/caml_tmp/compare.c:  asize_t newsize = 2 * (ctx->compare_stack_limit - ctx->compare_stack);
stdlib_r/caml_tmp/compare.c:  asize_t sp_offset = sp - ctx->compare_stack;
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack == ctx->compare_stack_init) {
stdlib_r/caml_tmp/compare.c:    memcpy(newstack, ctx->compare_stack_init,
stdlib_r/caml_tmp/compare.c:      realloc(ctx->compare_stack, sizeof(struct compare_item) * newsize);
stdlib_r/caml_tmp/compare.c:  ctx->compare_stack = newstack;
stdlib_r/caml_tmp/compare.c:  ctx->compare_stack_limit = newstack + newsize;
stdlib_r/caml_tmp/compare.c:  sp = ctx->compare_stack;
stdlib_r/caml_tmp/compare.c:          ctx->caml_compare_unordered = 0;
stdlib_r/caml_tmp/compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
stdlib_r/caml_tmp/compare.c:          ctx->caml_compare_unordered = 0;
stdlib_r/caml_tmp/compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
stdlib_r/caml_tmp/compare.c:      ctx->caml_compare_unordered = 0;
stdlib_r/caml_tmp/compare.c:      if (ctx->caml_compare_unordered && !total) return UNORDERED;
stdlib_r/caml_tmp/compare.c:        if (sp >= ctx->compare_stack_limit) sp = compare_resize_stack_r(ctx, sp);
stdlib_r/caml_tmp/compare.c:    if (sp == ctx->compare_stack) return EQUAL; /* we're done */
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/major_gc.c:  Assert (ctx->gray_vals_cur == ctx->gray_vals_end);
stdlib_r/caml_tmp/major_gc.c:  if (ctx->gray_vals_size < ctx->caml_stat_heap_size / 128){
stdlib_r/caml_tmp/major_gc.c:                     (intnat) ctx->gray_vals_size * sizeof (value) / 512);
stdlib_r/caml_tmp/major_gc.c:    new = (value *) realloc ((char *) ctx->gray_vals,
stdlib_r/caml_tmp/major_gc.c:                             2 * ctx->gray_vals_size * sizeof (value));
stdlib_r/caml_tmp/major_gc.c:      ctx->gray_vals_cur = ctx->gray_vals;
stdlib_r/caml_tmp/major_gc.c:      ctx->heap_is_pure = 0;
stdlib_r/caml_tmp/major_gc.c:      ctx->gray_vals = new;
stdlib_r/caml_tmp/major_gc.c:      ctx->gray_vals_cur = ctx->gray_vals + ctx->gray_vals_size;
stdlib_r/caml_tmp/major_gc.c:      ctx->gray_vals_size *= 2;
stdlib_r/caml_tmp/major_gc.c:      ctx->gray_vals_end = ctx->gray_vals + ctx->gray_vals_size;
stdlib_r/caml_tmp/major_gc.c:    ctx->gray_vals_cur = ctx->gray_vals + ctx->gray_vals_size / 2;
stdlib_r/caml_tmp/major_gc.c:    ctx->heap_is_pure = 0;
stdlib_r/caml_tmp/major_gc.c:        *ctx->gray_vals_cur++ = v;
stdlib_r/caml_tmp/major_gc.c:        if (ctx->gray_vals_cur >= ctx->gray_vals_end) realloc_gray_vals_r (ctx);
stdlib_r/caml_tmp/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/major_gc.c:  Assert (ctx->gray_vals_cur == gray_vals);
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_gc_phase = Phase_mark;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_gc_subphase = Subphase_main;
stdlib_r/caml_tmp/major_gc.c:  ctx->markhp = NULL;
stdlib_r/caml_tmp/major_gc.c:  ++ ctx->major_gc_counter;
stdlib_r/caml_tmp/major_gc.c:  caml_gc_message (0x40, "Subphase = %ld\n", ctx->caml_gc_subphase);
stdlib_r/caml_tmp/major_gc.c:  gray_vals_ptr = ctx->gray_vals_cur;
stdlib_r/caml_tmp/major_gc.c:    if (gray_vals_ptr > ctx->gray_vals){
stdlib_r/caml_tmp/major_gc.c:              if (gray_vals_ptr >= ctx->gray_vals_end) {
stdlib_r/caml_tmp/major_gc.c:                ctx->gray_vals_cur = gray_vals_ptr;
stdlib_r/caml_tmp/major_gc.c:                gray_vals_ptr = ctx->gray_vals_cur;
stdlib_r/caml_tmp/major_gc.c:    }else if (ctx->markhp != NULL){       // phc - keep processing current chunk
stdlib_r/caml_tmp/major_gc.c:      if (ctx->markhp == ctx->limit){     // this chunk is done with marking
stdlib_r/caml_tmp/major_gc.c:        ctx->chunk = Chunk_next (ctx->chunk);
stdlib_r/caml_tmp/major_gc.c:        if (ctx->chunk == NULL){          // no more chunk to mark
stdlib_r/caml_tmp/major_gc.c:          ctx->markhp = NULL;
stdlib_r/caml_tmp/major_gc.c:          ctx->markhp = ctx->chunk;       // next chunk
stdlib_r/caml_tmp/major_gc.c:          ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/major_gc.c:        if (Is_gray_val (Val_hp (ctx->markhp))){
stdlib_r/caml_tmp/major_gc.c:          Assert (gray_vals_ptr == ctx->gray_vals);
stdlib_r/caml_tmp/major_gc.c:          *gray_vals_ptr++ = Val_hp (ctx->markhp);
stdlib_r/caml_tmp/major_gc.c:        ctx->markhp += Bhsize_hp (ctx->markhp); // mark next block as todo
stdlib_r/caml_tmp/major_gc.c:      ctx->chunk = ctx->caml_heap_start;
stdlib_r/caml_tmp/major_gc.c:      ctx->markhp = ctx->chunk;
stdlib_r/caml_tmp/major_gc.c:      ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/major_gc.c:      switch (ctx->caml_gc_subphase){
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_gc_subphase = Subphase_weak1;
stdlib_r/caml_tmp/major_gc.c:        ctx->weak_prev = &(ctx->caml_weak_list_head);
stdlib_r/caml_tmp/major_gc.c:        cur = *(ctx->weak_prev);
stdlib_r/caml_tmp/major_gc.c:          ctx->weak_prev = &Field (cur, 0);
stdlib_r/caml_tmp/major_gc.c:          ctx->gray_vals_cur = gray_vals_ptr;
stdlib_r/caml_tmp/major_gc.c:          gray_vals_ptr = ctx->gray_vals_cur;
stdlib_r/caml_tmp/major_gc.c:          ctx->caml_gc_subphase = Subphase_weak2;
stdlib_r/caml_tmp/major_gc.c:          ctx->weak_prev = &(ctx->caml_weak_list_head);
stdlib_r/caml_tmp/major_gc.c:        cur = *ctx->weak_prev;
stdlib_r/caml_tmp/major_gc.c:            *ctx->weak_prev = Field (cur, 0);
stdlib_r/caml_tmp/major_gc.c:            // ctx->weak_prev==&ptr
stdlib_r/caml_tmp/major_gc.c:            ctx->weak_prev = &Field (cur, 0);
stdlib_r/caml_tmp/major_gc.c:          ctx->caml_gc_subphase = Subphase_final;
stdlib_r/caml_tmp/major_gc.c:        ctx->gray_vals_cur = gray_vals_ptr;
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->caml_heap_start;
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_gc_phase = Phase_sweep;
stdlib_r/caml_tmp/major_gc.c:        ctx->chunk = ctx->caml_heap_start;
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->chunk;
stdlib_r/caml_tmp/major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_fl_size_at_phase_change = ctx->caml_fl_cur_size;
stdlib_r/caml_tmp/major_gc.c:  ctx->gray_vals_cur = gray_vals_ptr;
stdlib_r/caml_tmp/major_gc.c:    if (ctx->caml_gc_sweep_hp < ctx->limit){
stdlib_r/caml_tmp/major_gc.c:      hp = ctx->caml_gc_sweep_hp;
stdlib_r/caml_tmp/major_gc.c:      ctx->caml_gc_sweep_hp += Bhsize_hd (hd);
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_gc_sweep_hp = caml_fl_merge_block_r (ctx, Bp_hp (hp));
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_fl_merge = Bp_hp (hp);
stdlib_r/caml_tmp/major_gc.c:      Assert (ctx->caml_gc_sweep_hp <= ctx->limit);
stdlib_r/caml_tmp/major_gc.c:      ctx->chunk = Chunk_next (ctx->chunk);
stdlib_r/caml_tmp/major_gc.c:      if (ctx->chunk == NULL){
stdlib_r/caml_tmp/major_gc.c:        ++ (ctx->caml_stat_major_collections);
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_gc_phase = Phase_idle;
stdlib_r/caml_tmp/major_gc.c:        ctx->caml_gc_sweep_hp = ctx->chunk;
stdlib_r/caml_tmp/major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
stdlib_r/caml_tmp/major_gc.c:  p = (double) ctx->caml_allocated_words * 3.0 * (100 + ctx->caml_percent_free)
stdlib_r/caml_tmp/major_gc.c:      / Wsize_bsize (ctx->caml_stat_heap_size) / ctx->caml_percent_free / 2.0;
stdlib_r/caml_tmp/major_gc.c:  if (ctx->caml_dependent_size > 0){
stdlib_r/caml_tmp/major_gc.c:    dp = (double) ctx->caml_dependent_allocated * (100 + ctx->caml_percent_free)
stdlib_r/caml_tmp/major_gc.c:         / ctx->caml_dependent_size / ctx->caml_percent_free;
stdlib_r/caml_tmp/major_gc.c:  if (p < ctx->caml_extra_heap_resources) p = ctx->caml_extra_heap_resources;
stdlib_r/caml_tmp/major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
stdlib_r/caml_tmp/major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 250
stdlib_r/caml_tmp/major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 5 / 3);
stdlib_r/caml_tmp/major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
stdlib_r/caml_tmp/major_gc.c:    Assert (ctx->caml_gc_phase == Phase_sweep);
stdlib_r/caml_tmp/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) caml_compact_heap_maybe_r (ctx);
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_stat_major_words += ctx->caml_allocated_words;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_allocated_words = 0;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_dependent_allocated = 0;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_extra_heap_resources = 0.0;
stdlib_r/caml_tmp/major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
stdlib_r/caml_tmp/major_gc.c:  while (ctx->caml_gc_phase == Phase_mark) mark_slice_r (ctx, LONG_MAX);
stdlib_r/caml_tmp/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_sweep);
stdlib_r/caml_tmp/major_gc.c:  while (ctx->caml_gc_phase == Phase_sweep) sweep_slice_r (ctx, LONG_MAX);
stdlib_r/caml_tmp/major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_stat_major_words += ctx->caml_allocated_words;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_allocated_words = 0;
stdlib_r/caml_tmp/major_gc.c:  if (result < ctx->caml_major_heap_increment){
stdlib_r/caml_tmp/major_gc.c:    result = ctx->caml_major_heap_increment;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_stat_heap_size = clip_heap_chunk_size (heap_size);
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
stdlib_r/caml_tmp/major_gc.c:  Assert (ctx->caml_stat_heap_size % Page_size == 0);
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_heap_start = (char *) caml_alloc_for_heap (ctx->caml_stat_heap_size);
stdlib_r/caml_tmp/major_gc.c:  if (ctx->caml_heap_start == NULL)
stdlib_r/caml_tmp/major_gc.c:  Chunk_next (ctx->caml_heap_start) = NULL;
stdlib_r/caml_tmp/major_gc.c:  if (caml_page_table_add(In_heap, ctx->caml_heap_start,
stdlib_r/caml_tmp/major_gc.c:                          ctx->caml_heap_start + ctx->caml_stat_heap_size) != 0) {
stdlib_r/caml_tmp/major_gc.c:  caml_make_free_blocks_r (ctx, (value *) ctx->caml_heap_start,
stdlib_r/caml_tmp/major_gc.c:                         Wsize_bsize (ctx->caml_stat_heap_size), 1, Caml_white);
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_gc_phase = Phase_idle;
stdlib_r/caml_tmp/major_gc.c:  ctx->gray_vals_size = 2048;
stdlib_r/caml_tmp/major_gc.c:  ctx->gray_vals = (value *) malloc (ctx->gray_vals_size * sizeof (value));
stdlib_r/caml_tmp/major_gc.c:  if (ctx->gray_vals == NULL)
stdlib_r/caml_tmp/major_gc.c:  ctx->gray_vals_cur = ctx->gray_vals;
stdlib_r/caml_tmp/major_gc.c:  ctx->gray_vals_end = ctx->gray_vals + ctx->gray_vals_size;
stdlib_r/caml_tmp/major_gc.c:  ctx->heap_is_pure = 1;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_allocated_words = 0;
stdlib_r/caml_tmp/major_gc.c:  ctx->caml_extra_heap_resources = 0.0;
stdlib_r/caml_tmp/callback.c:  for (nv = ctx->named_value_table[h]; nv != NULL; nv = nv->next) {
stdlib_r/caml_tmp/callback.c:  nv->next = ctx->named_value_table[h];
stdlib_r/caml_tmp/callback.c:  ctx->named_value_table[h] = nv;
stdlib_r/caml_tmp/callback.c:  for (nv = ctx->named_value_table[hash_value_name(name)];
stdlib_r/caml_tmp/compact.c:  char *ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:  ctx->compact_fl = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:  while (Chunk_size (ctx->compact_fl) - Chunk_alloc (ctx->compact_fl) <= Bhsize_wosize (3)
stdlib_r/caml_tmp/compact.c:         && Chunk_size (Chunk_next (ctx->compact_fl))
stdlib_r/caml_tmp/compact.c:            - Chunk_alloc (Chunk_next (ctx->compact_fl))
stdlib_r/caml_tmp/compact.c:    ctx->compact_fl = Chunk_next (ctx->compact_fl);
stdlib_r/caml_tmp/compact.c:  chunk = ctx->compact_fl;
stdlib_r/caml_tmp/compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/compact.c:    ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:    ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:      value *pp = &ctx->caml_weak_list_head;
stdlib_r/caml_tmp/compact.c:          if (Field (p,i) != ctx->caml_weak_none){
stdlib_r/caml_tmp/compact.c:    ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:    ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:    ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:    wanted = ctx->caml_percent_free * (live / 100 + 1);
stdlib_r/caml_tmp/compact.c:    ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:    ch = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:  ++ ctx->caml_stat_compactions;
stdlib_r/caml_tmp/compact.c:  live = Wsize_bsize (ctx->caml_stat_heap_size) - ctx->caml_fl_cur_size;
stdlib_r/caml_tmp/compact.c:  target_words = live + ctx->caml_percent_free * (live / 100 + 1)
stdlib_r/caml_tmp/compact.c:  if (target_size < ctx->caml_stat_heap_size / 2){
stdlib_r/caml_tmp/compact.c:    Chunk_next (chunk) = ctx->caml_heap_start;
stdlib_r/caml_tmp/compact.c:    ctx->caml_heap_start = chunk;
stdlib_r/caml_tmp/compact.c:    ++ ctx->caml_stat_heap_chunks;
stdlib_r/caml_tmp/compact.c:    ctx->caml_stat_heap_size += Chunk_size (chunk);
stdlib_r/caml_tmp/compact.c:    if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
stdlib_r/caml_tmp/compact.c:      ctx->caml_stat_top_heap_size = ctx->caml_stat_heap_size;
stdlib_r/caml_tmp/compact.c:    Assert (ctx->caml_stat_heap_chunks == 1);
stdlib_r/caml_tmp/compact.c:    Assert (Chunk_next (ctx->caml_heap_start) == NULL);
stdlib_r/caml_tmp/compact.c:    Assert (ctx->caml_stat_heap_size == Chunk_size (chunk));
stdlib_r/caml_tmp/compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/compact.c:  if (ctx->caml_percent_max >= 1000000) return;
stdlib_r/caml_tmp/compact.c:  if (ctx->caml_stat_major_collections < 3) return;
stdlib_r/caml_tmp/compact.c:  fw = 3.0 * ctx->caml_fl_cur_size - 2.0 * ctx->caml_fl_size_at_phase_change;
stdlib_r/caml_tmp/compact.c:  if (fw < 0) fw = ctx->caml_fl_cur_size;
stdlib_r/caml_tmp/compact.c:  if (fw >= Wsize_bsize (ctx->caml_stat_heap_size)){
stdlib_r/caml_tmp/compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
stdlib_r/caml_tmp/compact.c:                   (uintnat) ctx->caml_fl_size_at_phase_change);
stdlib_r/caml_tmp/compact.c:  if (fp >= ctx->caml_percent_max){
stdlib_r/caml_tmp/compact.c:    fw = ctx->caml_fl_cur_size;
stdlib_r/caml_tmp/compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
stdlib_r/caml_tmp/win32.c:  DWORD *ctx_ip = &(ctx->Eip);
stdlib_r/caml_tmp/win32.c:  DWORD *ctx_sp = &(ctx->Esp);
stdlib_r/caml_tmp/io.c:  channel->next = ctx->caml_all_opened_channels;
stdlib_r/caml_tmp/io.c:  if (ctx->caml_all_opened_channels != NULL)
stdlib_r/caml_tmp/io.c:    ctx->caml_all_opened_channels->prev = channel;
stdlib_r/caml_tmp/io.c:  ctx->caml_all_opened_channels = channel;
stdlib_r/caml_tmp/io.c:    Assert (channel == ctx->caml_all_opened_channels);
stdlib_r/caml_tmp/io.c:    ctx->caml_all_opened_channels = ctx->caml_all_opened_channels->next;
stdlib_r/caml_tmp/io.c:    if (ctx->caml_all_opened_channels != NULL)
stdlib_r/caml_tmp/io.c:      ctx->caml_all_opened_channels->prev = NULL;
stdlib_r/caml_tmp/io.c:  for (channel = ctx->caml_all_opened_channels;
stdlib_r/caml_tmp/weak.c:  for (i = 1; i < size; i++) Field (res, i) = ctx->caml_weak_none;
stdlib_r/caml_tmp/weak.c:  Field (res, 0) = ctx->caml_weak_list_head;
stdlib_r/caml_tmp/weak.c:  ctx->caml_weak_list_head = res;
stdlib_r/caml_tmp/weak.c:      if (ctx->caml_weak_ref_table.ptr >= ctx->caml_weak_ref_table.limit){
stdlib_r/caml_tmp/weak.c:        CAMLassert (ctx->caml_weak_ref_table.ptr == ctx->caml_weak_ref_table.limit);
stdlib_r/caml_tmp/weak.c:        caml_realloc_ref_table_r (ctx, &(ctx->caml_weak_ref_table));
stdlib_r/caml_tmp/weak.c:      *(ctx->caml_weak_ref_table.ptr++) = &Field (ar, offset);
stdlib_r/caml_tmp/weak.c:    Field (ar, offset) = ctx->caml_weak_none;
stdlib_r/caml_tmp/weak.c:  if (Field (ar, offset) == ctx->caml_weak_none){
stdlib_r/caml_tmp/weak.c:    if (ctx->caml_gc_phase == Phase_mark && Is_block (elt) && Is_in_heap (elt)){
stdlib_r/caml_tmp/weak.c:  if (v == ctx->caml_weak_none) CAMLreturn (None_val);
stdlib_r/caml_tmp/weak.c:    if (v == ctx->caml_weak_none) CAMLreturn (None_val);
stdlib_r/caml_tmp/weak.c:        if (ctx->caml_gc_phase == Phase_mark && Is_block (f) && Is_in_heap (f)){
stdlib_r/caml_tmp/weak.c:  return Val_bool (Field (ar, offset) != ctx->caml_weak_none);
stdlib_r/caml_tmp/weak.c:  if (ctx->caml_gc_phase == Phase_mark && ctx->caml_gc_subphase == Subphase_weak1){
stdlib_r/caml_tmp/weak.c:      if (v != ctx->caml_weak_none && Is_block (v) && Is_in_heap (v)
stdlib_r/caml_tmp/weak.c:        Field (ars, offset_s + i) = ctx->caml_weak_none;
stdlib_r/caml_tmp/custom.c:  l->next = ctx->custom_ops_table;
stdlib_r/caml_tmp/custom.c:  ctx->custom_ops_table = l;
stdlib_r/caml_tmp/custom.c:  for (l = ctx->custom_ops_table; l != NULL; l = l->next)
stdlib_r/caml_tmp/custom.c:  for (l = ctx->custom_ops_final_table; l != NULL; l = l->next)
stdlib_r/caml_tmp/custom.c:  l->next = ctx->custom_ops_final_table;
stdlib_r/caml_tmp/custom.c:  ctx->custom_ops_final_table = l;
stdlib_r/caml_tmp/memory.h:  ctx->caml_young_ptr -= Bhsize_wosize (wosize);                            \
stdlib_r/caml_tmp/memory.h:  if (ctx->caml_young_ptr < ctx->caml_young_start){                              \
stdlib_r/caml_tmp/memory.h:    ctx->caml_young_ptr += Bhsize_wosize (wosize);                          \
stdlib_r/caml_tmp/memory.h:    ctx->caml_young_ptr -= Bhsize_wosize (wosize);                          \
stdlib_r/caml_tmp/memory.h:  Hd_hp (ctx->caml_young_ptr) = Make_header ((wosize), (tag), Caml_black);  \
stdlib_r/caml_tmp/memory.h:  (result) = Val_hp (ctx->caml_young_ptr);                                  \
stdlib_r/caml_tmp/memory.h:    if (ctx->caml_gc_phase == Phase_mark) caml_darken_r (ctx, _old_, NULL); \
stdlib_r/caml_tmp/memory.h:      if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){            \
stdlib_r/caml_tmp/memory.h:        CAMLassert (ctx->caml_ref_table.ptr == ctx->caml_ref_table.limit);  \
stdlib_r/caml_tmp/memory.h:        caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);               \
stdlib_r/caml_tmp/memory.h:      *(ctx->caml_ref_table).ptr++ = (fp);                                  \
stdlib_r/caml_tmp/memory.h:  struct caml__roots_block *caml__frame = ctx->caml_local_roots
stdlib_r/caml_tmp/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
stdlib_r/caml_tmp/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
stdlib_r/caml_tmp/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
stdlib_r/caml_tmp/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
stdlib_r/caml_tmp/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
stdlib_r/caml_tmp/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/memory.h:    (caml__roots_##x.next = ctx->caml_local_roots), \
stdlib_r/caml_tmp/memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/memory.h:  ctx->caml_local_roots = caml__frame; \
stdlib_r/caml_tmp/memory.h:  ctx->caml_local_roots = caml__frame; \
stdlib_r/caml_tmp/context.c:  ctx->major_gc_counter = 0;
stdlib_r/caml_tmp/context.c:  return Val_int(ctx->count_id);
stdlib_r/caml_tmp/context.c:  printf("caml_young_ptr     : %p\n", (void*)ctx->caml_young_ptr);
stdlib_r/caml_tmp/context.c:  printf("caml_young_limit   : %p\n", (void*)ctx->caml_young_limit);
stdlib_r/caml_tmp/context.c:  printf("caml_young_base    : %p\n", (void*)ctx->caml_young_base);
stdlib_r/caml_tmp/context.c:  printf("caml_young_start   : %p\n", (void*)ctx->caml_young_start);
stdlib_r/caml_tmp/context.c:  printf("caml_young_end     : %p\n", (void*)ctx->caml_young_end);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_ptr, caml_young_ptr);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_limit!=caml_young_limit)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_limit, caml_young_limit);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_base!=caml_young_base)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_base, caml_young_base);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_start!=caml_young_start)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_start, caml_young_start);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_end!=caml_young_end)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_end, caml_young_end);
stdlib_r/caml_tmp/context.c:  ctx->caml_young_ptr     = caml_young_ptr;
stdlib_r/caml_tmp/context.c:  ctx->caml_young_limit   = caml_young_limit;
stdlib_r/caml_tmp/context.c:  ctx->caml_young_base    = caml_young_base;
stdlib_r/caml_tmp/context.c:  ctx->caml_young_start   = caml_young_start;
stdlib_r/caml_tmp/context.c:  ctx->caml_young_end     = caml_young_end;
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_ptr, caml_young_ptr);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_limit!=caml_young_limit)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_limit, caml_young_limit);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_base!=caml_young_base)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_base, caml_young_base);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_start!=caml_young_start)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_start, caml_young_start);
stdlib_r/caml_tmp/context.c:  if (ctx->caml_young_end!=caml_young_end)
stdlib_r/caml_tmp/context.c:           ctx->caml_young_end, caml_young_end);
stdlib_r/caml_tmp/context.c:  caml_young_ptr     = ctx->caml_young_ptr; 
stdlib_r/caml_tmp/context.c:  caml_young_limit   = ctx->caml_young_limit;
stdlib_r/caml_tmp/context.c:  caml_young_base    = ctx->caml_young_base;
stdlib_r/caml_tmp/context.c:  caml_young_start   = ctx->caml_young_start;  
stdlib_r/caml_tmp/context.c:  caml_young_end     = ctx->caml_young_end;
stdlib_r/caml_tmp/reentrant_call:compact.c:  while (Chunk_size (ctx->compact_fl) - Chunk_alloc (ctx->compact_fl) <= Bhsize_wosize (3)
stdlib_r/caml_tmp/reentrant_call:compact.c:         && Chunk_size (Chunk_next (ctx->compact_fl))
stdlib_r/caml_tmp/reentrant_call:compact.c:            - Chunk_alloc (Chunk_next (ctx->compact_fl))
stdlib_r/caml_tmp/reentrant_call:compact.c:    ctx->compact_fl = Chunk_next (ctx->compact_fl);
stdlib_r/caml_tmp/reentrant_call:compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/reentrant_call:compact.c:  live = Wsize_bsize (ctx->caml_stat_heap_size) - ctx->caml_fl_cur_size;
stdlib_r/caml_tmp/reentrant_call:compact.c:    if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
stdlib_r/caml_tmp/reentrant_call:compact.c:    Assert (ctx->caml_stat_heap_chunks == 1);
stdlib_r/caml_tmp/reentrant_call:compact.c:    Assert (Chunk_next (ctx->caml_heap_start) == NULL);
stdlib_r/caml_tmp/reentrant_call:compact.c:    Assert (ctx->caml_stat_heap_size == Chunk_size (chunk));
stdlib_r/caml_tmp/reentrant_call:compact.c:                                          Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/reentrant_call:compact.c:  if (ctx->caml_percent_max >= 1000000) return;
stdlib_r/caml_tmp/reentrant_call:compact.c:  if (ctx->caml_stat_major_collections < 3) return;
stdlib_r/caml_tmp/reentrant_call:compact.c:  if (fw >= Wsize_bsize (ctx->caml_stat_heap_size)){
stdlib_r/caml_tmp/reentrant_call:compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
stdlib_r/caml_tmp/reentrant_call:compact.c:    fp = 100.0 * fw / (Wsize_bsize (ctx->caml_stat_heap_size) - fw);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) {
stdlib_r/caml_tmp/reentrant_call:compare.c:    free(ctx->compare_stack);
stdlib_r/caml_tmp/reentrant_call:compare.c:  asize_t newsize = 2 * (ctx->compare_stack_limit - ctx->compare_stack);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack == ctx->compare_stack_init) {
stdlib_r/caml_tmp/reentrant_call:compare.c:      realloc(ctx->compare_stack, sizeof(struct compare_item) * newsize);
stdlib_r/caml_tmp/reentrant_call:compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
stdlib_r/caml_tmp/reentrant_call:compare.c:          if (ctx->caml_compare_unordered && !total) return UNORDERED;
stdlib_r/caml_tmp/reentrant_call:compare.c:      if (ctx->caml_compare_unordered && !total) return UNORDERED;
stdlib_r/caml_tmp/reentrant_call:compare.c:        if (sp >= ctx->compare_stack_limit) sp = compare_resize_stack_r(ctx, sp);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/reentrant_call:compare.c:  if (ctx->compare_stack != ctx->compare_stack_init) compare_free_stack_r(ctx);
stdlib_r/caml_tmp/reentrant_call:context.c:  return Val_int(ctx->count_id);
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_limit!=caml_young_limit)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_base!=caml_young_base)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_start!=caml_young_start)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_end!=caml_young_end)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_ptr!=caml_young_ptr)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_limit!=caml_young_limit)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_base!=caml_young_base)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_start!=caml_young_start)
stdlib_r/caml_tmp/reentrant_call:context.c:  if (ctx->caml_young_end!=caml_young_end)
stdlib_r/caml_tmp/reentrant_call:finalise.c:  if (ctx->to_do_tl == NULL){
stdlib_r/caml_tmp/reentrant_call:finalise.c:    Assert (ctx->to_do_tl->next == NULL);
stdlib_r/caml_tmp/reentrant_call:finalise.c:  Assert (ctx->final_young == ctx->final_old);
stdlib_r/caml_tmp/reentrant_call:finalise.c:    Assert (Is_block (ctx->final_table[i].val));
stdlib_r/caml_tmp/reentrant_call:finalise.c:    Assert (Is_in_heap (ctx->final_table[i].val));
stdlib_r/caml_tmp/reentrant_call:finalise.c:    if (Is_white_val (ctx->final_table[i].val)) ++ todo_count;
stdlib_r/caml_tmp/reentrant_call:finalise.c:      Assert (Is_block (ctx->final_table[i].val));
stdlib_r/caml_tmp/reentrant_call:finalise.c:      Assert (Is_in_heap (ctx->final_table[i].val));
stdlib_r/caml_tmp/reentrant_call:finalise.c:      if (Is_white_val (ctx->final_table[i].val)){
stdlib_r/caml_tmp/reentrant_call:finalise.c:        if (Tag_val (ctx->final_table[i].val) == Forward_tag){
stdlib_r/caml_tmp/reentrant_call:finalise.c:          Assert (ctx->final_table[i].offset == 0);
stdlib_r/caml_tmp/reentrant_call:finalise.c:          fv = Forward_val (ctx->final_table[i].val);
stdlib_r/caml_tmp/reentrant_call:finalise.c:            if (Is_block (ctx->final_table[i].val)
stdlib_r/caml_tmp/reentrant_call:finalise.c:                && Is_in_heap (ctx->final_table[i].val)){
stdlib_r/caml_tmp/reentrant_call:finalise.c:      CAMLassert (Is_white_val (ctx->to_do_tl->item[i].val));
stdlib_r/caml_tmp/reentrant_call:finalise.c:      caml_darken_r (ctx, ctx->to_do_tl->item[i].val, NULL);
stdlib_r/caml_tmp/reentrant_call:finalise.c:  if (ctx->running_finalisation_function) return;
stdlib_r/caml_tmp/reentrant_call:finalise.c:  if (ctx->to_do_hd != NULL){
stdlib_r/caml_tmp/reentrant_call:finalise.c:      while (ctx->to_do_hd != NULL && ctx->to_do_hd->size == 0){
stdlib_r/caml_tmp/reentrant_call:finalise.c:        free (ctx->to_do_hd);
stdlib_r/caml_tmp/reentrant_call:finalise.c:        if (ctx->to_do_hd == NULL) ctx->to_do_tl = NULL;
stdlib_r/caml_tmp/reentrant_call:finalise.c:      if (ctx->to_do_hd == NULL) break;
stdlib_r/caml_tmp/reentrant_call:finalise.c:      Assert (ctx->to_do_hd->size > 0);
stdlib_r/caml_tmp/reentrant_call:finalise.c:      // phc - for f in reverse(ctx->to_do_head)
stdlib_r/caml_tmp/reentrant_call:finalise.c:  Assert (ctx->final_old == ctx->final_young);
stdlib_r/caml_tmp/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
stdlib_r/caml_tmp/reentrant_call:finalise.c:  Assert (ctx->final_old == ctx->final_young);
stdlib_r/caml_tmp/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
stdlib_r/caml_tmp/reentrant_call:finalise.c:  Assert (ctx->final_old <= ctx->final_young);
stdlib_r/caml_tmp/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
stdlib_r/caml_tmp/reentrant_call:finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
stdlib_r/caml_tmp/reentrant_call:finalise.c:  Assert (ctx->final_old <= ctx->final_young);
stdlib_r/caml_tmp/reentrant_call:finalise.c:  if (ctx->final_young >= ctx->final_size){
stdlib_r/caml_tmp/reentrant_call:finalise.c:    if (ctx->final_table == NULL){
stdlib_r/caml_tmp/reentrant_call:finalise.c:      Assert (ctx->final_old == 0);
stdlib_r/caml_tmp/reentrant_call:finalise.c:      Assert (ctx->final_young == 0);
stdlib_r/caml_tmp/reentrant_call:finalise.c:      ctx->final_table = caml_stat_resize (ctx->final_table,
stdlib_r/caml_tmp/reentrant_call:finalise.c:  Assert (ctx->final_young < ctx->final_size);
stdlib_r/caml_tmp/reentrant_call:freelist.c:    if (ctx->caml_fl_merge == cur) ctx->caml_fl_merge = prev;
stdlib_r/caml_tmp/reentrant_call:freelist.c:                                  Assert (ctx->fl_prev != NULL);
stdlib_r/caml_tmp/reentrant_call:freelist.c:      sz = Wosize_bp (Next (ctx->flp[i]));
stdlib_r/caml_tmp/reentrant_call:freelist.c:                                   i, ctx->flp[i], Next(ctx->flp[i]));
stdlib_r/caml_tmp/reentrant_call:freelist.c:    if (ctx->flp_size == 0){ 
stdlib_r/caml_tmp/reentrant_call:freelist.c:      prev = Next (ctx->flp[ctx->flp_size - 1]);
stdlib_r/caml_tmp/reentrant_call:freelist.c:      if (ctx->beyond != NULL) prev = ctx->beyond;
stdlib_r/caml_tmp/reentrant_call:freelist.c:    while (ctx->flp_size < FLP_MAX){
stdlib_r/caml_tmp/reentrant_call:freelist.c:    if (ctx->beyond != NULL){
stdlib_r/caml_tmp/reentrant_call:freelist.c:    prevsz = Wosize_bp (Next (ctx->flp[FLP_MAX-1]));
stdlib_r/caml_tmp/reentrant_call:freelist.c:        prevsz = Wosize_bp (Next (ctx->flp[i-1]));
stdlib_r/caml_tmp/reentrant_call:freelist.c:        if (Wosize_bp (Next (ctx->flp[i])) <= prevsz){
stdlib_r/caml_tmp/reentrant_call:freelist.c:          ctx->beyond = Next (ctx->flp[i]);
stdlib_r/caml_tmp/reentrant_call:freelist.c:            memmove (&(ctx->flp[i+j]), 
stdlib_r/caml_tmp/reentrant_call:freelist.c:                     &(ctx->flp[i+1]), 
stdlib_r/caml_tmp/reentrant_call:freelist.c:                     sizeof (block *) * (ctx->flp_size-i-1));
stdlib_r/caml_tmp/reentrant_call:freelist.c:             memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
stdlib_r/caml_tmp/reentrant_call:freelist.c:              memmove (&(ctx->flp[i+j]), 
stdlib_r/caml_tmp/reentrant_call:freelist.c:                       &(ctx->flp[i+1]), 
stdlib_r/caml_tmp/reentrant_call:freelist.c:              memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
stdlib_r/caml_tmp/reentrant_call:freelist.c:              memmove (&(ctx->flp[i]), &buf[0], 
stdlib_r/caml_tmp/reentrant_call:freelist.c:          ctx->beyond = Next (ctx->flp[FLP_MAX - 1]);
stdlib_r/caml_tmp/reentrant_call:freelist.c:    while (ctx->flp_size > 0 && Next (ctx->flp[ctx->flp_size - 1]) >= changed) 
stdlib_r/caml_tmp/reentrant_call:freelist.c:    if (ctx->beyond >= changed) 
stdlib_r/caml_tmp/reentrant_call:freelist.c:  Next (ctx->fl_head) = NULL;
stdlib_r/caml_tmp/reentrant_call:freelist.c:    truncate_flp_r (ctx, ctx->fl_head);
stdlib_r/caml_tmp/reentrant_call:freelist.c://    last_fragment or Next(ctx->caml_fl_merge).
stdlib_r/caml_tmp/reentrant_call:freelist.c:  if (ctx->last_fragment == Hp_bp (bp)){
stdlib_r/caml_tmp/reentrant_call:freelist.c:    Assert (ctx->caml_fl_merge == prev);
stdlib_r/caml_tmp/reentrant_call:freelist.c:                                                  Assert (ctx->fl_last != NULL);
stdlib_r/caml_tmp/reentrant_call:freelist.c:                                           Assert (Next (ctx->fl_last) == NULL);
stdlib_r/caml_tmp/reentrant_call:freelist.c:    Next (ctx->fl_last) = bp;
stdlib_r/caml_tmp/reentrant_call:freelist.c:    if (ctx->fl_last==ctx->caml_fl_merge && bp<ctx->caml_gc_sweep_hp){
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:          if (ctx->caml_gc_phase == Phase_sweep && cur_hp >= ctx->caml_gc_sweep_hp){
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:          == Wsize_bsize (ctx->caml_stat_heap_size));
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:                      + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:    intnat heap_words = Wsize_bsize (ctx->caml_stat_heap_size);
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:    intnat top_heap_words = Wsize_bsize (ctx->caml_stat_top_heap_size);
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:             + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:                    + (double) Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:  Store_field (res, 0, Val_long (Wsize_bsize (ctx->caml_minor_heap_size)));  /* s */
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:  Store_field (res, 1,Val_long(Wsize_bsize(ctx->caml_major_heap_increment)));/* i */
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:  Store_field (res, 2, Val_long (ctx->caml_percent_free));                   /* o */
stdlib_r/caml_tmp/reentrant_call:gc_ctrl.c:  Store_field (res, 4, Val_long (ctx->caml_percent_max));                    /* O */
stdlib_r/caml_tmp/reentrant_call:globroots.c:  caml_insert_global_root_r(ctx, &ctx->caml_global_roots, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:  caml_delete_global_root_r(ctx, &ctx->caml_global_roots, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:    caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_young, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:      caml_delete_global_root_r(ctx, &ctx->caml_global_roots_old, r);
stdlib_r/caml_tmp/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
stdlib_r/caml_tmp/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
stdlib_r/caml_tmp/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_old);
stdlib_r/caml_tmp/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots);
stdlib_r/caml_tmp/reentrant_call:globroots.c:  caml_iterate_global_roots_r(ctx, f, &ctx->caml_global_roots_young);
stdlib_r/caml_tmp/reentrant_call:globroots.c:    caml_insert_global_root_r(ctx, &ctx->caml_global_roots_old, gr->root);
stdlib_r/caml_tmp/reentrant_call:io.c:  if (ctx->caml_all_opened_channels != NULL)
stdlib_r/caml_tmp/reentrant_call:io.c:    if (ctx->caml_all_opened_channels != NULL)
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  Assert (ctx->gray_vals_cur == ctx->gray_vals_end);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->gray_vals_size < ctx->caml_stat_heap_size / 128){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        if (ctx->gray_vals_cur >= ctx->gray_vals_end) realloc_gray_vals_r (ctx);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  Assert (ctx->gray_vals_cur == gray_vals);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:    }else if (ctx->markhp != NULL){       // phc - keep processing current chunk
stdlib_r/caml_tmp/reentrant_call:major_gc.c:      if (ctx->markhp == ctx->limit){     // this chunk is done with marking
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        ctx->chunk = Chunk_next (ctx->chunk);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        if (ctx->chunk == NULL){          // no more chunk to mark
stdlib_r/caml_tmp/reentrant_call:major_gc.c:          ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        if (Is_gray_val (Val_hp (ctx->markhp))){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:          *gray_vals_ptr++ = Val_hp (ctx->markhp);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        ctx->markhp += Bhsize_hp (ctx->markhp); // mark next block as todo
stdlib_r/caml_tmp/reentrant_call:major_gc.c:      ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:      switch (ctx->caml_gc_subphase){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        ctx->weak_prev = &(ctx->caml_weak_list_head);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        cur = *(ctx->weak_prev);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:          ctx->weak_prev = &(ctx->caml_weak_list_head);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:    if (ctx->caml_gc_sweep_hp < ctx->limit){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        ctx->caml_gc_sweep_hp = caml_fl_merge_block_r (ctx, Bp_hp (hp));
stdlib_r/caml_tmp/reentrant_call:major_gc.c:      Assert (ctx->caml_gc_sweep_hp <= ctx->limit);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:      ctx->chunk = Chunk_next (ctx->chunk);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:      if (ctx->chunk == NULL){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        ++ (ctx->caml_stat_major_collections);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:        ctx->limit = ctx->chunk + Chunk_size (ctx->chunk);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:      / Wsize_bsize (ctx->caml_stat_heap_size) / ctx->caml_percent_free / 2.0;
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->caml_dependent_size > 0){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 250
stdlib_r/caml_tmp/reentrant_call:major_gc.c:    computed_work = (intnat) (p * Wsize_bsize (ctx->caml_stat_heap_size) * 5 / 3);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_mark){
stdlib_r/caml_tmp/reentrant_call:major_gc.c:    Assert (ctx->caml_gc_phase == Phase_sweep);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) caml_compact_heap_maybe_r (ctx);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->caml_gc_phase == Phase_idle) start_cycle_r (ctx);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  while (ctx->caml_gc_phase == Phase_mark) mark_slice_r (ctx, LONG_MAX);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  Assert (ctx->caml_gc_phase == Phase_sweep);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  while (ctx->caml_gc_phase == Phase_sweep) sweep_slice_r (ctx, LONG_MAX);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  Assert (ctx->caml_gc_phase == Phase_idle);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  Assert (ctx->caml_stat_heap_size % Page_size == 0);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  ctx->caml_heap_start = (char *) caml_alloc_for_heap (ctx->caml_stat_heap_size);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->caml_heap_start == NULL)
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  Chunk_next (ctx->caml_heap_start) = NULL;
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  caml_make_free_blocks_r (ctx, (value *) ctx->caml_heap_start,
stdlib_r/caml_tmp/reentrant_call:major_gc.c:                         Wsize_bsize (ctx->caml_stat_heap_size), 1, Caml_white);
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  ctx->gray_vals = (value *) malloc (ctx->gray_vals_size * sizeof (value));
stdlib_r/caml_tmp/reentrant_call:major_gc.c:  if (ctx->gray_vals == NULL)
stdlib_r/caml_tmp/reentrant_call:md5.c:    if ((ctx->bits[0] = t + ((uint32) len << 3)) < t)
stdlib_r/caml_tmp/reentrant_call:md5.c:        byteReverse(ctx->in, 16);
stdlib_r/caml_tmp/reentrant_call:md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/reentrant_call:md5.c:        memcpy(ctx->in, buf, 64);
stdlib_r/caml_tmp/reentrant_call:md5.c:        byteReverse(ctx->in, 16);
stdlib_r/caml_tmp/reentrant_call:md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/reentrant_call:md5.c:    memcpy(ctx->in, buf, len);
stdlib_r/caml_tmp/reentrant_call:md5.c:    count = (ctx->bits[0] >> 3) & 0x3F;
stdlib_r/caml_tmp/reentrant_call:md5.c:        byteReverse(ctx->in, 16);
stdlib_r/caml_tmp/reentrant_call:md5.c:        caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/reentrant_call:md5.c:        memset(ctx->in, 0, 56);
stdlib_r/caml_tmp/reentrant_call:md5.c:    byteReverse(ctx->in, 14);
stdlib_r/caml_tmp/reentrant_call:md5.c:    caml_MD5Transform(ctx->buf, (uint32 *) ctx->in);
stdlib_r/caml_tmp/reentrant_call:memory.c:                   (ctx->caml_stat_heap_size + Chunk_size (m)) / 1024);
stdlib_r/caml_tmp/reentrant_call:memory.c:  if (ctx->caml_stat_heap_size > ctx->caml_stat_top_heap_size){
stdlib_r/caml_tmp/reentrant_call:memory.c:  if (ctx->caml_gc_phase == Phase_mark
stdlib_r/caml_tmp/reentrant_call:memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
stdlib_r/caml_tmp/reentrant_call:memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
stdlib_r/caml_tmp/reentrant_call:memory.c:            || (ctx->caml_gc_phase == Phase_sweep
stdlib_r/caml_tmp/reentrant_call:memory.c:      if (ctx->caml_in_minor_collection)
stdlib_r/caml_tmp/reentrant_call:memory.c:  if (ctx->caml_gc_phase == Phase_mark
stdlib_r/caml_tmp/reentrant_call:memory.c:      || (ctx->caml_gc_phase == Phase_sweep && (addr)hp >= (addr)ctx->caml_gc_sweep_hp)){
stdlib_r/caml_tmp/reentrant_call:memory.c:    Assert (ctx->caml_gc_phase == Phase_idle
stdlib_r/caml_tmp/reentrant_call:memory.c:            || (ctx->caml_gc_phase == Phase_sweep
stdlib_r/caml_tmp/reentrant_call:memory.c:  if (ctx->caml_allocated_words > Wsize_bsize (ctx->caml_minor_heap_size)){
stdlib_r/caml_tmp/reentrant_call:memory.c:  if (ctx->caml_extra_heap_resources > 1.0){
stdlib_r/caml_tmp/reentrant_call:memory.c:  if (ctx->caml_extra_heap_resources
stdlib_r/caml_tmp/reentrant_call:memory.c:           > (double) Wsize_bsize (ctx->caml_minor_heap_size) / 2.0
stdlib_r/caml_tmp/reentrant_call:memory.c:             / (double) Wsize_bsize (ctx->caml_stat_heap_size)) {
stdlib_r/caml_tmp/reentrant_call:memory.c:    if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){
stdlib_r/caml_tmp/reentrant_call:memory.c:      caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);
stdlib_r/caml_tmp/reentrant_call:memory.c:    *(ctx->caml_ref_table.ptr++) = fp;
stdlib_r/caml_tmp/reentrant_call:memory.h:  if (ctx->caml_young_ptr < ctx->caml_young_start){                              \
stdlib_r/caml_tmp/reentrant_call:memory.h:  Hd_hp (ctx->caml_young_ptr) = Make_header ((wosize), (tag), Caml_black);  \
stdlib_r/caml_tmp/reentrant_call:memory.h:  (result) = Val_hp (ctx->caml_young_ptr);                                  \
stdlib_r/caml_tmp/reentrant_call:memory.h:    if (ctx->caml_gc_phase == Phase_mark) caml_darken_r (ctx, _old_, NULL); \
stdlib_r/caml_tmp/reentrant_call:memory.h:      if (ctx->caml_ref_table.ptr >= ctx->caml_ref_table.limit){            \
stdlib_r/caml_tmp/reentrant_call:memory.h:        CAMLassert (ctx->caml_ref_table.ptr == ctx->caml_ref_table.limit);  \
stdlib_r/caml_tmp/reentrant_call:memory.h:        caml_realloc_ref_table_r (ctx, &ctx->caml_ref_table);               \
stdlib_r/caml_tmp/reentrant_call:memory.h:      *(ctx->caml_ref_table).ptr++ = (fp);                                  \
stdlib_r/caml_tmp/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/reentrant_call:memory.h:    (ctx->caml_local_roots = &caml__roots_##x), \
stdlib_r/caml_tmp/reentrant_call:minor_gc.c://  if (ctx->caml_young_ptr != ctx->caml_young_end) caml_minor_collection_r (ctx);
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:                                    Assert (ctx->caml_young_ptr == ctx->caml_young_end);
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:  if (ctx->caml_young_start != NULL){
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:    free (ctx->caml_young_base);
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:  reset_table (&(ctx->caml_ref_table));
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:  reset_table (&(ctx->caml_weak_ref_table));
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:  while (ctx->oldify_todo_list != 0){
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:  if (ctx->caml_young_ptr != ctx->caml_young_end){
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:    if (ctx->caml_young_ptr < ctx->caml_young_start)
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:    ctx->caml_stat_minor_words += Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:    clear_table (&(ctx->caml_ref_table));
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:    clear_table (&(ctx->caml_weak_ref_table));
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:  if (ctx->caml_force_major_slice) caml_minor_collection_r(ctx);
stdlib_r/caml_tmp/reentrant_call:minor_gc.c:                                             Assert (ctx->caml_force_major_slice);
stdlib_r/caml_tmp/reentrant_call:printexc.c:  if (ctx->caml_backtrace_active
stdlib_r/caml_tmp/reentrant_call:weak.c:      if (ctx->caml_weak_ref_table.ptr >= ctx->caml_weak_ref_table.limit){
stdlib_r/caml_tmp/reentrant_call:weak.c:        CAMLassert (ctx->caml_weak_ref_table.ptr == ctx->caml_weak_ref_table.limit);
stdlib_r/caml_tmp/reentrant_call:weak.c:        caml_realloc_ref_table_r (ctx, &(ctx->caml_weak_ref_table));
stdlib_r/caml_tmp/reentrant_call:weak.c:      *(ctx->caml_weak_ref_table.ptr++) = &Field (ar, offset);
stdlib_r/caml_tmp/reentrant_call:weak.c:    if (ctx->caml_gc_phase == Phase_mark && Is_block (elt) && Is_in_heap (elt)){
stdlib_r/caml_tmp/reentrant_call:weak.c:        if (ctx->caml_gc_phase == Phase_mark && Is_block (f) && Is_in_heap (f)){
stdlib_r/caml_tmp/reentrant_call:weak.c:  if (ctx->caml_gc_phase == Phase_mark && ctx->caml_gc_subphase == Subphase_weak1){
stdlib_r/caml_tmp/reentrant_call:win32.c:  DWORD *ctx_ip = &(ctx->Eip);
stdlib_r/caml_tmp/reentrant_call:win32.c:  DWORD *ctx_sp = &(ctx->Esp);
stdlib_r/caml_tmp/freelist.c:    ctx->caml_fl_cur_size -= Whsize_hd (h);
stdlib_r/caml_tmp/freelist.c:    if (ctx->caml_fl_merge == cur) ctx->caml_fl_merge = prev;
stdlib_r/caml_tmp/freelist.c:      if (flpi + 1 < ctx->flp_size && ctx->flp[flpi + 1] == cur){
stdlib_r/caml_tmp/freelist.c:        ctx->flp[flpi + 1] = prev;
stdlib_r/caml_tmp/freelist.c:      }else if (flpi == ctx->flp_size - 1){
stdlib_r/caml_tmp/freelist.c:        ctx->beyond = (prev == ctx->fl_head) ? NULL : prev;
stdlib_r/caml_tmp/freelist.c:        -- ctx->flp_size;
stdlib_r/caml_tmp/freelist.c:    ctx->caml_fl_cur_size -= wh_sz;
stdlib_r/caml_tmp/freelist.c:  if (policy == Policy_next_fit) ctx->fl_prev = prev;
stdlib_r/caml_tmp/freelist.c:                                  Assert (ctx->fl_prev != NULL);
stdlib_r/caml_tmp/freelist.c:    prev = ctx->fl_prev;
stdlib_r/caml_tmp/freelist.c:    ctx->fl_last = prev;
stdlib_r/caml_tmp/freelist.c:    prev = ctx->fl_head;
stdlib_r/caml_tmp/freelist.c:    while (prev != ctx->fl_prev){
stdlib_r/caml_tmp/freelist.c:    for (i = 0; i < ctx->flp_size; i++){
stdlib_r/caml_tmp/freelist.c:      sz = Wosize_bp (Next (ctx->flp[i]));
stdlib_r/caml_tmp/freelist.c:                                   i, ctx->flp[i], Next(ctx->flp[i]));
stdlib_r/caml_tmp/freelist.c:    if (ctx->flp_size == 0){ 
stdlib_r/caml_tmp/freelist.c:      prev = ctx->fl_head;
stdlib_r/caml_tmp/freelist.c:      prev = Next (ctx->flp[ctx->flp_size - 1]);
stdlib_r/caml_tmp/freelist.c:      if (ctx->beyond != NULL) prev = ctx->beyond;
stdlib_r/caml_tmp/freelist.c:    while (ctx->flp_size < FLP_MAX){
stdlib_r/caml_tmp/freelist.c:        ctx->fl_last = prev;
stdlib_r/caml_tmp/freelist.c:        ctx->beyond = (prev==ctx->fl_head) ? NULL : prev;
stdlib_r/caml_tmp/freelist.c:          ctx->flp[ctx->flp_size] = prev;
stdlib_r/caml_tmp/freelist.c:          ++ ctx->flp_size;
stdlib_r/caml_tmp/freelist.c:            ctx->beyond = cur;
stdlib_r/caml_tmp/freelist.c:            i = ctx->flp_size - 1;
stdlib_r/caml_tmp/freelist.c:                                       ctx->flp_size - 1, prev, cur);
stdlib_r/caml_tmp/freelist.c:    ctx->beyond = cur;
stdlib_r/caml_tmp/freelist.c:    if (ctx->beyond != NULL){
stdlib_r/caml_tmp/freelist.c:      prev = ctx->beyond;
stdlib_r/caml_tmp/freelist.c:      prev = ctx->flp[ctx->flp_size - 1];
stdlib_r/caml_tmp/freelist.c:    prevsz = Wosize_bp (Next (ctx->flp[FLP_MAX-1]));
stdlib_r/caml_tmp/freelist.c:        ctx->beyond = cur;
stdlib_r/caml_tmp/freelist.c:                                 ctx->flp_size, prev, cur);
stdlib_r/caml_tmp/freelist.c:    ctx->fl_last = prev;
stdlib_r/caml_tmp/freelist.c:    Assert (0 <= i && i < ctx->flp_size + 1);
stdlib_r/caml_tmp/freelist.c:    if (i < ctx->flp_size){
stdlib_r/caml_tmp/freelist.c:        prevsz = Wosize_bp (Next (ctx->flp[i-1]));
stdlib_r/caml_tmp/freelist.c:      if (i == ctx->flp_size - 1){
stdlib_r/caml_tmp/freelist.c:        if (Wosize_bp (Next (ctx->flp[i])) <= prevsz){
stdlib_r/caml_tmp/freelist.c:          ctx->beyond = Next (ctx->flp[i]);
stdlib_r/caml_tmp/freelist.c:          -- ctx->flp_size;
stdlib_r/caml_tmp/freelist.c:          ctx->beyond = NULL;
stdlib_r/caml_tmp/freelist.c:        prev = ctx->flp[i];
stdlib_r/caml_tmp/freelist.c:        while (prev != ctx->flp[i+1]){
stdlib_r/caml_tmp/freelist.c:        if (FLP_MAX >= ctx->flp_size + j - 1){
stdlib_r/caml_tmp/freelist.c:            memmove (&(ctx->flp[i+j]), 
stdlib_r/caml_tmp/freelist.c:                     &(ctx->flp[i+1]), 
stdlib_r/caml_tmp/freelist.c:                     sizeof (block *) * (ctx->flp_size-i-1));
stdlib_r/caml_tmp/freelist.c:             memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
stdlib_r/caml_tmp/freelist.c:          ctx->flp_size += j - 1;
stdlib_r/caml_tmp/freelist.c:              memmove (&(ctx->flp[i+j]), 
stdlib_r/caml_tmp/freelist.c:                       &(ctx->flp[i+1]), 
stdlib_r/caml_tmp/freelist.c:              memmove (&(ctx->flp[i]), &buf[0], sizeof (block *) * j);
stdlib_r/caml_tmp/freelist.c:              memmove (&(ctx->flp[i]), &buf[0], 
stdlib_r/caml_tmp/freelist.c:          ctx->flp_size = FLP_MAX - 1;
stdlib_r/caml_tmp/freelist.c:          ctx->beyond = Next (ctx->flp[FLP_MAX - 1]);
stdlib_r/caml_tmp/freelist.c:  ctx->last_fragment = NULL;
stdlib_r/caml_tmp/freelist.c:  ctx->caml_fl_merge = ctx->fl_head;
stdlib_r/caml_tmp/freelist.c:  if (changed == ctx->fl_head){
stdlib_r/caml_tmp/freelist.c:    ctx->flp_size = 0;
stdlib_r/caml_tmp/freelist.c:    ctx->beyond = NULL;
stdlib_r/caml_tmp/freelist.c:    while (ctx->flp_size > 0 && Next (ctx->flp[ctx->flp_size - 1]) >= changed) 
stdlib_r/caml_tmp/freelist.c:      -- ctx->flp_size;
stdlib_r/caml_tmp/freelist.c:    if (ctx->beyond >= changed) 
stdlib_r/caml_tmp/freelist.c:      ctx->beyond = NULL;
stdlib_r/caml_tmp/freelist.c:  Next (ctx->fl_head) = NULL;
stdlib_r/caml_tmp/freelist.c:    ctx->fl_prev = ctx->fl_head;
stdlib_r/caml_tmp/freelist.c:    truncate_flp_r (ctx, ctx->fl_head);
stdlib_r/caml_tmp/freelist.c:  ctx->caml_fl_cur_size = 0;
stdlib_r/caml_tmp/freelist.c://    last_fragment or Next(ctx->caml_fl_merge).
stdlib_r/caml_tmp/freelist.c:  ctx->caml_fl_cur_size += Whsize_hd (hd);
stdlib_r/caml_tmp/freelist.c:  prev = ctx->caml_fl_merge;
stdlib_r/caml_tmp/freelist.c:  Assert (prev < bp || prev == ctx->fl_head);
stdlib_r/caml_tmp/freelist.c:  if (ctx->last_fragment == Hp_bp (bp)){
stdlib_r/caml_tmp/freelist.c:      bp = ctx->last_fragment;
stdlib_r/caml_tmp/freelist.c:      ctx->caml_fl_cur_size += Whsize_wosize (0);
stdlib_r/caml_tmp/freelist.c:      if (policy == Policy_next_fit && ctx->fl_prev == cur) ctx->fl_prev = prev;
stdlib_r/caml_tmp/freelist.c:    Assert (ctx->caml_fl_merge == prev);
stdlib_r/caml_tmp/freelist.c:    ctx->caml_fl_merge = bp;
stdlib_r/caml_tmp/freelist.c:    ctx->last_fragment = bp;
stdlib_r/caml_tmp/freelist.c:    ctx->caml_fl_cur_size -= Whsize_wosize (0);
stdlib_r/caml_tmp/freelist.c:                                                  Assert (ctx->fl_last != NULL);
stdlib_r/caml_tmp/freelist.c:                                           Assert (Next (ctx->fl_last) == NULL);
stdlib_r/caml_tmp/freelist.c:  ctx->caml_fl_cur_size += Whsize_bp (bp);
stdlib_r/caml_tmp/freelist.c:  if (bp > ctx->fl_last){
stdlib_r/caml_tmp/freelist.c:    Next (ctx->fl_last) = bp;
stdlib_r/caml_tmp/freelist.c:    if (ctx->fl_last==ctx->caml_fl_merge && bp<ctx->caml_gc_sweep_hp){
stdlib_r/caml_tmp/freelist.c:      ctx->caml_fl_merge = (char *) Field (bp, 1);
stdlib_r/caml_tmp/freelist.c:    if (policy==Policy_first_fit && ctx->flp_size<FLP_MAX){
stdlib_r/caml_tmp/freelist.c:      ctx->flp [ctx->flp_size++] = ctx->fl_last;
stdlib_r/caml_tmp/freelist.c:    prev = ctx->fl_head;
stdlib_r/caml_tmp/freelist.c:    while (cur != NULL && cur < bp){   Assert (prev < bp || prev == ctx->fl_head);
stdlib_r/caml_tmp/freelist.c:    }                                  Assert (prev < bp || prev == ctx->fl_head);
stdlib_r/caml_tmp/freelist.c:    if (prev == ctx->caml_fl_merge && bp < ctx->caml_gc_sweep_hp){
stdlib_r/caml_tmp/freelist.c:      ctx->caml_fl_merge = (char *) Field (bp, 1);
stdlib_r/caml_tmp/freelist.c:    ctx->fl_prev = ctx->fl_head;
stdlib_r/caml_tmp/freelist.c:    ctx->flp_size = 0;
stdlib_r/caml_tmp/freelist.c:    ctx->beyond = NULL;
stdlib_r/caml_tmp/minor_gc.c://  if (ctx->caml_young_ptr != ctx->caml_young_end) caml_minor_collection_r (ctx);
stdlib_r/caml_tmp/minor_gc.c:                                    Assert (ctx->caml_young_ptr == ctx->caml_young_end);
stdlib_r/caml_tmp/minor_gc.c:  if (ctx->caml_young_start != NULL){
stdlib_r/caml_tmp/minor_gc.c:    caml_page_table_remove(In_young, ctx->caml_young_start, ctx->caml_young_end);
stdlib_r/caml_tmp/minor_gc.c:    free (ctx->caml_young_base);
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_young_base = new_heap_base;
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_young_start = new_heap;
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_young_end = new_heap + size;
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_young_limit = ctx->caml_young_start;
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_young_ptr = ctx->caml_young_end;
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_minor_heap_size = size;
stdlib_r/caml_tmp/minor_gc.c:  reset_table (&(ctx->caml_ref_table));
stdlib_r/caml_tmp/minor_gc.c:  reset_table (&(ctx->caml_weak_ref_table));
stdlib_r/caml_tmp/minor_gc.c:    Assert (Hp_val (v) >= ctx->caml_young_ptr);
stdlib_r/caml_tmp/minor_gc.c:          Field (result, 1) = ctx->oldify_todo_list;    /* Add this block */
stdlib_r/caml_tmp/minor_gc.c:          ctx->oldify_todo_list = v;                    /*  to the "to do" list. */
stdlib_r/caml_tmp/minor_gc.c:  while (ctx->oldify_todo_list != 0){
stdlib_r/caml_tmp/minor_gc.c:    v = ctx->oldify_todo_list;                /* Get the head. */
stdlib_r/caml_tmp/minor_gc.c:    ctx->oldify_todo_list = Field (new_v, 1); /* Remove from list. */
stdlib_r/caml_tmp/minor_gc.c:  if (ctx->caml_young_ptr != ctx->caml_young_end){
stdlib_r/caml_tmp/minor_gc.c:    ctx->caml_in_minor_collection = 1;
stdlib_r/caml_tmp/minor_gc.c:    for (r = ctx->caml_ref_table.base; r < ctx->caml_ref_table.ptr; r++){
stdlib_r/caml_tmp/minor_gc.c:    for (r = ctx->caml_weak_ref_table.base; r < ctx->caml_weak_ref_table.ptr; r++){
stdlib_r/caml_tmp/minor_gc.c:    if (ctx->caml_young_ptr < ctx->caml_young_start)
stdlib_r/caml_tmp/minor_gc.c:      ctx->caml_young_ptr = ctx->caml_young_start;
stdlib_r/caml_tmp/minor_gc.c:    ctx->caml_stat_minor_words += Wsize_bsize (ctx->caml_young_end - ctx->caml_young_ptr);
stdlib_r/caml_tmp/minor_gc.c:    ctx->caml_young_ptr = ctx->caml_young_end;
stdlib_r/caml_tmp/minor_gc.c:    ctx->caml_young_limit = ctx->caml_young_start;
stdlib_r/caml_tmp/minor_gc.c:    clear_table (&(ctx->caml_ref_table));
stdlib_r/caml_tmp/minor_gc.c:    clear_table (&(ctx->caml_weak_ref_table));
stdlib_r/caml_tmp/minor_gc.c:    ctx->caml_in_minor_collection = 0;
stdlib_r/caml_tmp/minor_gc.c:    for (p = (value *) ctx->caml_young_start; p < (value *) ctx->caml_young_end; ++p){
stdlib_r/caml_tmp/minor_gc.c:  intnat prev_alloc_words = ctx->caml_allocated_words;
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_stat_promoted_words += ctx->caml_allocated_words - prev_alloc_words;
stdlib_r/caml_tmp/minor_gc.c:  ++ ctx->caml_stat_minor_collections;
stdlib_r/caml_tmp/minor_gc.c:  ctx->caml_force_major_slice = 0;
stdlib_r/caml_tmp/minor_gc.c:  if (ctx->caml_force_major_slice) caml_minor_collection_r(ctx);
stdlib_r/caml_tmp/minor_gc.c:    caml_alloc_table (tbl, ctx->caml_minor_heap_size / sizeof (value) / 8, 256);
stdlib_r/caml_tmp/minor_gc.c:                                             Assert (ctx->caml_force_major_slice);
stdlib_r/caml_tmp/finalise.c:  if (ctx->to_do_tl == NULL){
stdlib_r/caml_tmp/finalise.c:    ctx->to_do_hd = result;
stdlib_r/caml_tmp/finalise.c:    ctx->to_do_tl = result;
stdlib_r/caml_tmp/finalise.c:    Assert (ctx->to_do_tl->next == NULL);
stdlib_r/caml_tmp/finalise.c:    ctx->to_do_tl->next = result;
stdlib_r/caml_tmp/finalise.c:    ctx->to_do_tl = result;
stdlib_r/caml_tmp/finalise.c: * 2. put white blocks in the ctx->final_table into the todo table
stdlib_r/caml_tmp/finalise.c:  Assert (ctx->final_young == ctx->final_old);
stdlib_r/caml_tmp/finalise.c:  for (i = 0; i < ctx->final_old; i++){
stdlib_r/caml_tmp/finalise.c:    Assert (Is_block (ctx->final_table[i].val));
stdlib_r/caml_tmp/finalise.c:    Assert (Is_in_heap (ctx->final_table[i].val));
stdlib_r/caml_tmp/finalise.c:    if (Is_white_val (ctx->final_table[i].val)) ++ todo_count;
stdlib_r/caml_tmp/finalise.c:    for (i = 0; i < ctx->final_old; i++){
stdlib_r/caml_tmp/finalise.c:      Assert (Is_block (ctx->final_table[i].val));
stdlib_r/caml_tmp/finalise.c:      Assert (Is_in_heap (ctx->final_table[i].val));
stdlib_r/caml_tmp/finalise.c:      if (Is_white_val (ctx->final_table[i].val)){
stdlib_r/caml_tmp/finalise.c:        if (Tag_val (ctx->final_table[i].val) == Forward_tag){
stdlib_r/caml_tmp/finalise.c:          Assert (ctx->final_table[i].offset == 0);
stdlib_r/caml_tmp/finalise.c:          fv = Forward_val (ctx->final_table[i].val);
stdlib_r/caml_tmp/finalise.c:            ctx->final_table[i].val = fv;
stdlib_r/caml_tmp/finalise.c:            if (Is_block (ctx->final_table[i].val)
stdlib_r/caml_tmp/finalise.c:                && Is_in_heap (ctx->final_table[i].val)){
stdlib_r/caml_tmp/finalise.c:        ctx->to_do_tl->item[k++] = ctx->final_table[i];
stdlib_r/caml_tmp/finalise.c:        ctx->final_table[j++] = ctx->final_table[i];
stdlib_r/caml_tmp/finalise.c:    ctx->final_young = ctx->final_old = j;
stdlib_r/caml_tmp/finalise.c:    ctx->to_do_tl->size = k;
stdlib_r/caml_tmp/finalise.c:      CAMLassert (Is_white_val (ctx->to_do_tl->item[i].val));
stdlib_r/caml_tmp/finalise.c:      caml_darken_r (ctx, ctx->to_do_tl->item[i].val, NULL);
stdlib_r/caml_tmp/finalise.c:  if (ctx->running_finalisation_function) return;
stdlib_r/caml_tmp/finalise.c:  if (ctx->to_do_hd != NULL){
stdlib_r/caml_tmp/finalise.c:      while (ctx->to_do_hd != NULL && ctx->to_do_hd->size == 0){
stdlib_r/caml_tmp/finalise.c:        struct to_do *next_hd = ctx->to_do_hd->next;
stdlib_r/caml_tmp/finalise.c:        free (ctx->to_do_hd);
stdlib_r/caml_tmp/finalise.c:        ctx->to_do_hd = next_hd;
stdlib_r/caml_tmp/finalise.c:        if (ctx->to_do_hd == NULL) ctx->to_do_tl = NULL;
stdlib_r/caml_tmp/finalise.c:      if (ctx->to_do_hd == NULL) break;
stdlib_r/caml_tmp/finalise.c:      Assert (ctx->to_do_hd->size > 0);
stdlib_r/caml_tmp/finalise.c:      // phc - for f in reverse(ctx->to_do_head)
stdlib_r/caml_tmp/finalise.c:      -- ctx->to_do_hd->size;
stdlib_r/caml_tmp/finalise.c:      f = ctx->to_do_hd->item[ctx->to_do_hd->size];
stdlib_r/caml_tmp/finalise.c:      ctx->running_finalisation_function = 1;
stdlib_r/caml_tmp/finalise.c:      ctx->running_finalisation_function = 0;
stdlib_r/caml_tmp/finalise.c:  Assert (ctx->final_old == ctx->final_young);
stdlib_r/caml_tmp/finalise.c:  for (i = 0; i < ctx->final_old; i++) 
stdlib_r/caml_tmp/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
stdlib_r/caml_tmp/finalise.c:  for (todo = ctx->to_do_hd; todo != NULL; todo = todo->next){
stdlib_r/caml_tmp/finalise.c:  Assert (ctx->final_old == ctx->final_young);
stdlib_r/caml_tmp/finalise.c:  for (i = 0; i < ctx->final_old; i++) 
stdlib_r/caml_tmp/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
stdlib_r/caml_tmp/finalise.c://                 ctx->final_old, ctx->final_young);
stdlib_r/caml_tmp/finalise.c:  Assert (ctx->final_old <= ctx->final_young);
stdlib_r/caml_tmp/finalise.c:  for (i = ctx->final_old; i < ctx->final_young; i++){
stdlib_r/caml_tmp/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].fun);
stdlib_r/caml_tmp/finalise.c:    Call_action_r (ctx, f, ctx->final_table[i].val);
stdlib_r/caml_tmp/finalise.c:  ctx->final_old = ctx->final_young;
stdlib_r/caml_tmp/finalise.c:  Assert (ctx->final_old <= ctx->final_young);
stdlib_r/caml_tmp/finalise.c:  if (ctx->final_young >= ctx->final_size){
stdlib_r/caml_tmp/finalise.c:    if (ctx->final_table == NULL){
stdlib_r/caml_tmp/finalise.c:      ctx->final_table = caml_stat_alloc (new_size * sizeof (struct final));
stdlib_r/caml_tmp/finalise.c:      Assert (ctx->final_old == 0);
stdlib_r/caml_tmp/finalise.c:      Assert (ctx->final_young == 0);
stdlib_r/caml_tmp/finalise.c:      ctx->final_size = new_size;
stdlib_r/caml_tmp/finalise.c:      uintnat new_size = ctx->final_size * 2;
stdlib_r/caml_tmp/finalise.c:      ctx->final_table = caml_stat_resize (ctx->final_table,
stdlib_r/caml_tmp/finalise.c:      ctx->final_size = new_size;
stdlib_r/caml_tmp/finalise.c:  Assert (ctx->final_young < ctx->final_size);
stdlib_r/caml_tmp/finalise.c:  ctx->final_table[ctx->final_young].fun = f;
stdlib_r/caml_tmp/finalise.c:    ctx->final_table[ctx->final_young].offset = Infix_offset_val (v);
stdlib_r/caml_tmp/finalise.c:    ctx->final_table[ctx->final_young].val = v - Infix_offset_val (v);
stdlib_r/caml_tmp/finalise.c:    ctx->final_table[ctx->final_young].offset = 0;
stdlib_r/caml_tmp/finalise.c:    ctx->final_table[ctx->final_young].val = v;
stdlib_r/caml_tmp/finalise.c:  ++ ctx->final_young;
stdlib_r/caml_tmp/finalise.c:  ctx->running_finalisation_function = 0;
